---
title: "Prediction + uncertainty"
subtitle: "Lecture 18"
format:
  revealjs: default
editor_options: 
  chunk_output_type: console
jupyter: python3
execute:
  warning: false
  error: false
---

## Setup {.smaller}

```{python}
#| message: false

import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style="whitegrid", rc={"figure.figsize": (8, 6), "axes.labelsize": 16, "xtick.labelsize": 14, "ytick.labelsize": 14})

```

# Model selection

## Data: Candy Rankings {.smaller}

```{python}
candy_rankings = pd.read_csv("data/candy_rankings.csv")

candy_rankings.info()
```

## Full model {.smaller}

::: question
What percent of the variability in win percentages is explained by the model?
:::

```{python}
full_model = smf.ols('winpercent ~ chocolate + fruity + caramel + peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent', data=candy_rankings).fit()
print(f'R-squared: {full_model.rsquared.round(3)}')
print(f'Adjusted R-squared: {full_model.rsquared_adj.round(3)}')
```

## Akaike Information Criterion {.smaller}

$$ AIC = -2log(L) + 2k $$

::: incremental
-   $L$: likelihood of the model
    -   Likelihood of seeing these data given the estimated model parameters
    -   Won't go into calculating it in this course (but you will in future courses)
-   Used for model selection, lower the better
    -   Value is not informative on its own
-   Applies a penalty for number of parameters in the model, $k$
    -   Different penalty than adjusted $R^2$ but similar idea
:::

::: fragment
```{python}
#| label: aic-full-model
print(f'AIC: {full_model.aic}')
```
:::

## Model selection -- a little faster {.smaller}

```{python results="hide"}
selected_model = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=candy_rankings).fit()
```

```{python}
print(selected_model.summary2())
```

## Selected variables {.smaller}

| variable         | selected |
|------------------|:--------:|
| chocolate        |    x     |
| fruity           |    x     |
| caramel          |          |
| peanutyalmondy   |    x     |
| nougat           |          |
| crispedricewafer |    x     |
| hard             |    x     |
| bar              |          |
| pluribus         |          |
| sugarpercent     |    x     |
| pricepercent     |          |

------------------------------------------------------------------------

## Coefficient interpretation {.smaller}

::: question
Interpret the slopes of `chocolate` and `sugarpercent` in context of the data.
:::

```{python}
#| echo: false
coefficients = selected_model.params
print(coefficients)
```

## AIC {.smaller}

As expected, the selected model has a smaller AIC than the full model.
In fact, the selected model has the minimum AIC of all possible main effects models.

```{python}
print(f'AIC: {full_model.aic}')
```

```{python}
print(f'AIC: {selected_model.aic}')
```

------------------------------------------------------------------------

## Parismony {.smaller}

::: columns
::: {.column width="50%"}
::: question
Look at the variables in the full and the selected model.
Can you guess why some of them may have been dropped?
Remember: We like parsimonious models.
:::
:::

::: {.column width="50%"}
| variable         | selected |
|------------------|:--------:|
| chocolate        |    x     |
| fruity           |    x     |
| caramel          |          |
| peanutyalmondy   |    x     |
| nougat           |          |
| crispedricewafer |    x     |
| hard             |    x     |
| bar              |          |
| pluribus         |          |
| sugarpercent     |    x     |
| pricepercent     |          |
:::
:::

# Prediction

## New observation {.smaller}

To make a prediction for a new observation we need to create a data frame with that observation.

::: question
Suppose we want to make a prediction for a candy that contains chocolate, isn't fruity, has no peanuts or almonds, has a wafer, isn't hard, and has a sugar content in the 20th percentile.

<br>

The following will result in an incorrect prediction.
Why?
How would you correct it?
:::

```{python}
new_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [20]})
```

## New observation, corrected {.smaller}

```{python}
new_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [0.20]})
new_candy
```

## Prediction {.smaller}

```{python}
prediction = selected_model.predict(new_candy)
print(f'Prediction: {prediction}')
```

## Uncertainty around prediction {.smaller}

::: fragment
-   Confidence interval around $\hat{y}$ for new data (average win percentage for candy types with the given characteristics):

```{python}
confidence_interval = selected_model.get_prediction(new_candy).conf_int()
print(f'Confidence Interval: {confidence_interval}')
```
:::

::: fragment
-   Prediction interval around $\hat{y}$ for new data (predicted score for an individual type of candy with the given characteristics ):

```{python}
prediction_interval = selected_model.get_prediction(new_candy).summary_frame(alpha=0.05)
print(f'Prediction Interval: {prediction_interval}')
```
:::
