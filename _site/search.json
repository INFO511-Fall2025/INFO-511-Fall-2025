[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "",
    "text": "This course presents fundamental aspects of data science, including Python programming (e.g., data collection, cleaning, visualization), statistics, and mathematics (e.g., linear algebra and calculus). The course establishes the foundation for advanced data-intensive classes, providing both theoretical understanding and practical knowledge essential for comprehending Data Science and its applications.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "",
    "text": "This course presents fundamental aspects of data science, including Python programming (e.g., data collection, cleaning, visualization), statistics, and mathematics (e.g., linear algebra and calculus). The course establishes the foundation for advanced data-intensive classes, providing both theoretical understanding and practical knowledge essential for comprehending Data Science and its applications.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-offering",
    "href": "course-syllabus.html#course-offering",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course offering",
    "text": "Course offering\n468-2251-1 INFO 511 201 – Fundamentals of Data Science",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#instructor-information",
    "href": "course-syllabus.html#instructor-information",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Instructor Information",
    "text": "Instructor Information\n\nInstructor\nDr. Shannon McWaters,\nInstructor,\nSchool of Information\nsmcwaters@arizona.edu\nOffice Hours: By appointment via zoom",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Prerequisites",
    "text": "Prerequisites\nStudents should be comfortable with mathematical functions of one and two variables, should have at least some familiarity with basic concepts of probability and experience with a programming language.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-format",
    "href": "course-syllabus.html#course-format",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course Format",
    "text": "Course Format\nAsynchronous online lectures. Videos of each lecture will be available no later than the Monday of each week by 9am AZ time.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-objective",
    "href": "course-syllabus.html#course-objective",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course Objective",
    "text": "Course Objective\nThis course will (1) discuss effective approaches for acquiring, manipulating, and analyzing data, (2) outline fundamental concepts in programming such as variables, data types, and control structures, (3) discuss the use of Python libraries that are central for data science (e.g., NumPy, Matplotlib), (4) present a solid foundation in descriptive statistics, probability and (5) discuss the mathematical foundations that are essential for data science.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nRecognize the skills required to perform data science tasks from data acquisition to storytelling with data.\nReview fundamental topics in linear algebra and calculus for data science.\nUse Python programming techniques to prepare, visualize, and transform data.\nDemonstrate an understanding of how data science projects are approached.\nCommunicate and present data science projects and results.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Textbooks",
    "text": "Textbooks\nThe following are freely available through the UA’s library system.\n\nData Science from Scratch first principles with Python. Joel Grus. O’Reilly, 2nd edition, 2019. (Main textbook)\nEssential Math for Data Science.\n\nAll remaining books are freely available online.\n\nPython for Data Analysis. Wes McKinneyz. Python for Data Analysis. O’Reilly, 3rd edition 2023.\nPractical Statistics for Data Science. Peter Bruce, Andrew Bruce, Peter Gedeck. O’Rielly, 2016.\nIntroduction to Statistical Learning with Applications in Python. James Garth, Witten Daniela, Hastie Trevor, Tibshirani Robert. Springer, 2021/2023.\nPython Companion to Statistical Thinking in the 21st Century. Russell A. Poldrack.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-competencies",
    "href": "course-syllabus.html#course-competencies",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course competencies",
    "text": "Course competencies\nThis course is a core requirement for the MS in Data Science. It will help you master the following competencies:\n\nMS1. Students will establish the ability to exercise the four key techniques of computational thinking: decomposition, pattern recognition, abstraction, and algorithms.\nMS2. Students will obtain the skills of collecting, manipulating, and analyzing different types of data at different scales, and interpreting the results properly.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-schedule",
    "href": "course-syllabus.html#course-schedule",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course Schedule",
    "text": "Course Schedule\nAn up-to-date schedule, assignments, and due dates can be found on the course website: info511-fall2025.netlify.app.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-community",
    "href": "course-syllabus.html#course-community",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course Community",
    "text": "Course Community\n\nU of A Community Standard\nAll students must adhere to the U of A Student Rights & Responsibilities: The University of Arizona is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with U of A’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you have a name that differs from those that appear in your official U of A records, please let me know! You’ll be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website: info511-fall2025.netlify.app.\nI will regularly send course announcements via email and Slack, make sure to check one or the other of these regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course Slack. There is a chance another student has already asked a similar question, so please check the other posts on Slack before adding a new question. If you know the answer to a question posted on Slack, I encourage you to respond!\n\nCheck out the Support page for more resources.\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#lectures",
    "href": "course-syllabus.html#lectures",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Lectures",
    "text": "Lectures\nThe goal of the lectures is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to meaningfully contribute to in-class exercises and discussion.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. See the U of A Libraries loaner technology if you need a loaner laptop.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#activities-assessment",
    "href": "course-syllabus.html#activities-assessment",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on five components: application exercises, assignments, and final project.\n\nApplication exercises\nEach due Friday the following week after assigned, there will be 16 in total\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice apply the statistical concepts and code introduced in the prepare assignment. These AEs are due by the end of the week of the corresponding lecture period (Friday at midnight). To submit the AEs all you need to do is to push your work to your GitHub repo.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs (will result in full credit for AEs in the final course grade.\n\n\nHomework assignments\nThere will be 6 in total\nIn homework assignments, you will apply what you’ve learned in the videos and during lectures to complete data analysis tasks. You may discuss homework assignments with other students; however, homeworks should be completed and submitted individually. Homework assignments must be typed up using the provided ds.py scripts, all work must be pushed to your GitHub repository for the homework by the deadline.\nHomework assignments are due at 5pm AZ time on the indicated due date.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nSummative assignments\nThere will be two summative assignments in this course. Each assignment will be open-note take-home. Through these assignments you have the opportunity to demonstrate what you’ve learned in the course thus far. The assignments will focus on both conceptual understanding of the content and application through analysis and computational tasks. The content of the assignment will be related to the content in videos and reading assignments, lectures, application exercises, and homework assignments.\nMore detail about the assignments will be given during the semester.\n\n\nProject\nThe purpose of the project is to apply what you’ve learned throughout the semester to analyze an interesting data-driven research question. The project will be completed individually, working through codabench to meet a benchmark. The top 5 individuals will receive 5% extra credit in the class. The write-up will be due on the same day as the end of the competition. More to come on this.\nYou cannot pass this course if you have not completed the project.\nMore information about the project will be provided during the semester.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nHomeworks (6, lowest dropped)\n30%\n\n\nProject\n25%\n\n\nSummative assignment 1\n10%\n\n\nSummative assignment 2\n10%\n\n\nApplication Exercises (13 required)\n25%\n\n\n\nWhile there are no specific points allocated to participation, we will be recording your participation (mainly via slack) in periodically throughout the semester, and this information will be used as “extra credit” if you’re in between two grades and a minor bump would help. For reference, this would equate to a 1-2% bump.\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 90\n\n\nB\n80 - 89.99\n\n\nC\n70 - 79.99\n\n\nD\n60 - 69.99\n\n\nE\n50-59.99\n\n\nF\n&lt; 50\n\n\n\nThese are upper bounds for grade cutoffs, depending on the class performance the cutoffs may be lowered but they won’t be increased.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework. The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an assignment, for example).\nDon’t procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\nPlease abide by the following as you work on assignments in this course:\n\nCollaboration: No work is permitted to be completed collaboratively.\n\nYou may discuss homework assignments with other students; however, you may not directly share (or copy) code or write up with other students. Unauthorized sharing (or copying) of the code or write up will be considered a violation for all students involved, resulting in a 0 for the assignment.\nYou may not discuss or otherwise work with others on the assignments. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the assignment date.\nFor the project, collaboration within teams is not only allowed, but expected. Communication between individuals at a high level is also allowed however you may not share code or components of the project across teams.\nOn assignments you may not directly share work (including code) with another student in this class.\n\nOnline resources: I am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of generative artificial intelligence (AI): You should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course1:\n\nCognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\nEthical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you.\n\nYou are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#college-of-information-science-academic-integrity-policy",
    "href": "course-syllabus.html#college-of-information-science-academic-integrity-policy",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "College of Information Science Academic Integrity Policy",
    "text": "College of Information Science Academic Integrity Policy\nThis policy agreed upon by faculty in the College of Information Science at the University of Arizona (InfoSci) applies in addition to the Dean of Students’ Code of Academic Integrity.\nStudents in courses at the U of A InfoSci are expected to maintain rigor in their academic performance with intent to learn, practice, and overcome challenges toward personal growth and enrichment. As future professionals in digital environments, InfoSci students are also expected to exercise transparency and integrity in collaborations and in the use of tools and resources that may aid completion in assignments for our courses.\nConsider the following PROHIBITED practices in this course, unless the instructor has specifically written instructions or permission to do otherwise:\n\nPosting a question on an online site such as Chegg.com, and copying and pasting some or all of the response into an assessment\nPosting an assessment from the course on online sharing sites such as Course Hero. Aiding other students in violation of academic integrity is also a violation, and is potential copyright infringement.\nGenerating and submitting, in whole or in part, text or code through Artificial Intelligence such as ChatGPT, QuillBot, and text summarizers\nUsing, in whole or in part, computer code not written by the student (for example, from another student, a book, or the internet) in an assignment or project. This includes using such code in modified or unmodified form.\nSearching for solutions to projects or assignments on the internet or through other tools, when your instructor intended for you to learn the solution through exercises (e.g. Googling for the solution to a question on an assignment).\nSimultaneously submitting the same assignment as another student enrolled into the course without prior permission from the instructor\n\nExceptions: Clear Instructions will be Provided\nIn any cases in which this course requires or permits students to use practices in the list above, clear written instructions will specify the tools allowed or required, so students can be certain they are working as instructed. See the U of A InfoSci Academic Integrity Policy, the U of A Code of Academic Integrity and Syllabus policy for more information.\n\nLLMs and ChatGPT\nLarge language models (LLMs) like ChatGPT are a type of artificial intelligence (AI) engine that can look like it generates the code you need for Python homeworks and short answer questions. You are encouraged to use ChatGPT to debug code and experiment. However, abuse of ChatGPT can be traced (e.g., failing to give credit or cite ChatGPT when it is used) which could result in your suspension or termination from the course and even your program of study. Keep in mind, too, that while the code may appear legitimate, early studies have shown ChatGPT is not all that accurate with sophisticated coding. Exercise your scholarly discretion and maintain a sense of integrity in your statistical learning journey.\nSee my additional policies on this subject above.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#late-work-extensions",
    "href": "course-syllabus.html#late-work-extensions",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Late work & extensions",
    "text": "Late work & extensions\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework assignment will be dropped to accommodate such circumstances.\n\nhomeworks may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for other assessments in the course.\nThere is no late work accepted for assignments.\nThe late work policy for the project will be provided with the project instructions.\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing an application exercise or homework assignment by the stated due date, you may email me (gchism@arizona.edu) before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let me know if you need help contacting your academic dean.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#regrade-requests",
    "href": "course-syllabus.html#regrade-requests",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Regrade requests",
    "text": "Regrade requests\nRegrade requests must be submitted on GitHub within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the project presentations.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#incomplete-grade",
    "href": "course-syllabus.html#incomplete-grade",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "“Incomplete” grade",
    "text": "“Incomplete” grade\nThe grade of “I” may be awarded only at the end of a term, when all but a minor portion of the course work has been satisfactorily completed. The grade of I is not to be awarded in place of a failing grade or when the student is expected to repeat the course; in such a case, a grade other than I must be assigned. Students should make arrangements with the instructor to receive an incomplete grade before the end of the term. If the incomplete is not removed by the instructor within one year the I grade will revert to a failing grade.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#tutoring",
    "href": "course-syllabus.html#tutoring",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Tutoring",
    "text": "Tutoring\nTutoring can be found through the U of A Think Tank.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accessibility",
    "href": "course-syllabus.html#accessibility",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Accessibility",
    "text": "Accessibility\nAccessibility and Accommodations: At the University of Arizona, we strive to make learning experiences as accessible as possible. If you anticipate or experience barriers based on disability or pregnancy, please contact the Disability Resource Center (520-621-3268, https://drc.arizona.edu) to establish reasonable accommodations.\nNote: If you’ve read this far in the syllabus, email me a picture of your pet if you have one or your favorite meme!",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#additional-university-policies",
    "href": "course-syllabus.html#additional-university-policies",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Additional university policies",
    "text": "Additional university policies\nAdditional policies can be found at this link (please read through them): https://catalog.arizona.edu/syllabus-policies",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#safety-on-campus-and-in-the-classroom",
    "href": "course-syllabus.html#safety-on-campus-and-in-the-classroom",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Safety on Campus and in the Classroom",
    "text": "Safety on Campus and in the Classroom\nFor a list of emergency procedures for all types of incidents, please visit the website of the Critical Incident Response Team (CIRT): https://cirt.arizona.edu/case-emergency/overview\nAlso watch the video available at https://arizona.sabacloud.com/Saba/Web_spf/NA7P1PRD161/common/learningeventdetail/crtfy000000000003560",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Important dates",
    "text": "Important dates\n\nWednesday, January 15: Classes begin\nMonday, January 20: Martin Luther King Jr. Day\nWednesday, January 22: Drop/add ends\nTuesday, February 11: Last day to drop without a W (withdraw)\nSaturday, March 8 - Sunday, March 16: Spring recess ☀️\nTuesday, April 01: Last day to withdraw from a class online through UAccess\nWednesday, May 07: Last day of class, no registration changes can be made\nWednesday, May 14: Project reports, due by 5pm AZ time\n\nFor more important dates, see the full U of A Academic Calendar.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#graduate-student-resources",
    "href": "course-syllabus.html#graduate-student-resources",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Graduate Student Resources",
    "text": "Graduate Student Resources\nUniversity of Arizona’s Basic Needs Resources page for graduate students: http://basicneeds.arizona.edu/index.html",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "INFO 511 - Fundamentals of Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "computing/computing-git.html",
    "href": "computing/computing-git.html",
    "title": "Setting up Git",
    "section": "",
    "text": "Git is a distributed version control system that allows developers to track changes in their code, collaborate with others, and manage different versions of their projects. GitHub, on the other hand, is a web-based platform built around Git’s functionality, offering a place to host repositories, collaborate with others, and provide tools for code review, project management, and community interaction. While Git provides the underlying version control capabilities, GitHub enhances these capabilities with a user-friendly interface and additional features. Together, they are crucial for modern software development, fostering collaboration, ensuring code integrity, and enabling the open-source movement.",
    "crumbs": [
      "Computing",
      "Setting up Git"
    ]
  },
  {
    "objectID": "computing/computing-git.html#git-vs.-github",
    "href": "computing/computing-git.html#git-vs.-github",
    "title": "Setting up Git",
    "section": "",
    "text": "Git is a distributed version control system that allows developers to track changes in their code, collaborate with others, and manage different versions of their projects. GitHub, on the other hand, is a web-based platform built around Git’s functionality, offering a place to host repositories, collaborate with others, and provide tools for code review, project management, and community interaction. While Git provides the underlying version control capabilities, GitHub enhances these capabilities with a user-friendly interface and additional features. Together, they are crucial for modern software development, fostering collaboration, ensuring code integrity, and enabling the open-source movement.",
    "crumbs": [
      "Computing",
      "Setting up Git"
    ]
  },
  {
    "objectID": "computing/computing-git.html#installing-git",
    "href": "computing/computing-git.html#installing-git",
    "title": "Setting up Git",
    "section": "Installing Git",
    "text": "Installing Git\n\nFor Mac:\n\nCheck Existing Installation: Since macOS might already have Git installed, open Terminal and type:\ngit --version\nIf Git is installed, this command will return the version number. If not, proceed to the next steps.\nInstall Homebrew (if not already installed): Homebrew is a package manager for macOS that makes it easy to install software. In Terminal, type:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nInstall Git via Homebrew: After installing Homebrew, you can easily install Git with:\nbrew install git\n\n\n\nWindows (PC)\nFor Windows (PC):\n\nDownload the Installer: Go to Git’s official website and download the Windows installer.\nRun the Installer: Execute the downloaded .exe file. This will open the installation wizard.\nInstallation Settings: During installation, you’ll be presented with several options. For most users, the default settings will be adequate. However, you can customize them based on your preferences.\nFinish the Installation: Click through the rest of the setup steps, and Git will be installed.\nOpen Git Bash or Command Prompt: Once installed, you can use Git Bash (a Git-specific command terminal) or the regular Command Prompt to use Git.",
    "crumbs": [
      "Computing",
      "Setting up Git"
    ]
  },
  {
    "objectID": "computing/computing-git.html#setting-up-git",
    "href": "computing/computing-git.html#setting-up-git",
    "title": "Setting up Git",
    "section": "Setting up Git",
    "text": "Setting up Git\n\nTerminal\nSetting Up Git Configurations in a terminal:\n\nOpen a new terminal: Ctrl + Shift + N (Windows); Cmd + Space to open spotlight search, type terminal and hit return.\nSet Git Configurations: In the terminal, set your email and name which will be used for commits:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n\n\n\nVS Code\nSetting Up Git Configurations in VS Code:\n\nOpen VS Code and ensure you have the Git extension installed. By default, VS Code comes with it.\nSet Git Configurations: In the terminal, set your email and name which will be used for commits:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n\nSetting Up Token-Based Authentication for GitHub:\n\nGenerate a New Token on GitHub:\n\nGo to your GitHub settings (click your profile picture in the top right &gt; Settings).\nIn the left sidebar, click on Developer settings.\nClick on Personal access tokens, then Generate new token.\nGive your token a name, set the necessary scopes (permissions). For typical Git operations, you’ll need repo, workflow, and write:packages, read:packages, delete:packages (for package management), and user (for account details).\nClick Generate token at the bottom.\n\nCopy the Generated Token: Once generated, you’ll see the token value. Make sure to copy the token now as you won’t be able to view it again.\n(Optional) Use Token in VS Code: When you push or pull from a GitHub repository, VS Code will prompt for authentication. Instead of your GitHub password, you’ll provide the token you just generated.\nIf you previously saved your credentials and VS Code isn’t prompting for authentication, you might need to update or remove your old credentials.\nFor Mac: If you’re on a Mac and had previously saved your credentials in the Keychain, you can update them:\n\nOpen Keychain Access, which you can find with Spotlight.\nIn Keychain Access, search for github.com.\nFind the internet password entry for github.com and edit or delete it.\n(Optional) The next time you push/pull from VS Code, you’ll be prompted for your username and the new token.\n\nFor Windows: If you’re on Windows and had previously saved your credentials:\n\nGo to the Control Panel &gt; User Accounts &gt; Credential Manager &gt; Windows Credentials.\nFind the credentials related to GitHub and edit or remove them.\n(Optional) The next time you push/pull from VS Code, you’ll be prompted for your username and the new token.",
    "crumbs": [
      "Computing",
      "Setting up Git"
    ]
  },
  {
    "objectID": "computing/computing-quarto.html",
    "href": "computing/computing-quarto.html",
    "title": "Setting up Quarto",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system built on Pandoc, the universal document converter. It is designed to create dynamic and reproducible documents, presentations, and reports. Quarto extends the functionality of Markdown by integrating with computational tools like Jupyter, R, and Python, allowing users to weave together narrative text and code in a single document. This integration enables the direct embedding of code outputs (like graphs and tables) into the final document, which is especially useful in data science and academic research."
  },
  {
    "objectID": "computing/computing-quarto.html#quarto",
    "href": "computing/computing-quarto.html#quarto",
    "title": "Setting up Quarto",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system built on Pandoc, the universal document converter. It is designed to create dynamic and reproducible documents, presentations, and reports. Quarto extends the functionality of Markdown by integrating with computational tools like Jupyter, R, and Python, allowing users to weave together narrative text and code in a single document. This integration enables the direct embedding of code outputs (like graphs and tables) into the final document, which is especially useful in data science and academic research."
  },
  {
    "objectID": "computing/computing-quarto.html#installing-quarto",
    "href": "computing/computing-quarto.html#installing-quarto",
    "title": "Setting up Quarto",
    "section": "Installing Quarto",
    "text": "Installing Quarto\n\nFor all systems:\n\nCheck the newest version: quarto.org has a great Get Started page\nIf Quarto is installed, this command in a terminal will return the version number. If not, proceed to the next steps.\nquarto -v \nInstall Homebrew (if not already installed): Homebrew is a package manager for macOS that makes it easy to install software. In Terminal, type:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nInstall Git via Homebrew: After installing Homebrew, you can easily install Git with:\nbrew install git\n\n\n\nWindows (PC)\nFor Windows (PC):\n\nDownload the Installer: Go to Git’s official website and download the Windows installer.\nRun the Installer: Execute the downloaded .exe file. This will open the installation wizard.\nInstallation Settings: During installation, you’ll be presented with several options. For most users, the default settings will be adequate. However, you can customize them based on your preferences.\nFinish the Installation: Click through the rest of the setup steps, and Git will be installed.\nOpen Git Bash or Command Prompt: Once installed, you can use Git Bash (a Git-specific command terminal) or the regular Command Prompt to use Git."
  },
  {
    "objectID": "computing/computing-quarto.html#setting-up-git",
    "href": "computing/computing-quarto.html#setting-up-git",
    "title": "Setting up Quarto",
    "section": "Setting up Git",
    "text": "Setting up Git\n\nTerminal\nSetting Up Git Configurations in a terminal:\n\nOpen a new terminal: Ctrl + Shift + N (Windows); Cmd + Space to open spotlight search, type terminal and hit return.\nSet Git Configurations: In the terminal, set your email and name which will be used for commits:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n\n\n\nVS Code\nSetting Up Git Configurations in VS Code:\n\nOpen VS Code and ensure you have the Git extension installed. By default, VS Code comes with it.\nSet Git Configurations: In the terminal, set your email and name which will be used for commits:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n\nSetting Up Token-Based Authentication for GitHub:\n\nGenerate a New Token on GitHub:\n\nGo to your GitHub settings (click your profile picture in the top right &gt; Settings).\nIn the left sidebar, click on Developer settings.\nClick on Personal access tokens, then Generate new token.\nGive your token a name, set the necessary scopes (permissions). For typical Git operations, you’ll need repo, workflow, and write:packages, read:packages, delete:packages (for package management), and user (for account details).\nClick Generate token at the bottom.\n\nCopy the Generated Token: Once generated, you’ll see the token value. Make sure to copy the token now as you won’t be able to view it again.\n(Optional) Use Token in VS Code: When you push or pull from a GitHub repository, VS Code will prompt for authentication. Instead of your GitHub password, you’ll provide the token you just generated.\nIf you previously saved your credentials and VS Code isn’t prompting for authentication, you might need to update or remove your old credentials.\nFor Mac: If you’re on a Mac and had previously saved your credentials in the Keychain, you can update them:\n\nOpen Keychain Access, which you can find with Spotlight.\nIn Keychain Access, search for github.com.\nFind the internet password entry for github.com and edit or delete it.\n(Optional) The next time you push/pull from VS Code, you’ll be prompted for your username and the new token.\n\nFor Windows: If you’re on Windows and had previously saved your credentials:\n\nGo to the Control Panel &gt; User Accounts &gt; Credential Manager &gt; Windows Credentials.\nFind the credentials related to GitHub and edit or remove them.\n(Optional) The next time you push/pull from VS Code, you’ll be prompted for your username and the new token."
  },
  {
    "objectID": "computing/computing-access.html",
    "href": "computing/computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA313 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA313 under My reservations to access the RStudio instance you’ll use for the course."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "The short answer is, yes! But you will need to install a specific versions of Python, Conda, and Jupyter Lab for everything to work as expected. You will also need to install the Python libraries we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TA are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is one pathway (but see the computing documentation on another, more guaranteed way):\n\nDownload and install Python 3.12.0: https://www.python.org/downloads/\nDownload and install Miniconda: https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html\nInstall Jupyter Lab: https://jupyter.org/install\nInstall VSCode: https://code.visualstudio.com/\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages in the terminal with pip install ___\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-python-and-vscode-instead-of-using-jupyter-lab-online",
    "href": "course-faq.html#can-i-use-a-local-install-of-python-and-vscode-instead-of-using-jupyter-lab-online",
    "title": "FAQ",
    "section": "",
    "text": "The short answer is, yes! But you will need to install a specific versions of Python, Conda, and Jupyter Lab for everything to work as expected. You will also need to install the Python libraries we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TA are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is one pathway (but see the computing documentation on another, more guaranteed way):\n\nDownload and install Python 3.12.0: https://www.python.org/downloads/\nDownload and install Miniconda: https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html\nInstall Jupyter Lab: https://jupyter.org/install\nInstall VSCode: https://code.visualstudio.com/\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages in the terminal with pip install ___\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "slides/07-wrangling.html#study-tips-for-the-exam",
    "href": "slides/07-wrangling.html#study-tips-for-the-exam",
    "title": "Data wrangling",
    "section": "Study tips for the exam",
    "text": "Study tips for the exam\n\n\nGo over lecture materials and application exercises\nReview labs and feedback you’ve received so far\nDo the exercises at the end of readings from both books\nDo the exam review (to be posted on Friday)"
  },
  {
    "objectID": "slides/07-wrangling.html#frequently-asked-question",
    "href": "slides/07-wrangling.html#frequently-asked-question",
    "title": "Data wrangling",
    "section": "Frequently asked question",
    "text": "Frequently asked question\nIs there a limit to a DataFrame size?\nNo, a DataFrame can be any number of rows or columns. However, when you print it, it will only print the first few rows and the columns that fit across the screen.\nIf you want to see more rows and columns, you can:\n\nOpen it in the data viewer with df.head(n)\nExplicitly print more rows with, e.g., print(df.head(25))\nExplicitly select or rearrange columns"
  },
  {
    "objectID": "slides/07-wrangling.html#options-for-a-dataframe",
    "href": "slides/07-wrangling.html#options-for-a-dataframe",
    "title": "Data wrangling",
    "section": "Options for a DataFrame",
    "text": "Options for a DataFrame\n\nDefaultprint()SubsettingRelocating\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv('data/midwest.csv')\nprint(df)\n\n      PID     county state   area  poptotal   popdensity  popwhite  popblack  \\\n0     561      ADAMS    IL  0.052     66090  1270.961540     63917      1702   \n1     562  ALEXANDER    IL  0.014     10626   759.000000      7054      3496   \n2     563       BOND    IL  0.022     14991   681.409091     14477       429   \n3     564      BOONE    IL  0.017     30806  1812.117650     29344       127   \n4     565      BROWN    IL  0.018      5836   324.222222      5264       547   \n..    ...        ...   ...    ...       ...          ...       ...       ...   \n432  3048   WAUKESHA    WI  0.034    304715  8962.205880    298313      1096   \n433  3049    WAUPACA    WI  0.045     46104  1024.533330     45695        22   \n434  3050   WAUSHARA    WI  0.037     19385   523.918919     19094        29   \n435  3051  WINNEBAGO    WI  0.035    140320  4009.142860    136822       697   \n436  3052       WOOD    WI  0.048     73605  1533.437500     72157        90   \n\n     popamerindian  popasian  ...  percollege  percprof  poppovertyknown  \\\n0               98       249  ...   19.631392  4.355859            63628   \n1               19        48  ...   11.243308  2.870315            10529   \n2               35        16  ...   17.033819  4.488572            14235   \n3               46       150  ...   17.278954  4.197800            30337   \n4               14         5  ...   14.475999  3.367680             4815   \n..             ...       ...  ...         ...       ...              ...   \n432            672      2699  ...   35.396784  7.667090           299802   \n433            125        92  ...   16.549869  3.138596            44412   \n434             70        43  ...   15.064584  2.620907            19163   \n435            685      1728  ...   24.995504  5.659847           133950   \n436            481       722  ...   21.666382  4.583725            72685   \n\n     percpovertyknown  percbelowpoverty  percchildbelowpovert  \\\n0           96.274777         13.151443             18.011717   \n1           99.087145         32.244278             45.826514   \n2           94.956974         12.068844             14.036061   \n3           98.477569          7.209019             11.179536   \n4           82.505140         13.520249             13.022889   \n..                ...               ...                   ...   \n432         98.387674          3.121060              3.785820   \n433         96.330036          8.488697             10.071411   \n434         98.854785         13.786985             20.050708   \n435         95.460376          8.804031             10.592031   \n436         98.750085          8.525831             11.162997   \n\n     percadultpoverty  percelderlypoverty  inmetro  category  \n0           11.009776           12.443812        0       AAR  \n1           27.385647           25.228976        0       LHR  \n2           10.852090           12.697410        0       AAR  \n3            5.536013            6.217047        1       ALU  \n4           11.143211           19.200000        0       AAR  \n..                ...                 ...      ...       ...  \n432          2.590061            4.085479        1       HLU  \n433          6.953799           10.338641        0       AAR  \n434         11.695784           11.804558        0       AAR  \n435          8.660587            6.661094        1       HAU  \n436          7.375656            7.882918        0       AAR  \n\n[437 rows x 28 columns]\n\n\n\n\n\nprint(df.head(13))\n\n    PID     county state   area  poptotal   popdensity  popwhite  popblack  \\\n0   561      ADAMS    IL  0.052     66090  1270.961540     63917      1702   \n1   562  ALEXANDER    IL  0.014     10626   759.000000      7054      3496   \n2   563       BOND    IL  0.022     14991   681.409091     14477       429   \n3   564      BOONE    IL  0.017     30806  1812.117650     29344       127   \n4   565      BROWN    IL  0.018      5836   324.222222      5264       547   \n5   566     BUREAU    IL  0.050     35688   713.760000     35157        50   \n6   567    CALHOUN    IL  0.017      5322   313.058824      5298         1   \n7   568    CARROLL    IL  0.027     16805   622.407407     16519       111   \n8   569       CASS    IL  0.024     13437   559.875000     13384        16   \n9   570  CHAMPAIGN    IL  0.058    173025  2983.189660    146506     16559   \n10  571  CHRISTIAN    IL  0.042     34418   819.476190     34176        82   \n11  572      CLARK    IL  0.030     15921   530.700000     15842        10   \n12  573       CLAY    IL  0.028     14460   516.428571     14403         4   \n\n    popamerindian  popasian  ...  percollege   percprof  poppovertyknown  \\\n0              98       249  ...   19.631392   4.355859            63628   \n1              19        48  ...   11.243308   2.870315            10529   \n2              35        16  ...   17.033819   4.488572            14235   \n3              46       150  ...   17.278954   4.197800            30337   \n4              14         5  ...   14.475999   3.367680             4815   \n5              65       195  ...   18.904624   3.275891            35107   \n6               8        15  ...   11.917388   3.209601             5241   \n7              30        61  ...   16.197121   3.055727            16455   \n8               8        23  ...   14.107649   3.206799            13081   \n9             331      8033  ...   41.295808  17.757448           154934   \n10             51        89  ...   13.567226   3.089998            33788   \n11             26        36  ...   15.110863   2.776225            15615   \n12             17        29  ...   13.683010   2.788432            14248   \n\n    percpovertyknown  percbelowpoverty  percchildbelowpovert  \\\n0          96.274777         13.151443             18.011717   \n1          99.087145         32.244278             45.826514   \n2          94.956974         12.068844             14.036061   \n3          98.477569          7.209019             11.179536   \n4          82.505140         13.520249             13.022889   \n5          98.372002         10.399635             14.158819   \n6          98.478016         15.149781             13.787761   \n7          97.917287         11.710726             17.225462   \n8          97.350599         13.875086             17.994784   \n9          89.544286         15.572437             14.132234   \n10         98.169562         11.708299             16.320612   \n11         98.078010         12.007685             15.321547   \n12         98.533887         16.774284             20.582578   \n\n    percadultpoverty  percelderlypoverty  inmetro  category  \n0          11.009776           12.443812        0       AAR  \n1          27.385647           25.228976        0       LHR  \n2          10.852090           12.697410        0       AAR  \n3           5.536013            6.217047        1       ALU  \n4          11.143211           19.200000        0       AAR  \n5           8.179287           11.008586        0       AAR  \n6          12.932331           21.085271        0       LAR  \n7          10.027037            9.525052        0       AAR  \n8          11.914343           13.660180        0       AAR  \n9          17.562728            8.105017        1       HAU  \n10          9.569700           11.490641        0       AAR  \n11         10.131775           12.595420        0       AAR  \n12         14.464114           17.670078        0       LAR  \n\n[13 rows x 28 columns]\n\n\n\n\n\nselected_columns = df[['county', 'state', 'percbelowpoverty', 'percollege']]\nprint(selected_columns)\n\n        county state  percbelowpoverty  percollege\n0        ADAMS    IL         13.151443   19.631392\n1    ALEXANDER    IL         32.244278   11.243308\n2         BOND    IL         12.068844   17.033819\n3        BOONE    IL          7.209019   17.278954\n4        BROWN    IL         13.520249   14.475999\n..         ...   ...               ...         ...\n432   WAUKESHA    WI          3.121060   35.396784\n433    WAUPACA    WI          8.488697   16.549869\n434   WAUSHARA    WI         13.786985   15.064584\n435  WINNEBAGO    WI          8.804031   24.995504\n436       WOOD    WI          8.525831   21.666382\n\n[437 rows x 4 columns]\n\n\n\n\n\nrelocated_columns = df[['county', 'state', 'percbelowpoverty', 'percollege', *df.columns.difference(['county', 'state', 'percbelowpoverty', 'percollege'])]]\nprint(relocated_columns)\n\n        county state  percbelowpoverty  percollege   PID   area category  \\\n0        ADAMS    IL         13.151443   19.631392   561  0.052      AAR   \n1    ALEXANDER    IL         32.244278   11.243308   562  0.014      LHR   \n2         BOND    IL         12.068844   17.033819   563  0.022      AAR   \n3        BOONE    IL          7.209019   17.278954   564  0.017      ALU   \n4        BROWN    IL         13.520249   14.475999   565  0.018      AAR   \n..         ...   ...               ...         ...   ...    ...      ...   \n432   WAUKESHA    WI          3.121060   35.396784  3048  0.034      HLU   \n433    WAUPACA    WI          8.488697   16.549869  3049  0.045      AAR   \n434   WAUSHARA    WI         13.786985   15.064584  3050  0.037      AAR   \n435  WINNEBAGO    WI          8.804031   24.995504  3051  0.035      HAU   \n436       WOOD    WI          8.525831   21.666382  3052  0.048      AAR   \n\n     inmetro  percadultpoverty  percamerindan  ...  percwhite  popadults  \\\n0          0         11.009776       0.148283  ...  96.712059      43298   \n1          0         27.385647       0.178807  ...  66.384340       6724   \n2          0         10.852090       0.233473  ...  96.571276       9669   \n3          1          5.536013       0.149322  ...  95.254171      19272   \n4          0         11.143211       0.239890  ...  90.198766       3979   \n..       ...               ...            ...  ...        ...        ...   \n432        1          2.590061       0.220534  ...  97.899020     195837   \n433        0          6.953799       0.271126  ...  99.112875      30109   \n434        0         11.695784       0.361104  ...  98.498839      13316   \n435        1          8.660587       0.488170  ...  97.507127      88960   \n436        0          7.375656       0.653488  ...  98.032742      46796   \n\n     popamerindian  popasian  popblack   popdensity  popother  \\\n0               98       249      1702  1270.961540       124   \n1               19        48      3496   759.000000         9   \n2               35        16       429   681.409091        34   \n3               46       150       127  1812.117650      1139   \n4               14         5       547   324.222222         6   \n..             ...       ...       ...          ...       ...   \n432            672      2699      1096  8962.205880      1935   \n433            125        92        22  1024.533330       170   \n434             70        43        29   523.918919       149   \n435            685      1728       697  4009.142860       388   \n436            481       722        90  1533.437500       155   \n\n     poppovertyknown  poptotal  popwhite  \n0              63628     66090     63917  \n1              10529     10626      7054  \n2              14235     14991     14477  \n3              30337     30806     29344  \n4               4815      5836      5264  \n..               ...       ...       ...  \n432           299802    304715    298313  \n433            44412     46104     45695  \n434            19163     19385     19094  \n435           133950    140320    136822  \n436            72685     73605     72157  \n\n[437 rows x 28 columns]"
  },
  {
    "objectID": "slides/07-wrangling.html#why-join",
    "href": "slides/07-wrangling.html#why-join",
    "title": "Data wrangling",
    "section": "Why join?",
    "text": "Why join?\nSuppose we want to answer questions like:\n\nIs there a relationship between\n- number of DS courses taken\n- motivation for taking course\n- …\nand performance in this course?”\n\n\nEach of these would require joining class performance data with an outside data source so we can have all relevant information (columns) in a single data frame."
  },
  {
    "objectID": "slides/07-wrangling.html#setup",
    "href": "slides/07-wrangling.html#setup",
    "title": "Data wrangling",
    "section": "Setup",
    "text": "Setup\nFor the next few slides…\n\n\n\nx = pd.DataFrame({\n    'id': [1, 2, 3],\n    'value_x': ['x1', 'x2', 'x3']\n})\nprint(x)\n\n   id value_x\n0   1      x1\n1   2      x2\n2   3      x3\n\n\n\n\ny = pd.DataFrame({\n    'id': [1, 2, 4],\n    'value_y': ['y1', 'y2', 'y4']\n})\n\nprint(y)\n\n   id value_y\n0   1      y1\n1   2      y2\n2   4      y4"
  },
  {
    "objectID": "slides/07-wrangling.html#left-join",
    "href": "slides/07-wrangling.html#left-join",
    "title": "Data wrangling",
    "section": "Left join",
    "text": "Left join\n\n\n\n\n\nleft_merged = pd.merge(x, y, on='id', how='left')\nprint(left_merged)\n\n   id value_x value_y\n0   1      x1      y1\n1   2      x2      y2\n2   3      x3     NaN"
  },
  {
    "objectID": "slides/07-wrangling.html#right-join",
    "href": "slides/07-wrangling.html#right-join",
    "title": "Data wrangling",
    "section": "Right join",
    "text": "Right join\n\n\n\n\n\nright_merged = pd.merge(x, y, on='id', how='right')\nprint(right_merged)\n\n   id value_x value_y\n0   1      x1      y1\n1   2      x2      y2\n2   4     NaN      y4"
  },
  {
    "objectID": "slides/07-wrangling.html#outer-full-join",
    "href": "slides/07-wrangling.html#outer-full-join",
    "title": "Data wrangling",
    "section": "Outer (full) join",
    "text": "Outer (full) join\n\n\n\n\n\nouter_merged = pd.merge(x, y, on='id', how='outer')\nprint(outer_merged)\n\n   id value_x value_y\n0   1      x1      y1\n1   2      x2      y2\n2   3      x3     NaN\n3   4     NaN      y4"
  },
  {
    "objectID": "slides/07-wrangling.html#inner-join",
    "href": "slides/07-wrangling.html#inner-join",
    "title": "Data wrangling",
    "section": "Inner join",
    "text": "Inner join\n\n\n\n\n\ninner_merged = pd.merge(x, y, on='id', how='inner')\nprint(inner_merged)\n\n   id value_x value_y\n0   1      x1      y1\n1   2      x2      y2"
  },
  {
    "objectID": "slides/07-wrangling.html#semi-join",
    "href": "slides/07-wrangling.html#semi-join",
    "title": "Data wrangling",
    "section": "Semi-join",
    "text": "Semi-join\n\n\n\n\n\nsemi_merged = x[x['id'].isin(y['id'])]\nprint(semi_merged)\n\n   id value_x\n0   1      x1\n1   2      x2"
  },
  {
    "objectID": "slides/07-wrangling.html#anti-join",
    "href": "slides/07-wrangling.html#anti-join",
    "title": "Data wrangling",
    "section": "Anti-join",
    "text": "Anti-join\n\n\n\n\n\nanti_merged = x[~x['id'].isin(y['id'])]\nprint(anti_merged)\n\n   id value_x\n2   3      x3\n\n\n\n\n🤮"
  },
  {
    "objectID": "slides/07-wrangling.html#pivoting-.melt",
    "href": "slides/07-wrangling.html#pivoting-.melt",
    "title": "Data wrangling",
    "section": "Pivoting (.melt())",
    "text": "Pivoting (.melt())\n\n\nData sets can’t be labeled as wide or long, but they can be made wider or longer for a certain analysis that requires a certain format.\nWhen pivoting longer, variable names that turn into values are characters by default. If you need them to be in another format, you need to explicitly make that transformation, which you can do within the melt() function."
  },
  {
    "objectID": "slides/07-wrangling.html#ae-05-majors-wrangling",
    "href": "slides/07-wrangling.html#ae-05-majors-wrangling",
    "title": "Data wrangling",
    "section": "ae-05-majors-wrangling",
    "text": "ae-05-majors-wrangling\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#setup",
    "href": "slides/13-hypothesis-testing.html#setup",
    "title": "Hypothesis testing",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nimport random\n\nsns.set(font_scale=2)\nsns.set_theme(style = \"whitegrid\")"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#how-can-we-answer-questions-using-statistics",
    "href": "slides/13-hypothesis-testing.html#how-can-we-answer-questions-using-statistics",
    "title": "Hypothesis testing",
    "section": "How can we answer questions using statistics?",
    "text": "How can we answer questions using statistics?\n\nStatistical hypothesis testing is the procedure that assesses evidence provided by the data in favor of or against some claim about the population (often about a population parameter or potential associations)"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#the-hypothesis-testing-framework",
    "href": "slides/13-hypothesis-testing.html#the-hypothesis-testing-framework",
    "title": "Hypothesis testing",
    "section": "The hypothesis testing framework",
    "text": "The hypothesis testing framework\n\n\nEi incumbit probatio qui dicit, non qui negat"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#the-hypothesis-testing-framework-1",
    "href": "slides/13-hypothesis-testing.html#the-hypothesis-testing-framework-1",
    "title": "Hypothesis testing",
    "section": "The hypothesis testing framework",
    "text": "The hypothesis testing framework\n\n\nStart with two hypotheses about the population: the null hypothesis and the alternative hypothesis.\nChoose a (representative) sample, collect data, and analyze the data.\nFigure out how likely it is to see data like what we observed, IF the null hypothesis were in fact true.\nIf our data would have been extremely unlikely if the null claim were true, then we reject it and deem the alternative claim worthy of further study. Otherwise, we cannot reject the null claim."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#organ-donation-consultants",
    "href": "slides/13-hypothesis-testing.html#organ-donation-consultants",
    "title": "Hypothesis testing",
    "section": "Organ donation consultants",
    "text": "Organ donation consultants"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#organ-donation-consultants-1",
    "href": "slides/13-hypothesis-testing.html#organ-donation-consultants-1",
    "title": "Hypothesis testing",
    "section": "Organ donation consultants",
    "text": "Organ donation consultants\nOne consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!).\n\n\nIs this a reasonable claim to make?"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#organ-donation-consultants-2",
    "href": "slides/13-hypothesis-testing.html#organ-donation-consultants-2",
    "title": "Hypothesis testing",
    "section": "Organ donation consultants",
    "text": "Organ donation consultants"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#organ-donation-consultants-3",
    "href": "slides/13-hypothesis-testing.html#organ-donation-consultants-3",
    "title": "Hypothesis testing",
    "section": "Organ donation consultants",
    "text": "Organ donation consultants\nOne consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!).\n\n\nIs there sufficient evidence to suggest that her complication rate is lower than the overall US rate?"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#the-hypothesis-testing-framework-2",
    "href": "slides/13-hypothesis-testing.html#the-hypothesis-testing-framework-2",
    "title": "Hypothesis testing",
    "section": "The hypothesis testing framework",
    "text": "The hypothesis testing framework\n\n\nStart with two hypotheses about the population: the null hypothesis and the alternative hypothesis.\nChoose a (representative) sample, collect data, and analyze the data.\nFigure out how likely it is to see data like what we observed, IF the null hypothesis were in fact true.\nIf our data would have been extremely unlikely if the null claim were true, then we reject it and deem the alternative claim worthy of further study. Otherwise, we cannot reject the null claim."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#two-competing-hypotheses",
    "href": "slides/13-hypothesis-testing.html#two-competing-hypotheses",
    "title": "Hypothesis testing",
    "section": "Two competing hypotheses",
    "text": "Two competing hypotheses\nThe null hypothesis (often denoted \\(H_0\\)) states that “nothing unusual is happening” or “there is no relationship,” etc.\n\nOn the other hand, the alternative hypothesis (often denoted \\(H_1\\) or \\(H_A\\)) states the opposite: that there is some sort of relationship.\n\n\n\nIn statistical hypothesis testing we always first assume that the null hypothesis is true and then evaluate the weight of proof we have against this claim."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#defining-the-hypotheses",
    "href": "slides/13-hypothesis-testing.html#defining-the-hypotheses",
    "title": "Hypothesis testing",
    "section": "1. Defining the hypotheses",
    "text": "1. Defining the hypotheses\nThe null and alternative hypotheses are defined for parameters.\n\nWhat will our null and alternative hypotheses be for this example?\n\n\n\n\n\\(H_0\\): the true proportion of complications among her patients is equal to the US population rate\n\\(H_1\\): the true proportion of complications among her patients is less than the US population rate\n\n\n\n\nExpressed in symbols:\n\n\\(H_0: p = 0.10\\)\n\\(H_1: p &lt; 0.10\\)\n\n\n\nwhere \\(p\\) is the true proportion of transplants with complications among her patients."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#collecting-and-summarizing-data",
    "href": "slides/13-hypothesis-testing.html#collecting-and-summarizing-data",
    "title": "Hypothesis testing",
    "section": "2. Collecting and summarizing data",
    "text": "2. Collecting and summarizing data\nWith these two hypotheses, we now take our sample and summarize the data.\nThe choice of summary statistic calculated depends on the type of data. In our example, we use the sample proportion: \\(\\widehat{p} = 3/62 \\approx 0.048\\):"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\nNext, we calculate the probability of getting data like ours, or more extreme, if \\(H_0\\) were in fact actually true.\n\nThis is a conditional probability:\n\nGiven that \\(H_0\\) is true (i.e., if \\(p\\) were actually 0.10), what would be the probability of observing \\(\\widehat{p} = 3/62\\)?”\n\n\n\n\nThis probability is known as the p-value."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-1",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-1",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\nLet’s simulate a distribution for \\(\\hat{p}\\) such that the probability of complication for each patient is 0.10 for 62 patients.\nThis null distribution for \\(\\hat{p}\\) represents the distribution of the observed proportions we might expect, if the null hypothesis were true.\n\n\nWhen sampling from the null distribution, what is the expected proportion of complications? What would the expected count be of patients experiencing complications?"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-2",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-2",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-3",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-3",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-4",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-4",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-5",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-5",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-6",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-6",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\nSupposing that the true proportion of complications is 10%, if we were to take repeated samples of 62 liver transplants, about 11.5% of them would have 3 or fewer complications.\n\nThat is, p = 0.115."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#making-a-conclusion",
    "href": "slides/13-hypothesis-testing.html#making-a-conclusion",
    "title": "Hypothesis testing",
    "section": "4. Making a conclusion",
    "text": "4. Making a conclusion\nIf it is very unlikely to observe our data (or more extreme) if \\(H_0\\) were actually true, then that might give us enough evidence to suggest that it is actually false (and that \\(H_1\\) is true).\n\nWhat is “small enough”?\n\n\n\nWe often consider a numeric cutpoint (the significance level) defined prior to conducting the analysis.\nMany analyses use \\(\\alpha = 0.05\\). This means that if \\(H_0\\) were in fact true, we would expect to make the wrong decision only 5% of the time.\n\n\n\nIf the p-value is less than \\(\\alpha\\), we say the results are statistically significant. In such a case, we would make the decision to reject the null hypothesis."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#what-do-we-conclude-when-p-ge-alpha",
    "href": "slides/13-hypothesis-testing.html#what-do-we-conclude-when-p-ge-alpha",
    "title": "Hypothesis testing",
    "section": "What do we conclude when \\(p \\ge \\alpha\\)?",
    "text": "What do we conclude when \\(p \\ge \\alpha\\)?\nIf the p-value is \\(\\alpha\\) or greater, we say the results are not statistically significant and we fail to reject the null hypothesis.\n\nImportantly, we never “accept” the null hypothesis – we performed the analysis assuming that \\(H_0\\) was true to begin with and assessed the probability of seeing our observed data or more extreme under this assumption."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#making-a-conclusion-1",
    "href": "slides/13-hypothesis-testing.html#making-a-conclusion-1",
    "title": "Hypothesis testing",
    "section": "4. Making a conclusion",
    "text": "4. Making a conclusion\n\nThere is insufficient evidence at \\(\\alpha = 0.05\\) to suggest that the consultant’s complication rate is less than the US average."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#vacation-rentals-in-tucson-az",
    "href": "slides/13-hypothesis-testing.html#vacation-rentals-in-tucson-az",
    "title": "Hypothesis testing",
    "section": "Vacation rentals in Tucson, AZ",
    "text": "Vacation rentals in Tucson, AZ\n\n\nYour friend claims that the mean price per guest per night for Airbnbs in Tucson, AZ is $100. What do you make of this statement?"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#defining-the-hypotheses-1",
    "href": "slides/13-hypothesis-testing.html#defining-the-hypotheses-1",
    "title": "Hypothesis testing",
    "section": "1. Defining the hypotheses",
    "text": "1. Defining the hypotheses\nRemember, the null and alternative hypotheses are defined for parameters, not statistics\n\nWhat will our null and alternative hypotheses be for this example?\n\n\n\n\\(H_0\\): the true mean price per guest is $100 per night\n\\(H_1\\): the true mean price per guest is NOT $100 per night\n\n\n\nExpressed in symbols:\n\n\\(H_0: \\mu = 100\\)\n\\(H_1: \\mu \\neq 100\\)\n\nwhere \\(\\mu\\) is the true population mean price per guest per night among Airbnb listings in Tucson."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#collecting-and-summarizing-data-1",
    "href": "slides/13-hypothesis-testing.html#collecting-and-summarizing-data-1",
    "title": "Hypothesis testing",
    "section": "2. Collecting and summarizing data",
    "text": "2. Collecting and summarizing data\nWith these two hypotheses, we now take our sample and summarize the data. We have a representative of 50 Airbnb listings in the file tucson.csv.\nThe choice of summary statistic calculated depends on the type of data. In our example, we use the sample proportion, \\(\\bar{x} = 116.24\\).\n\n\ntucson = pd.read_csv('data/tucson.csv')\n\nmean_price = tucson['ppg'].mean()\nprint(f\"Sample mean price: {mean_price:.2f}\")\n\nSample mean price: 116.24"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-7",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-7",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\nWe know that not every representative sample of 50 Airbnb listings in Tucson will have exactly a sample mean of exactly $116.24.\n\nHow might we deal with this variability in the sampling distribution of the mean using only the data that we have from our original sample?\n\n\nWe can take bootstrap samples, formed by sampling with replacement from our original dataset, of the same sample size as our original dataset."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-8",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-8",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\n\n\n\n\n\n\n\n\n\nMean price per guest per night: 116.24"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-9",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-9",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\n\n\n\n\n\n\n\n\n\nMean price per guest per night (Bootstrap 1): 115.20"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-10",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-10",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\n\n\n\n\n\n\n\n\n\nMean price per guest per night (Bootstrap 2): 115.08"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-11",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-11",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\n\n\n\n\n\n\n\n\n\nMean price per guest per night (Bootstrap 3): 103.20"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-12",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-12",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\n\n\n\n\n\n\n\n\n\nMean of bootstrap means: 116.24"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#shifting-the-distribution",
    "href": "slides/13-hypothesis-testing.html#shifting-the-distribution",
    "title": "Hypothesis testing",
    "section": "Shifting the distribution",
    "text": "Shifting the distribution\nWe’ve captured the variability in the sample mean among samples of size 50 from Tucson area Airbnbs, but remember that in the hypothesis testing paradigm, we must assess our observed evidence under the assumption that \\(H_0\\) is true.\n\nmean_price = bootstrap_means_df['boot_means'].mean()\nprint(f\"Sample mean price: {mean_price:.2f}\")\n\nSample mean price: 116.24\n\n\n\n\n\nWhere should the bootstrap distribution of means be centered under \\(H_0\\)?"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#shifting-the-distribution-1",
    "href": "slides/13-hypothesis-testing.html#shifting-the-distribution-1",
    "title": "Hypothesis testing",
    "section": "Shifting the distribution",
    "text": "Shifting the distribution\n\n\n\n\n\n\n\n\n\nMean of bootstrap means: 116.24"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#shifting-the-distribution-2",
    "href": "slides/13-hypothesis-testing.html#shifting-the-distribution-2",
    "title": "Hypothesis testing",
    "section": "Shifting the distribution",
    "text": "Shifting the distribution"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-13",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-13",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-14",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-14",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed"
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-15",
    "href": "slides/13-hypothesis-testing.html#assessing-the-evidence-observed-15",
    "title": "Hypothesis testing",
    "section": "3. Assessing the evidence observed",
    "text": "3. Assessing the evidence observed\nSupposing that the true mean price per guest were $100 a night, about 0.16% of bootstrap sample means were as extreme or even more so than our originally observed sample mean price per guest of $116.24.\n\nThat is, p = 0.0016."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#making-a-conclusion-2",
    "href": "slides/13-hypothesis-testing.html#making-a-conclusion-2",
    "title": "Hypothesis testing",
    "section": "4. Making a conclusion",
    "text": "4. Making a conclusion\nIf it is very unlikely to observe our data (or more extreme) if \\(H_0\\) were actually true, then that might give us enough evidence to suggest that it is actually false (and that \\(H_1\\) is true).\n\nThere is sufficient evidence at \\(\\alpha = 0.05\\) to suggest that the mean price per guest per night of Airbnb rentals in Tucson is not $100."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#ok-so-what-isnt-a-p-value",
    "href": "slides/13-hypothesis-testing.html#ok-so-what-isnt-a-p-value",
    "title": "Hypothesis testing",
    "section": "Ok, so what isn’t a p-value?",
    "text": "Ok, so what isn’t a p-value?\n\n“A p-value of 0.05 means the null hypothesis has a probability of only 5% of being true”\n\n\n“A p-value of 0.05 means there is a 95% chance or greater that the null hypothesis is incorrect”\n\n\n\nNO\n\n\n\np-values do not provide information on the probability that the null hypothesis is true given our observed data."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#what-can-go-wrong",
    "href": "slides/13-hypothesis-testing.html#what-can-go-wrong",
    "title": "Hypothesis testing",
    "section": "What can go wrong?",
    "text": "What can go wrong?\nRemember, a p-value is calculated assuming that \\(H_0\\) is true. It cannot be used to tell us how likely that assumption is correct. When we fail to reject the null hypothesis, we are stating that there is insufficient evidence to assert that it is false. This could be because…\n\n\n… \\(H_0\\) actually is true!\n… \\(H_0\\) is false, but we got unlucky and happened to get a sample that didn’t give us enough reason to say that \\(H_0\\) was false.\n\n\n\nEven more bad news, hypothesis testing does NOT give us the tools to determine which one of the two scenarios occurred."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#what-can-go-wrong-1",
    "href": "slides/13-hypothesis-testing.html#what-can-go-wrong-1",
    "title": "Hypothesis testing",
    "section": "What can go wrong?",
    "text": "What can go wrong?\nSuppose we test a certain null hypothesis, which can be either true or false (we never know for sure!). We make one of two decisions given our data: either reject or fail to reject \\(H_0\\).\n\nWe have the following four scenarios:\n\n\n\nDecision\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nFail to reject \\(H_0\\)\nCorrect decision\nType II Error\n\n\nReject \\(H_0\\)\nType I Error\nCorrect decision\n\n\n\n\n\n\\(\\alpha\\) is the probability of making a Type I error.\n\\(\\beta\\) is the probability of making a Type II error.\nThe power of a test is 1 - \\(\\beta\\): the probability that, if the null hypothesis is actually false, we correctly reject it."
  },
  {
    "objectID": "slides/13-hypothesis-testing.html#ae-09",
    "href": "slides/13-hypothesis-testing.html#ae-09",
    "title": "Hypothesis testing",
    "section": "ae-09",
    "text": "ae-09\nWork through the Tucson Airbnb prices data in an exercise on Quantifying uncertainty\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/16-linear-model-multiple-predictors.html#r-squared-r2",
    "href": "slides/16-linear-model-multiple-predictors.html#r-squared-r2",
    "title": "Linear regression with multiple predictors",
    "section": "R-squared (\\(R^2\\))",
    "text": "R-squared (\\(R^2\\))\n\nR-squared is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable or variables in a regression model.\n\n\\[\nR^2 = 1 - \\frac{RSS}{TSS}\n\\]"
  },
  {
    "objectID": "slides/16-linear-model-multiple-predictors.html#r2-broken-down",
    "href": "slides/16-linear-model-multiple-predictors.html#r2-broken-down",
    "title": "Linear regression with multiple predictors",
    "section": "\\(R^2\\) broken down",
    "text": "\\(R^2\\) broken down\n\nResidualsMean of observationsSums of squares\\(R^2\\)\n\n\n\n\nResiduals are the differences between the observed values and the predicted values from a regression model.\nIf \\(y_i\\) is an observed value and \\(\\hat{y}_i\\) is the predicted value, the residual \\(e_i\\) is given by:\n\\(e_i = y_i - \\hat{y}_i\\)\n\n\n\n\n\n\nThe mean \\(\\bar{y}\\) is the average of all observed values, calculated as\n\\(\\bar{y}=\\frac{1}{n} \\sum_{i=1}^{n}y_i\\)\nWhere \\(n\\) is the number of observations.\n\n\n\n\n\n\nThese are measures of variability within the data set.\nResidual Sum of Squares (RSS):\n\\(RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\n\n\n\n\nThis measures the total deviation of the predicted values from the observed values.\n\n\n\n\nTotal Sum of Squares (TSS):\n\\(TSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)\nThis measures the total deviation of the observed values from their mean.\n\n\n\n\n\n\nR-squared is calculated as:\n\\(R^2 = 1 - \\frac{RSS}{TSS}\\)\nThis value ranges from 0 to 1 and indicates how well the independent variables explain the variability of the dependent variable."
  },
  {
    "objectID": "slides/16-linear-model-multiple-predictors.html#adjusted-r-squared-r2_adj",
    "href": "slides/16-linear-model-multiple-predictors.html#adjusted-r-squared-r2_adj",
    "title": "Linear regression with multiple predictors",
    "section": "Adjusted R-squared (\\(R^2_{adj}\\))",
    "text": "Adjusted R-squared (\\(R^2_{adj}\\))\n\nFormulaKey pointsDegrees of freedom\n\n\n\\[\nR^2_{adj} = 1 - \\frac{RSS / df_{res}}{TSS / df_{tot}}\n\\]\n\n\n\\(df_{res}\\) represents the degrees of freedom of the residuals, which is the number of observations minus the number of predictors minus one.\n\\(df_{tot}\\)​ represents the degrees of freedom of the total variability, which is the number of observations minus one.\n\n\n\n\n\n\nPenalizes Complexity: Adjusted R-squared decreases when unnecessary predictors are added to the model, discouraging overfitting.\nComparability: It is more reliable than R-squared for comparing models with different numbers of predictors.\nValue Range: Unlike R-squared, adjusted R-squared can be negative if the model is worse than a simple mean model, though it typically ranges from 0 to 1.\n\n\n\n\n\n\n\\(df_{res}\\): Degrees of freedom related to the estimate of the population variance around the model’s predictions.\n\\(df_{tot}\\)​: Degrees of freedom related to the estimate of the population variance around the mean of the observed values."
  },
  {
    "objectID": "slides/16-linear-model-multiple-predictors.html#in-pursuit-of-occams-razor",
    "href": "slides/16-linear-model-multiple-predictors.html#in-pursuit-of-occams-razor",
    "title": "Linear regression with multiple predictors",
    "section": "In pursuit of Occam’s Razor",
    "text": "In pursuit of Occam’s Razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model.\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/11-probability.html#what-weve-done-so-far",
    "href": "slides/11-probability.html#what-weve-done-so-far",
    "title": "Intro to Probability",
    "section": "What we’ve done so far…",
    "text": "What we’ve done so far…\n\nUse visualization techniques to visualize data\n\nUse descriptive statistics to describe and summarize data\nUse data wrangling tools to manipulate data\n…all using the reproducible, shareable tools of Python, Jupyter, and git\n\nThat’s all great, but what we eventually want to do is to quantify uncertainty in order to make principled conclusions about the data"
  },
  {
    "objectID": "slides/11-probability.html#the-statistical-process",
    "href": "slides/11-probability.html#the-statistical-process",
    "title": "Intro to Probability",
    "section": "The statistical process",
    "text": "The statistical process\n\n\nStatistics is a process that converts data into useful information, where practitioners\n\n1️⃣ form a question of interest,\n\n\n2️⃣ collect and summarize data,\n\n\n3️⃣ and interpret the results."
  },
  {
    "objectID": "slides/11-probability.html#the-population-of-interest",
    "href": "slides/11-probability.html#the-population-of-interest",
    "title": "Intro to Probability",
    "section": "The population of interest",
    "text": "The population of interest\nThe population is the group we’d like to learn something about.\n\n\nWhat is the prevalence of diabetes among U.S. adults, and has it changed over time?\nDoes the average amount of caffeine vary by vendor in 12 oz. cups of coffee at U of A coffee shops?\nIs there a relationship between tumor type and five-year mortality among breast cancer patients?\n\n\n\nThe research question of interest is what we want to answer - often relating one or more numerical quantities or summary statistics.\n\n\nIf we had data from every unit in the population, we could just calculate what we wanted and be done!"
  },
  {
    "objectID": "slides/11-probability.html#sampling-from-the-population",
    "href": "slides/11-probability.html#sampling-from-the-population",
    "title": "Intro to Probability",
    "section": "Sampling from the population",
    "text": "Sampling from the population\nUnfortunately, we (usually) have to settle with a sample from the population.\n\nIdeally, the sample is representative (has similar characteristics as the population), allowing us to make conclusions that are generalizable (i.e. applicable) to the broader population of interest.\n\n\n\nWe’ll use probability and statistical inference (more on this later!) to draw conclusions about the population based on our sample."
  },
  {
    "objectID": "slides/11-probability.html#interpretations-of-probability",
    "href": "slides/11-probability.html#interpretations-of-probability",
    "title": "Intro to Probability",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\n\n“There is a 1 in 3 chance of selecting a white ball”"
  },
  {
    "objectID": "slides/11-probability.html#interpretations-of-probability-1",
    "href": "slides/11-probability.html#interpretations-of-probability-1",
    "title": "Intro to Probability",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\n\n“There is a 75% chance of rain tomorrow”"
  },
  {
    "objectID": "slides/11-probability.html#interpretations-of-probability-2",
    "href": "slides/11-probability.html#interpretations-of-probability-2",
    "title": "Intro to Probability",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\n\n“The surgery has a 50% probability of success”"
  },
  {
    "objectID": "slides/11-probability.html#interpretations-of-probability-3",
    "href": "slides/11-probability.html#interpretations-of-probability-3",
    "title": "Intro to Probability",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\n\nLong-run frequencies vs. degree of belief"
  },
  {
    "objectID": "slides/11-probability.html#what-do-we-need",
    "href": "slides/11-probability.html#what-do-we-need",
    "title": "Intro to Probability",
    "section": "What do we need?",
    "text": "What do we need?\nWe can think of probabilities as objects that model random phenomena. We’ll use three components to talk about probabilities:\n\n1️⃣ Sample space: the set of all possible outcomes\n\n\n2️⃣ Events: Subsets of the sample space, comprise any number of possible outcomes (including none of them!)\n\n\n3️⃣ Probability: Proportion of times an event would occur if we observed the random phenomenon an infinite number of times."
  },
  {
    "objectID": "slides/11-probability.html#sample-spaces",
    "href": "slides/11-probability.html#sample-spaces",
    "title": "Intro to Probability",
    "section": "Sample spaces",
    "text": "Sample spaces\nSample spaces depend on the random phenomenon in question\n\n\nTossing a single fair coin\nSum of rolling two fair six-sided dice\nGuessing the answer on a multiple choice question with choices a, b, c, d.\n\n\n\n\nWhat are the sample spaces for the random phenomena above?"
  },
  {
    "objectID": "slides/11-probability.html#events",
    "href": "slides/11-probability.html#events",
    "title": "Intro to Probability",
    "section": "Events",
    "text": "Events\nEvents are subsets of the sample space that comprise all possible outcomes from that event. These are the “plausibly reasonable” outcomes we may want to calculate the probabilities for:\n\n\nTossing a single fair coin\nSum of rolling two fair six-sided dice\nGuessing the answer on a multiple choice question with choices a, b, c, d.\n\n\n\n\n\nWhat are some examples of events for the random phenomena above?"
  },
  {
    "objectID": "slides/11-probability.html#probabilities",
    "href": "slides/11-probability.html#probabilities",
    "title": "Intro to Probability",
    "section": "Probabilities",
    "text": "Probabilities\nConsider the following possible events and their corresponding probabilities:\n\n\nGetting a head from a single fair coin toss: 0.5\nGetting a prime number sum from rolling two fair six-sided dice: 5/12\nGuessing the correct answer: 1/4\n\n\n\n\nWe’ll talk more about how we calculated these probabilities, but for now remember that probabilities are numbers describing the likelihood of each event’s occurrence, which map events to a number between 0 and 1, inclusive."
  },
  {
    "objectID": "slides/11-probability.html#set-operations",
    "href": "slides/11-probability.html#set-operations",
    "title": "Intro to Probability",
    "section": "Set operations",
    "text": "Set operations\nRemember that events are (sub)sets of the outcome space. For two sets (in this case events) \\(A\\) and \\(B\\), the most common relationships are:\n\n\nIntersection \\((A \\text{ and } B)\\): \\(A\\) and \\(B\\) both occur\nUnion \\((A \\text{ or } B)\\): \\(A\\) or \\(B\\) occurs (including when both occur)\nComplement \\((A^c)\\): \\(A\\) does not occur\n\n\n\n\nTwo sets \\(A\\) and \\(B\\) are said to be disjoint or mutually exclusive if they cannot happen at the same time, i.e. \\(A \\text{ and } B = \\emptyset\\)."
  },
  {
    "objectID": "slides/11-probability.html#combining-set-operations",
    "href": "slides/11-probability.html#combining-set-operations",
    "title": "Intro to Probability",
    "section": "Combining set operations",
    "text": "Combining set operations\n\n\nDeMorgan’s laws\n\n\nComplement of union: \\((A \\text{ or } B)^c = A^c \\text{ and } B^c\\)\nComplement of intersection: \\((A \\text{ and } B)^c = A^c \\text{ or } B^c\\)\n\n\n\nThese can be extended to more than two events"
  },
  {
    "objectID": "slides/11-probability.html#how-do-probabilities-work",
    "href": "slides/11-probability.html#how-do-probabilities-work",
    "title": "Intro to Probability",
    "section": "How do probabilities work?",
    "text": "How do probabilities work?\nKolmogorov axioms\n\n✅ The probability of any event is real number that’s \\(\\geq 0\\)\n\n\n✅ The probability of the entire sample space is 1\n\n\n✅ If \\(A\\) and \\(B\\) are disjoint events, then \\(P(A \\text{ or } B) = P(A) + P(B)\\)\n\n\n\nThe Kolmogorov axioms lead to all probabilities being between 0 and 1 inclusive, and also lead to important rules…"
  },
  {
    "objectID": "slides/11-probability.html#two-important-rules",
    "href": "slides/11-probability.html#two-important-rules",
    "title": "Intro to Probability",
    "section": "Two important rules",
    "text": "Two important rules\nSuppose we have events \\(A\\) and \\(B\\), with probabilities \\(P(A)\\) and \\(P(B)\\) of occurring. Based on the Kolmogorov axioms:\n\n\nComplement Rule: \\(P(A^c) = 1 - P(A)\\)\nInclusion-Exclusion: \\(P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\\)"
  },
  {
    "objectID": "slides/11-probability.html#practicing-with-probabilities",
    "href": "slides/11-probability.html#practicing-with-probabilities",
    "title": "Intro to Probability",
    "section": "Practicing with probabilities",
    "text": "Practicing with probabilities\n\n\n\n\n\n\n\n\n\nDid not die\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601\n\n\n\n\n\nSource: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5788283/"
  },
  {
    "objectID": "slides/11-probability.html#practicing-with-probabilities-1",
    "href": "slides/11-probability.html#practicing-with-probabilities-1",
    "title": "Intro to Probability",
    "section": "Practicing with probabilities",
    "text": "Practicing with probabilities\n\n\n\n\nDid not die\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601\n\n\n\n\n\nDefine events A = died and B = non-coffee drinker. Calculate the following for a randomly selected person in the cohort:\n\n\n\\(\\small{P(A)}\\)\n\\(\\small{P(B)}\\)\n\\(\\small{P(A \\text{ and } B)}\\)\n\\(\\small{P(A \\text{ or } B)}\\)\n\\(\\small{P(A \\text{ or } B^c)}\\)\n\n\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/09-exam-1-review.html#setup",
    "href": "slides/09-exam-1-review.html#setup",
    "title": "Exam 1 review",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\", font_scale=1.2)"
  },
  {
    "objectID": "slides/09-exam-1-review.html#explicit-vs.-implicit-type-coercion",
    "href": "slides/09-exam-1-review.html#explicit-vs.-implicit-type-coercion",
    "title": "Exam 1 review",
    "section": "Explicit vs. implicit type coercion",
    "text": "Explicit vs. implicit type coercion\n\nExplicit type coercion: You ask Python to change the type of a variable\nImplicit type coercion: Python changes / makes assumptions for you about the type of a variable without you asking for it\n\nThis happens because in a series, you can’t have multiple types of values"
  },
  {
    "objectID": "slides/09-exam-1-review.html#vectors",
    "href": "slides/09-exam-1-review.html#vectors",
    "title": "Exam 1 review",
    "section": "Vectors",
    "text": "Vectors\n\n\nA vector is a collection of values\n\nAtomic vectors can only contain values of the same type\nLists can contain values of different types\n\nWhy do we care? Because each column of a data frame is a vector.\n\n\n\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3],          # numeric (int)\n    'y': ['a', 'b', 'c'],    # character\n    'z': [True, False, True] # boolean\n})\ndf\n\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n0\n1\na\nTrue\n\n\n1\n2\nb\nFalse\n\n\n2\n3\nc\nTrue"
  },
  {
    "objectID": "slides/09-exam-1-review.html#explicit-coercion",
    "href": "slides/09-exam-1-review.html#explicit-coercion",
    "title": "Exam 1 review",
    "section": "Explicit coercion",
    "text": "Explicit coercion\n✅ From numeric to character\n\ndf['x_new'] = df['x'].astype(str)\ndf\n\n\n\n\n\n\n\n\nx\ny\nz\nx_new\n\n\n\n\n0\n1\na\nTrue\n1\n\n\n1\n2\nb\nFalse\n2\n\n\n2\n3\nc\nTrue\n3"
  },
  {
    "objectID": "slides/09-exam-1-review.html#explicit-coercion-1",
    "href": "slides/09-exam-1-review.html#explicit-coercion-1",
    "title": "Exam 1 review",
    "section": "Explicit coercion",
    "text": "Explicit coercion\n❌ From character to numeric\n\ndf['y_new'] = pd.to_numeric(df['y'], errors='coerce')\ndf\n\n\n\n\n\n\n\n\nx\ny\nz\nx_new\ny_new\n\n\n\n\n0\n1\na\nTrue\n1\nNaN\n\n\n1\n2\nb\nFalse\n2\nNaN\n\n\n2\n3\nc\nTrue\n3\nNaN"
  },
  {
    "objectID": "slides/09-exam-1-review.html#implicit-coercion",
    "href": "slides/09-exam-1-review.html#implicit-coercion",
    "title": "Exam 1 review",
    "section": "Implicit coercion",
    "text": "Implicit coercion\n\nWhich of the column types were implicitly coerced?\n\n\ndf = pd.DataFrame({\n    'w': [1, 2, 3],\n    'x': ['a', 'b', 4],\n    'y': ['c', 'd', np.nan],\n    'z': [5, 6, np.nan],\n})\ndf\n\n\n\n\n\n\n\n\nw\nx\ny\nz\n\n\n\n\n0\n1\na\nc\n5.0\n\n\n1\n2\nb\nd\n6.0\n\n\n2\n3\n4\nNaN\nNaN"
  },
  {
    "objectID": "slides/09-exam-1-review.html#collecting-data",
    "href": "slides/09-exam-1-review.html#collecting-data",
    "title": "Exam 1 review",
    "section": "Collecting data",
    "text": "Collecting data\n\nSuppose you conduct a survey and ask students their student ID number and number of credits they’re taking this semester. What is the type of each variable?\n\n\n\nsurvey_raw = pd.DataFrame({\n    'student_id': [273674, 298765, 287129, \"I don't remember\"],\n    'n_credits': [4, 4.5, \"I'm not sure yet\", \"2 - underloading\"]\n})\nsurvey_raw\n\n\n\n\n\n\n\n\nstudent_id\nn_credits\n\n\n\n\n0\n273674\n4\n\n\n1\n298765\n4.5\n\n\n2\n287129\nI'm not sure yet\n\n\n3\nI don't remember\n2 - underloading"
  },
  {
    "objectID": "slides/09-exam-1-review.html#cleaning-data",
    "href": "slides/09-exam-1-review.html#cleaning-data",
    "title": "Exam 1 review",
    "section": "Cleaning data",
    "text": "Cleaning data\n\nsurvey = survey_raw.copy()\nsurvey['student_id'] = survey['student_id'].replace(\"I don't remember\", np.nan)\nsurvey['n_credits'] = survey['n_credits'].replace({\n    \"I'm not sure yet\": np.nan,\n    \"2 - underloading\": \"2\"\n})\nsurvey['n_credits'] = pd.to_numeric(survey['n_credits'])\nsurvey\n\n\n\n\n\n\n\n\nstudent_id\nn_credits\n\n\n\n\n0\n273674.0\n4.0\n\n\n1\n298765.0\n4.5\n\n\n2\n287129.0\nNaN\n\n\n3\nNaN\n2.0"
  },
  {
    "objectID": "slides/09-exam-1-review.html#cleaning-data-alternative",
    "href": "slides/09-exam-1-review.html#cleaning-data-alternative",
    "title": "Exam 1 review",
    "section": "Cleaning data – alternative",
    "text": "Cleaning data – alternative\n\nsurvey = survey_raw.copy()\nsurvey['student_id'] = pd.to_numeric(survey['student_id'], errors='coerce')\nsurvey['n_credits'] = pd.to_numeric(survey['n_credits'], errors='coerce')\nsurvey\n\n\n\n\n\n\n\n\nstudent_id\nn_credits\n\n\n\n\n0\n273674.0\n4.0\n\n\n1\n298765.0\n4.5\n\n\n2\n287129.0\nNaN\n\n\n3\nNaN\nNaN"
  },
  {
    "objectID": "slides/09-exam-1-review.html#recap-type-coercion",
    "href": "slides/09-exam-1-review.html#recap-type-coercion",
    "title": "Exam 1 review",
    "section": "Recap: Type coercion",
    "text": "Recap: Type coercion\n\n\nIf variables in a DataFrame have multiple types of values, Python will coerce them into a single type, which may or may not be what you want.\nIf what Python does by default is not what you want, you can use explicit coercion functions like pd.to_numeric(), astype(), etc., to turn them into the types you want them to be, which will generally also involve cleaning up the features of the data that caused the unwanted implicit coercion in the first place."
  },
  {
    "objectID": "slides/09-exam-1-review.html#loan50-example-dataframe",
    "href": "slides/09-exam-1-review.html#loan50-example-dataframe",
    "title": "Exam 1 review",
    "section": "loan50 example DataFrame",
    "text": "loan50 example DataFrame\n\nloan50 = pd.read_csv(\"data/loan50.csv\")\nloan50.head()\n\n\n\n\n\n\n\n\nstate\nemp_length\nterm\nhomeownership\nannual_income\nverified_income\ndebt_to_income\ntotal_credit_limit\ntotal_credit_utilized\nnum_cc_carrying_balance\nloan_purpose\nloan_amount\ngrade\ninterest_rate\npublic_record_bankrupt\nloan_status\nhas_second_income\ntotal_income\n\n\n\n\n0\nNJ\n3.0\n60\nrent\n59000.0\nNot Verified\n0.557525\n95131\n32894\n8\ndebt_consolidation\n22000\nB\n10.90\n0\nCurrent\nFalse\n59000.0\n\n\n1\nCA\n10.0\n36\nrent\n60000.0\nNot Verified\n1.305683\n51929\n78341\n2\ncredit_card\n6000\nB\n9.92\n1\nCurrent\nFalse\n60000.0\n\n\n2\nSC\nNaN\n36\nmortgage\n75000.0\nVerified\n1.056280\n301373\n79221\n14\ndebt_consolidation\n25000\nE\n26.30\n0\nCurrent\nFalse\n75000.0\n\n\n3\nCA\n0.0\n36\nrent\n75000.0\nNot Verified\n0.574347\n59890\n43076\n10\ncredit_card\n6000\nB\n9.92\n0\nCurrent\nFalse\n75000.0\n\n\n4\nOH\n4.0\n60\nmortgage\n254000.0\nNot Verified\n0.238150\n422619\n60490\n2\nhome_improvement\n25000\nB\n9.43\n0\nCurrent\nFalse\n254000.0"
  },
  {
    "objectID": "slides/09-exam-1-review.html#aesthetic-mappings-1",
    "href": "slides/09-exam-1-review.html#aesthetic-mappings-1",
    "title": "Exam 1 review",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nWhat will the following code result in?\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', style='homeownership', palette='colorblind')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#aesthetic-mappings-2",
    "href": "slides/09-exam-1-review.html#aesthetic-mappings-2",
    "title": "Exam 1 review",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', style='homeownership', palette='colorblind')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#multiple-plot-layers",
    "href": "slides/09-exam-1-review.html#multiple-plot-layers",
    "title": "Exam 1 review",
    "section": "Multiple plot layers",
    "text": "Multiple plot layers\n\nWhat will the following code result in?\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', style='homeownership', palette='colorblind')\nsns.lineplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', legend=False, palette='colorblind')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#multiple-plot-layers-1",
    "href": "slides/09-exam-1-review.html#multiple-plot-layers-1",
    "title": "Exam 1 review",
    "section": "Multiple plot layers",
    "text": "Multiple plot layers\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', style='homeownership', palette='colorblind')\nsns.lineplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', legend=False, palette='colorblind')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#mapping-vs.-setting",
    "href": "slides/09-exam-1-review.html#mapping-vs.-setting",
    "title": "Exam 1 review",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nWhat will the following code result in?\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', palette='colorblind')\nsns.lineplot(data=loan50, x='annual_income', y='interest_rate', color='red', legend=False)\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#mapping-vs.-setting-1",
    "href": "slides/09-exam-1-review.html#mapping-vs.-setting-1",
    "title": "Exam 1 review",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', palette='colorblind')\nsns.lineplot(data=loan50, x='annual_income', y='interest_rate', color='red', legend=False)\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#recap-aesthetic-mappings",
    "href": "slides/09-exam-1-review.html#recap-aesthetic-mappings",
    "title": "Exam 1 review",
    "section": "Recap: Aesthetic mappings",
    "text": "Recap: Aesthetic mappings\n\n\nAesthetic mapping defined at the local level will be used only by the elements they’re defined for.\nSetting colors produces a manual color aesthetic, while mapping assigns colors automatically based on the qualifier."
  },
  {
    "objectID": "slides/09-exam-1-review.html#aside-legends",
    "href": "slides/09-exam-1-review.html#aside-legends",
    "title": "Exam 1 review",
    "section": "Aside: Legends",
    "text": "Aside: Legends\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=loan50, x='annual_income', y='interest_rate', hue='homeownership', style='homeownership')\nplt.legend(title='Home ownership')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#categorical",
    "href": "slides/09-exam-1-review.html#categorical",
    "title": "Exam 1 review",
    "section": "Categorical",
    "text": "Categorical\n\nCategorical variables — variables that have a fixed and known set of possible values — are used in the pandas library.\n\n\n\nThey are also useful when you want to display character vectors in a non-alphabetical order.\n\n\n\npandas: https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html"
  },
  {
    "objectID": "slides/09-exam-1-review.html#bar-plot",
    "href": "slides/09-exam-1-review.html#bar-plot",
    "title": "Exam 1 review",
    "section": "Bar plot",
    "text": "Bar plot\n\nplt.figure(figsize=(8, 6))\nsns.countplot(data=loan50, x='homeownership')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#bar-plot---reordered",
    "href": "slides/09-exam-1-review.html#bar-plot---reordered",
    "title": "Exam 1 review",
    "section": "Bar plot - reordered",
    "text": "Bar plot - reordered\n\nloan50['homeownership'] = pd.Categorical(loan50['homeownership'], categories=['mortgage', 'rent', 'own'])\nplt.figure(figsize=(8, 6))\nsns.countplot(data=loan50, x='homeownership')\nplt.show()"
  },
  {
    "objectID": "slides/09-exam-1-review.html#frequency-table",
    "href": "slides/09-exam-1-review.html#frequency-table",
    "title": "Exam 1 review",
    "section": "Frequency table",
    "text": "Frequency table\n\nloan50['homeownership'].value_counts()\n\nhomeownership\nmortgage    26\nrent        21\nown          3\nName: count, dtype: int64"
  },
  {
    "objectID": "slides/09-exam-1-review.html#under-the-hood",
    "href": "slides/09-exam-1-review.html#under-the-hood",
    "title": "Exam 1 review",
    "section": "Under the hood",
    "text": "Under the hood\n\nprint(type(loan50['homeownership']))\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\nprint(loan50['homeownership'].dtype)\n\ncategory\n\n\n\n\n\nprint(loan50['homeownership'].cat.categories)\n\nIndex(['mortgage', 'rent', 'own'], dtype='object')\n\n\n\n\n\nprint(loan50['homeownership'])\n\n0         rent\n1         rent\n2     mortgage\n3         rent\n4     mortgage\n5     mortgage\n6         rent\n7     mortgage\n8         rent\n9     mortgage\n10        rent\n11    mortgage\n12        rent\n13    mortgage\n14        rent\n15    mortgage\n16        rent\n17        rent\n18        rent\n19    mortgage\n20    mortgage\n21    mortgage\n22    mortgage\n23        rent\n24    mortgage\n25        rent\n26    mortgage\n27         own\n28    mortgage\n29    mortgage\n30        rent\n31    mortgage\n32    mortgage\n33        rent\n34        rent\n35         own\n36    mortgage\n37        rent\n38    mortgage\n39        rent\n40    mortgage\n41        rent\n42        rent\n43    mortgage\n44    mortgage\n45    mortgage\n46    mortgage\n47        rent\n48         own\n49    mortgage\nName: homeownership, dtype: category\nCategories (3, object): ['mortgage', 'rent', 'own']"
  },
  {
    "objectID": "slides/09-exam-1-review.html#recap-categorical",
    "href": "slides/09-exam-1-review.html#recap-categorical",
    "title": "Exam 1 review",
    "section": "Recap: Categorical",
    "text": "Recap: Categorical\n\n\nThe pandas.Categorical type is useful for dealing with categorical data and their levels.\nFactors and the order of their levels are relevant for displays (tables, plots) and they’ll be relevant for modeling (later in the course).\nCategorical is a data class in pandas."
  },
  {
    "objectID": "slides/09-exam-1-review.html#aside",
    "href": "slides/09-exam-1-review.html#aside",
    "title": "Exam 1 review",
    "section": "Aside: ==",
    "text": "Aside: ==\n\nloan50['homeownership_new'] = loan50['homeownership'].apply(lambda x: \"don't own\" if x == 'rent' else x)\nloan50[['homeownership', 'homeownership_new']].drop_duplicates()\n\n\n\n\n\n\n\n\nhomeownership\nhomeownership_new\n\n\n\n\n0\nrent\ndon't own\n\n\n2\nmortgage\nmortgage\n\n\n27\nown\nown"
  },
  {
    "objectID": "slides/09-exam-1-review.html#aside-filtering",
    "href": "slides/09-exam-1-review.html#aside-filtering",
    "title": "Exam 1 review",
    "section": "Aside: Filtering",
    "text": "Aside: Filtering\n\nloan50['homeownership_new'] = loan50['homeownership'].apply(lambda x: \"don't own\" if x in ['rent', 'mortgage'] else x)\nloan50[['homeownership', 'homeownership_new']].drop_duplicates()\n\n\n\n\n\n\n\n\nhomeownership\nhomeownership_new\n\n\n\n\n0\nrent\ndon't own\n\n\n2\nmortgage\ndon't own\n\n\n27\nown\nown"
  },
  {
    "objectID": "slides/05-visualizing-data.html#setup",
    "href": "slides/05-visualizing-data.html#setup",
    "title": "Visualizing various types of data",
    "section": "Setup",
    "text": "Setup\n\n# Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\n# Load the penguins dataset\npenguins = load_penguins()\n\n# Set theme\nsns.set_theme(style=\"whitegrid\")"
  },
  {
    "objectID": "slides/05-visualizing-data.html#examining-data-visualization",
    "href": "slides/05-visualizing-data.html#examining-data-visualization",
    "title": "Visualizing various types of data",
    "section": "Examining data visualization",
    "text": "Examining data visualization\n\n\nDiscuss the following for the visualization in the #lecture-discussions Slack Channel.\n\n\n\n\nSource: Twitter"
  },
  {
    "objectID": "slides/05-visualizing-data.html#violin-plots",
    "href": "slides/05-visualizing-data.html#violin-plots",
    "title": "Visualizing various types of data",
    "section": "Violin plots",
    "text": "Violin plots\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins)\nplt.title('Violin Plot of Body Mass by Species')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#multiple-geoms",
    "href": "slides/05-visualizing-data.html#multiple-geoms",
    "title": "Visualizing various types of data",
    "section": "Multiple geoms",
    "text": "Multiple geoms\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins)\nsns.stripplot(x=\"species\", y=\"body_mass_g\", data=penguins, jitter=False, color='black')\nplt.title('Violin Plot with Points of Body Mass by Species')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#multiple-geoms-1",
    "href": "slides/05-visualizing-data.html#multiple-geoms-1",
    "title": "Visualizing various types of data",
    "section": "Multiple geoms",
    "text": "Multiple geoms\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins)\nsns.stripplot(x=\"species\", y=\"body_mass_g\", data=penguins, jitter=True, color='black')\nplt.title('Violin Plot with Points of Body Mass by Species')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#multiple-geoms-aesthetics",
    "href": "slides/05-visualizing-data.html#multiple-geoms-aesthetics",
    "title": "Visualizing various types of data",
    "section": "Multiple geoms + aesthetics",
    "text": "Multiple geoms + aesthetics\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins)\nsns.stripplot(x=\"species\", y=\"body_mass_g\", data=penguins, jitter=True, hue='species')\nplt.title('Violin Plot with Jittered Points and Color by Species')\nplt.legend(title='Species')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#multiple-geoms-aesthetics-1",
    "href": "slides/05-visualizing-data.html#multiple-geoms-aesthetics-1",
    "title": "Visualizing various types of data",
    "section": "Multiple geoms + aesthetics",
    "text": "Multiple geoms + aesthetics\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins)\nsns.stripplot(x=\"species\", y=\"body_mass_g\", data=penguins, jitter=True, hue='species')\nplt.title('Violin Plot with Jittered Points, Color by Species, and No Legend')\nplt.legend(title='Species').remove()\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#multiple-geoms-aesthetics-2",
    "href": "slides/05-visualizing-data.html#multiple-geoms-aesthetics-2",
    "title": "Visualizing various types of data",
    "section": "Multiple geoms + aesthetics",
    "text": "Multiple geoms + aesthetics\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins, palette='colorblind')\nsns.stripplot(x=\"species\", y=\"body_mass_g\", data=penguins, jitter=True, hue='species', palette='colorblind')\nplt.title('Violin Plot with Jittered Points, Color by Species, No Legend, and Colorblind Palette')\nplt.legend(title='Species').remove()\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#the-way-data-is-displayed-matters",
    "href": "slides/05-visualizing-data.html#the-way-data-is-displayed-matters",
    "title": "Visualizing various types of data",
    "section": "The way data is displayed matters",
    "text": "The way data is displayed matters\n\nWhat do these three plots show?\n\n\n\n\n\n\n\n\nSource: #barbarplots"
  },
  {
    "objectID": "slides/05-visualizing-data.html#visualizing-penguins",
    "href": "slides/05-visualizing-data.html#visualizing-penguins",
    "title": "Visualizing various types of data",
    "section": "Visualizing penguins",
    "text": "Visualizing penguins\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins()\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtwork by @allison_horst."
  },
  {
    "objectID": "slides/05-visualizing-data.html#univariate-analysis-1",
    "href": "slides/05-visualizing-data.html#univariate-analysis-1",
    "title": "Visualizing various types of data",
    "section": "Univariate analysis",
    "text": "Univariate analysis\nAnalyzing a single variable:\n\nNumerical: histogram, box plot, density plot, etc.\nCategorical: bar plot, pie chart, etc."
  },
  {
    "objectID": "slides/05-visualizing-data.html#histogram",
    "href": "slides/05-visualizing-data.html#histogram",
    "title": "Visualizing various types of data",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.histplot(penguins['body_mass_g'], bins=30)\nplt.title('Histogram of Penguin Body Mass')\nplt.xlabel('Body Mass (g)')\nplt.ylabel('Count')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#boxplot",
    "href": "slides/05-visualizing-data.html#boxplot",
    "title": "Visualizing various types of data",
    "section": "Boxplot",
    "text": "Boxplot\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.boxplot(y=penguins['body_mass_g'])\nplt.title('Boxplot of Penguin Body Mass')\nplt.ylabel('Body Mass (g)')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#density-plot",
    "href": "slides/05-visualizing-data.html#density-plot",
    "title": "Visualizing various types of data",
    "section": "Density plot",
    "text": "Density plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.kdeplot(penguins['body_mass_g'], fill=True)\nplt.title('Density Plot of Penguin Body Mass')\nplt.xlabel('Body Mass (g)')\nplt.ylabel('Density')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#bivariate-analysis-1",
    "href": "slides/05-visualizing-data.html#bivariate-analysis-1",
    "title": "Visualizing various types of data",
    "section": "Bivariate analysis",
    "text": "Bivariate analysis\nAnalyzing the relationship between two variables:\n\nNumerical + numerical: scatterplot\nNumerical + categorical: side-by-side box plots, violin plots, etc.\nCategorical + categorical: stacked bar plots\nUsing an aesthetic (e.g., fill, color, shape, etc.) or facets to represent the second variable in any plot"
  },
  {
    "objectID": "slides/05-visualizing-data.html#side-by-side-box-plots",
    "href": "slides/05-visualizing-data.html#side-by-side-box-plots",
    "title": "Visualizing various types of data",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.boxplot(x=\"species\", y=\"body_mass_g\", data=penguins)\nplt.title('Side-by-side Box Plots of Body Mass by Species')\nplt.xlabel('Species')\nplt.ylabel('Body Mass (g)')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#density-plots",
    "href": "slides/05-visualizing-data.html#density-plots",
    "title": "Visualizing various types of data",
    "section": "Density plots",
    "text": "Density plots\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.kdeplot(data=penguins, x=\"body_mass_g\", hue=\"species\", fill=True)\nplt.title('Density Plot of Body Mass by Species')\nplt.xlabel('Body Mass (g)')\nplt.ylabel('Density')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#bechdel-test",
    "href": "slides/05-visualizing-data.html#bechdel-test",
    "title": "Visualizing various types of data",
    "section": "Bechdel Test",
    "text": "Bechdel Test\n\n\n\nThe Bechdel test also known as the Bechdel-Wallace test, is a measure of the representation of women in film and other fiction. The test asks whether a work features at least two female characters who have a conversation about something other than a man. Some versions of the test also require that those two female characters have names."
  },
  {
    "objectID": "slides/05-visualizing-data.html#load-bechdel-test-data",
    "href": "slides/05-visualizing-data.html#load-bechdel-test-data",
    "title": "Visualizing various types of data",
    "section": "Load Bechdel test data",
    "text": "Load Bechdel test data\nLoad the Bechdel test data with pd.read_csv()\n\nbechdel = pd.read_csv(\"data/bechdel.csv\")\n\nlist() the .columns names of the bechdel data:\n\nlist(bechdel.columns)\n\n['title', 'year', 'gross_2013', 'budget_2013', 'roi', 'binary', 'clean_test']"
  },
  {
    "objectID": "slides/05-visualizing-data.html#roi-by-test-result",
    "href": "slides/05-visualizing-data.html#roi-by-test-result",
    "title": "Visualizing various types of data",
    "section": "ROI by test result",
    "text": "ROI by test result\n\nWhat about this plot makes it difficult to evaluate how ROI varies by Bechdel test result?\n\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.stripplot(x='roi', y='clean_test', hue='binary', data=bechdel)\nplt.title('ROI by Bechdel Test Result')\nplt.xlabel('ROI')\nplt.ylabel('Bechdel Test Result')\nplt.legend(title='Binary')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#movies-with-high-roi",
    "href": "slides/05-visualizing-data.html#movies-with-high-roi",
    "title": "Visualizing various types of data",
    "section": "Movies with high ROI",
    "text": "Movies with high ROI\n\nWhat are the movies with highest ROI?\n\n\nhigh_roi_movies = bechdel[bechdel['roi'] &gt; 400][['title', 'roi', 'budget_2013', 'gross_2013', 'year', 'clean_test']]\nprint(high_roi_movies)\n\n                        title         roi  budget_2013   gross_2013  year  \\\n703       Paranormal Activity  671.336857       505595  339424558.0  2007   \n1319  The Blair Witch Project  648.065333       839077  543776715.0  1999   \n1575              El Mariachi  583.285665        11622    6778946.0  1992   \n\n     clean_test  \n703     dubious  \n1319         ok  \n1575    nowomen"
  },
  {
    "objectID": "slides/05-visualizing-data.html#roi-by-test-result-1",
    "href": "slides/05-visualizing-data.html#roi-by-test-result-1",
    "title": "Visualizing various types of data",
    "section": "ROI by test result",
    "text": "ROI by test result\n\nZoom in: What about this plot makes it difficult to evaluate how ROI varies by Bechdel test result?\n\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='roi', y='clean_test', hue='binary', data=bechdel)\nplt.xlim(0, 15)\nplt.title('Zoomed in ROI by Bechdel Test Result')\nplt.xlabel('ROI')\nplt.ylabel('Bechdel Test Result')\nplt.legend(title='Binary')\nplt.show()"
  },
  {
    "objectID": "slides/05-visualizing-data.html#sneak-preview",
    "href": "slides/05-visualizing-data.html#sneak-preview",
    "title": "Visualizing various types of data",
    "section": "Sneak preview…",
    "text": "Sneak preview…\n \n\nto next week’s data wrangling pipelines…"
  },
  {
    "objectID": "slides/05-visualizing-data.html#median-roi",
    "href": "slides/05-visualizing-data.html#median-roi",
    "title": "Visualizing various types of data",
    "section": "Median ROI",
    "text": "Median ROI\n\nmedian_roi = bechdel['roi'].median()\nprint(f\"Median ROI: {median_roi}\")\n\nMedian ROI: 3.9051317558839456"
  },
  {
    "objectID": "slides/05-visualizing-data.html#median-roi-by-test-result",
    "href": "slides/05-visualizing-data.html#median-roi-by-test-result",
    "title": "Visualizing various types of data",
    "section": "Median ROI by test result",
    "text": "Median ROI by test result\n\nmedian_roi_by_test = bechdel.groupby('clean_test')['roi'].median().reset_index()\nprint(median_roi_by_test)\n\n  clean_test       roi\n0    dubious  3.795816\n1        men  3.964457\n2     notalk  3.688260\n3    nowomen  3.265901\n4         ok  4.211049"
  },
  {
    "objectID": "slides/05-visualizing-data.html#roi-by-test-result-with-median-line",
    "href": "slides/05-visualizing-data.html#roi-by-test-result-with-median-line",
    "title": "Visualizing various types of data",
    "section": "ROI by test result with median line",
    "text": "ROI by test result with median line\n\nWhat does this plot say about return-on-investment on movies that pass the Bechdel test?\n\n\n\nCode\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='roi', y='clean_test', hue='binary', data=bechdel)\nplt.axvline(x=4.21, color='red', linestyle='--')\nplt.xlim(0, 15)\nplt.title('ROI by Bechdel Test Result with Median Line')\nplt.xlabel('ROI')\nplt.ylabel('Bechdel Test Result')\nplt.legend(title='Binary')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/06-preprocessing.html#setup",
    "href": "slides/06-preprocessing.html#setup",
    "title": "Data preprocessing",
    "section": "Setup",
    "text": "Setup\n\n# Import all required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nimport statsmodels.api as sm\nimport scipy.stats as stats\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"whitegrid\")"
  },
  {
    "objectID": "slides/06-preprocessing.html#data-preprocessing-1",
    "href": "slides/06-preprocessing.html#data-preprocessing-1",
    "title": "Data preprocessing",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\nData preprocessing refers to the manipulation, filtration, or augmentation of data before it is analyzed. It is a crucial step in the data science process.\n\n\nIt’s essentially data cleaning."
  },
  {
    "objectID": "slides/06-preprocessing.html#dataset",
    "href": "slides/06-preprocessing.html#dataset",
    "title": "Data preprocessing",
    "section": "Dataset",
    "text": "Dataset\nHuman Freedom Index\nThe Human Freedom Index is a report that attempts to summarize the idea of “freedom” through variables for many countries around the globe."
  },
  {
    "objectID": "slides/06-preprocessing.html#question",
    "href": "slides/06-preprocessing.html#question",
    "title": "Data preprocessing",
    "section": "Question",
    "text": "Question\nWhat trends are there within human freedom indices in different regions?"
  },
  {
    "objectID": "slides/06-preprocessing.html#dataset-human-freedom-index",
    "href": "slides/06-preprocessing.html#dataset-human-freedom-index",
    "title": "Data preprocessing",
    "section": "Dataset: Human Freedom Index",
    "text": "Dataset: Human Freedom Index\n\nhfi = pd.read_csv(\"data/hfi.csv\")\nhfi.head()\n\n\n\n\n\n\n\n\nyear\nISO_code\ncountries\nregion\npf_rol_procedural\npf_rol_civil\npf_rol_criminal\npf_rol\npf_ss_homicide\npf_ss_disappearances_disap\n...\nef_regulation_business_bribes\nef_regulation_business_licensing\nef_regulation_business_compliance\nef_regulation_business\nef_regulation\nef_score\nef_rank\nhf_score\nhf_rank\nhf_quartile\n\n\n\n\n0\n2016\nALB\nAlbania\nEastern Europe\n6.661503\n4.547244\n4.666508\n5.291752\n8.920429\n10.0\n...\n4.050196\n7.324582\n7.074366\n6.705863\n6.906901\n7.54\n34.0\n7.568140\n48.0\n2.0\n\n\n1\n2016\nDZA\nAlgeria\nMiddle East & North Africa\nNaN\nNaN\nNaN\n3.819566\n9.456254\n10.0\n...\n3.765515\n8.523503\n7.029528\n5.676956\n5.268992\n4.99\n159.0\n5.135886\n155.0\n4.0\n\n\n2\n2016\nAGO\nAngola\nSub-Saharan Africa\nNaN\nNaN\nNaN\n3.451814\n8.060260\n5.0\n...\n1.945540\n8.096776\n6.782923\n4.930271\n5.518500\n5.17\n155.0\n5.640662\n142.0\n4.0\n\n\n3\n2016\nARG\nArgentina\nLatin America & the Caribbean\n7.098483\n5.791960\n4.343930\n5.744791\n7.622974\n10.0\n...\n3.260044\n5.253411\n6.508295\n5.535831\n5.369019\n4.84\n160.0\n6.469848\n107.0\n3.0\n\n\n4\n2016\nARM\nArmenia\nCaucasus & Central Asia\nNaN\nNaN\nNaN\n5.003205\n8.808750\n10.0\n...\n4.575152\n9.319612\n6.491481\n6.797530\n7.378069\n7.57\n29.0\n7.241402\n57.0\n2.0\n\n\n\n\n5 rows × 123 columns"
  },
  {
    "objectID": "slides/06-preprocessing.html#understanding-the-data",
    "href": "slides/06-preprocessing.html#understanding-the-data",
    "title": "Data preprocessing",
    "section": "Understanding the data",
    "text": "Understanding the data\n\n.info().describe()\n\n\n\nhfi.info(verbose = True)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1458 entries, 0 to 1457\nData columns (total 123 columns):\n #    Column                              Dtype  \n---   ------                              -----  \n 0    year                                int64  \n 1    ISO_code                            object \n 2    countries                           object \n 3    region                              object \n 4    pf_rol_procedural                   float64\n 5    pf_rol_civil                        float64\n 6    pf_rol_criminal                     float64\n 7    pf_rol                              float64\n 8    pf_ss_homicide                      float64\n 9    pf_ss_disappearances_disap          float64\n 10   pf_ss_disappearances_violent        float64\n 11   pf_ss_disappearances_organized      float64\n 12   pf_ss_disappearances_fatalities     float64\n 13   pf_ss_disappearances_injuries       float64\n 14   pf_ss_disappearances                float64\n 15   pf_ss_women_fgm                     float64\n 16   pf_ss_women_missing                 float64\n 17   pf_ss_women_inheritance_widows      float64\n 18   pf_ss_women_inheritance_daughters   float64\n 19   pf_ss_women_inheritance             float64\n 20   pf_ss_women                         float64\n 21   pf_ss                               float64\n 22   pf_movement_domestic                float64\n 23   pf_movement_foreign                 float64\n 24   pf_movement_women                   float64\n 25   pf_movement                         float64\n 26   pf_religion_estop_establish         float64\n 27   pf_religion_estop_operate           float64\n 28   pf_religion_estop                   float64\n 29   pf_religion_harassment              float64\n 30   pf_religion_restrictions            float64\n 31   pf_religion                         float64\n 32   pf_association_association          float64\n 33   pf_association_assembly             float64\n 34   pf_association_political_establish  float64\n 35   pf_association_political_operate    float64\n 36   pf_association_political            float64\n 37   pf_association_prof_establish       float64\n 38   pf_association_prof_operate         float64\n 39   pf_association_prof                 float64\n 40   pf_association_sport_establish      float64\n 41   pf_association_sport_operate        float64\n 42   pf_association_sport                float64\n 43   pf_association                      float64\n 44   pf_expression_killed                float64\n 45   pf_expression_jailed                float64\n 46   pf_expression_influence             float64\n 47   pf_expression_control               float64\n 48   pf_expression_cable                 float64\n 49   pf_expression_newspapers            float64\n 50   pf_expression_internet              float64\n 51   pf_expression                       float64\n 52   pf_identity_legal                   float64\n 53   pf_identity_parental_marriage       float64\n 54   pf_identity_parental_divorce        float64\n 55   pf_identity_parental                float64\n 56   pf_identity_sex_male                float64\n 57   pf_identity_sex_female              float64\n 58   pf_identity_sex                     float64\n 59   pf_identity_divorce                 float64\n 60   pf_identity                         float64\n 61   pf_score                            float64\n 62   pf_rank                             float64\n 63   ef_government_consumption           float64\n 64   ef_government_transfers             float64\n 65   ef_government_enterprises           float64\n 66   ef_government_tax_income            float64\n 67   ef_government_tax_payroll           float64\n 68   ef_government_tax                   float64\n 69   ef_government                       float64\n 70   ef_legal_judicial                   float64\n 71   ef_legal_courts                     float64\n 72   ef_legal_protection                 float64\n 73   ef_legal_military                   float64\n 74   ef_legal_integrity                  float64\n 75   ef_legal_enforcement                float64\n 76   ef_legal_restrictions               float64\n 77   ef_legal_police                     float64\n 78   ef_legal_crime                      float64\n 79   ef_legal_gender                     float64\n 80   ef_legal                            float64\n 81   ef_money_growth                     float64\n 82   ef_money_sd                         float64\n 83   ef_money_inflation                  float64\n 84   ef_money_currency                   float64\n 85   ef_money                            float64\n 86   ef_trade_tariffs_revenue            float64\n 87   ef_trade_tariffs_mean               float64\n 88   ef_trade_tariffs_sd                 float64\n 89   ef_trade_tariffs                    float64\n 90   ef_trade_regulatory_nontariff       float64\n 91   ef_trade_regulatory_compliance      float64\n 92   ef_trade_regulatory                 float64\n 93   ef_trade_black                      float64\n 94   ef_trade_movement_foreign           float64\n 95   ef_trade_movement_capital           float64\n 96   ef_trade_movement_visit             float64\n 97   ef_trade_movement                   float64\n 98   ef_trade                            float64\n 99   ef_regulation_credit_ownership      float64\n 100  ef_regulation_credit_private        float64\n 101  ef_regulation_credit_interest       float64\n 102  ef_regulation_credit                float64\n 103  ef_regulation_labor_minwage         float64\n 104  ef_regulation_labor_firing          float64\n 105  ef_regulation_labor_bargain         float64\n 106  ef_regulation_labor_hours           float64\n 107  ef_regulation_labor_dismissal       float64\n 108  ef_regulation_labor_conscription    float64\n 109  ef_regulation_labor                 float64\n 110  ef_regulation_business_adm          float64\n 111  ef_regulation_business_bureaucracy  float64\n 112  ef_regulation_business_start        float64\n 113  ef_regulation_business_bribes       float64\n 114  ef_regulation_business_licensing    float64\n 115  ef_regulation_business_compliance   float64\n 116  ef_regulation_business              float64\n 117  ef_regulation                       float64\n 118  ef_score                            float64\n 119  ef_rank                             float64\n 120  hf_score                            float64\n 121  hf_rank                             float64\n 122  hf_quartile                         float64\ndtypes: float64(119), int64(1), object(3)\nmemory usage: 1.4+ MB\n\n\n\n\n\nhfi.describe()\n\n\n\n\n\n\n\n\nyear\npf_rol_procedural\npf_rol_civil\npf_rol_criminal\npf_rol\npf_ss_homicide\npf_ss_disappearances_disap\npf_ss_disappearances_violent\npf_ss_disappearances_organized\npf_ss_disappearances_fatalities\n...\nef_regulation_business_bribes\nef_regulation_business_licensing\nef_regulation_business_compliance\nef_regulation_business\nef_regulation\nef_score\nef_rank\nhf_score\nhf_rank\nhf_quartile\n\n\n\n\ncount\n1458.000000\n880.000000\n880.000000\n880.000000\n1378.000000\n1378.000000\n1369.000000\n1378.000000\n1279.000000\n1378.000000\n...\n1283.000000\n1357.000000\n1368.000000\n1374.000000\n1378.000000\n1378.000000\n1378.000000\n1378.000000\n1378.000000\n1378.000000\n\n\nmean\n2012.000000\n5.589355\n5.474770\n5.044070\n5.309641\n7.412980\n8.341855\n9.519458\n6.772869\n9.584972\n...\n4.886192\n7.698494\n6.981858\n6.317668\n7.019782\n6.785610\n76.973149\n6.993444\n77.007983\n2.490566\n\n\nstd\n2.582875\n2.080957\n1.428494\n1.724886\n1.529310\n2.832947\n3.225902\n1.744673\n2.768983\n1.559826\n...\n1.889168\n1.728507\n1.979200\n1.230988\n1.027625\n0.883601\n44.540142\n1.025811\n44.506549\n1.119698\n\n\nmin\n2008.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n2.009841\n2.483540\n2.880000\n1.000000\n3.765827\n1.000000\n1.000000\n\n\n25%\n2010.000000\n4.133333\n4.549550\n3.789724\n4.131746\n6.386978\n10.000000\n10.000000\n5.000000\n9.942607\n...\n3.433786\n6.874687\n6.368178\n5.591851\n6.429498\n6.250000\n38.000000\n6.336685\n39.000000\n1.000000\n\n\n50%\n2012.000000\n5.300000\n5.300000\n4.575189\n4.910797\n8.638278\n10.000000\n10.000000\n7.500000\n10.000000\n...\n4.418371\n8.074161\n7.466692\n6.265234\n7.082075\n6.900000\n77.000000\n6.923840\n76.000000\n2.000000\n\n\n75%\n2014.000000\n7.389499\n6.410975\n6.400000\n6.513178\n9.454402\n10.000000\n10.000000\n10.000000\n10.000000\n...\n6.227978\n8.991882\n8.209310\n7.139718\n7.720955\n7.410000\n115.000000\n7.894660\n115.000000\n3.000000\n\n\nmax\n2016.000000\n9.700000\n8.773533\n8.719848\n8.723094\n9.926568\n10.000000\n10.000000\n10.000000\n10.000000\n...\n9.623811\n9.999638\n9.865488\n9.272600\n9.439828\n9.190000\n162.000000\n9.126313\n162.000000\n4.000000\n\n\n\n\n8 rows × 120 columns"
  },
  {
    "objectID": "slides/06-preprocessing.html#identifying-missing-values",
    "href": "slides/06-preprocessing.html#identifying-missing-values",
    "title": "Data preprocessing",
    "section": "Identifying missing values",
    "text": "Identifying missing values\n\nhfi.isna().sum()\n\nyear                   0\nISO_code               0\ncountries              0\nregion                 0\npf_rol_procedural    578\n                    ... \nef_score              80\nef_rank               80\nhf_score              80\nhf_rank               80\nhf_quartile           80\nLength: 123, dtype: int64\n\n\n\n\nA lot of missing values 🙃"
  },
  {
    "objectID": "slides/06-preprocessing.html#handling-missing-data",
    "href": "slides/06-preprocessing.html#handling-missing-data",
    "title": "Data preprocessing",
    "section": "Handling missing data",
    "text": "Handling missing data\nOptions\n\n\nDo nothing…\nRemove them\nImputate\n\n\n\nWe will use the hf_score from hfi: 80 missing values"
  },
  {
    "objectID": "slides/06-preprocessing.html#imputation",
    "href": "slides/06-preprocessing.html#imputation",
    "title": "Data preprocessing",
    "section": "Imputation",
    "text": "Imputation\n\nIn statistics, imputation is the process of replacing missing data with substituted values.\n\n\n\nMean imputation\nMedian imputation\nMode imputation\nSeveral other methods"
  },
  {
    "objectID": "slides/06-preprocessing.html#mean-imputation",
    "href": "slides/06-preprocessing.html#mean-imputation",
    "title": "Data preprocessing",
    "section": "Mean imputation",
    "text": "Mean imputation\n\nDefinitionVisualCode\n\n\nHow it Works: Replace missing values with the arithmetic mean of the non-missing values in the same variable.\n\nPros:\n\n\nEasy and fast.\nWorks well with small numerical datasets\n\n\nCons:\n\n\nIt only works on the column level.\nWill give poor results on encoded categorical features.\nNot very accurate.\nDoesn’t account for the uncertainty in the imputations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhfi_copy = hfi.copy()\n\nmean_imputer = SimpleImputer(strategy = 'mean')\nhfi_copy['mean_hf_score'] = mean_imputer.fit_transform(hfi_copy[['hf_score']])\n\nmean_plot = sns.kdeplot(data = hfi_copy, x = 'hf_score', linewidth = 2, label = \"Original\")\nmean_plot = sns.kdeplot(data = hfi_copy, x = 'mean_hf_score', linewidth = 2, label = \"Mean Imputed\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "slides/06-preprocessing.html#median-imputation",
    "href": "slides/06-preprocessing.html#median-imputation",
    "title": "Data preprocessing",
    "section": "Median imputation",
    "text": "Median imputation\n\nDefinitionVisualCode\n\n\nHow it Works: Replace missing values with the arithmetic median of the non-missing values in the same variable.\n\nPros (same as mean):\n\n\nEasy and fast.\nWorks well with small numerical datasets\n\n\nCons (same as mean):\n\n\nIt only works on the column level.\nWill give poor results on encoded categorical features.\nNot very accurate.\nDoesn’t account for the uncertainty in the imputations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhfi_copy = hfi.copy()\n\nmedian_imputer = SimpleImputer(strategy = 'median')\nhfi_copy['median_hf_score'] = median_imputer.fit_transform(hfi_copy[['hf_score']])\n\nmedian_plot = sns.kdeplot(data = hfi_copy, x = 'hf_score', linewidth = 2, label = \"Original\")\nmedian_plot = sns.kdeplot(data = hfi_copy, x = 'median_hf_score', linewidth = 2, label = \"Median Imputed\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "slides/06-preprocessing.html#mode-imputation",
    "href": "slides/06-preprocessing.html#mode-imputation",
    "title": "Data preprocessing",
    "section": "Mode imputation",
    "text": "Mode imputation\n\nDefinitionVisualCode\n\n\nHow it Works: Replace missing values with the mode of the non-missing values in the same variable.\n\nPros\n\n\nEasy and fast.\nWorks well with categorical features.\n\n\nCons:\n\n\nDoesn’t factor the correlations between features.\nCan introduce bias in the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhfi_copy = hfi.copy()\n\nmode_imputer = SimpleImputer(strategy = 'most_frequent')\nhfi_copy['mode_hf_score'] = mode_imputer.fit_transform(hfi_copy[['hf_score']])\n\nmode_plot = sns.kdeplot(data = hfi_copy, x = 'hf_score', linewidth = 2, label = \"Original\")\nmode_plot = sns.kdeplot(data = hfi_copy, x = 'mode_hf_score', linewidth = 2, label = \"Mode Imputated\")\n\nplt.legend()\n\nplt.show()"
  },
  {
    "objectID": "slides/06-preprocessing.html#logical-operators",
    "href": "slides/06-preprocessing.html#logical-operators",
    "title": "Data preprocessing",
    "section": "Logical operators",
    "text": "Logical operators\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?"
  },
  {
    "objectID": "slides/06-preprocessing.html#logical-operators-cont.",
    "href": "slides/06-preprocessing.html#logical-operators-cont.",
    "title": "Data preprocessing",
    "section": "Logical operators cont.",
    "text": "Logical operators cont.\n\n\n\n\n\n\n\noperator\ndefinition\n\n\n\n\nx and y\nis x AND y?\n\n\nx or y\nis x OR y?\n\n\nx is None\nis x None?\n\n\nx is not None\nis x not None?\n\n\nx in y\nis x in y?\n\n\nx not in y\nis x not in y?\n\n\nnot x\nis not x? (only makes sense if x is True or False)"
  },
  {
    "objectID": "slides/06-preprocessing.html#looking-at-our-data",
    "href": "slides/06-preprocessing.html#looking-at-our-data",
    "title": "Data preprocessing",
    "section": "Looking at our data",
    "text": "Looking at our data\n\nhfi_copy = hfi\nhfi_copy.info(verbose=True)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1458 entries, 0 to 1457\nData columns (total 126 columns):\n #    Column                              Dtype  \n---   ------                              -----  \n 0    year                                int64  \n 1    ISO_code                            object \n 2    countries                           object \n 3    region                              object \n 4    pf_rol_procedural                   float64\n 5    pf_rol_civil                        float64\n 6    pf_rol_criminal                     float64\n 7    pf_rol                              float64\n 8    pf_ss_homicide                      float64\n 9    pf_ss_disappearances_disap          float64\n 10   pf_ss_disappearances_violent        float64\n 11   pf_ss_disappearances_organized      float64\n 12   pf_ss_disappearances_fatalities     float64\n 13   pf_ss_disappearances_injuries       float64\n 14   pf_ss_disappearances                float64\n 15   pf_ss_women_fgm                     float64\n 16   pf_ss_women_missing                 float64\n 17   pf_ss_women_inheritance_widows      float64\n 18   pf_ss_women_inheritance_daughters   float64\n 19   pf_ss_women_inheritance             float64\n 20   pf_ss_women                         float64\n 21   pf_ss                               float64\n 22   pf_movement_domestic                float64\n 23   pf_movement_foreign                 float64\n 24   pf_movement_women                   float64\n 25   pf_movement                         float64\n 26   pf_religion_estop_establish         float64\n 27   pf_religion_estop_operate           float64\n 28   pf_religion_estop                   float64\n 29   pf_religion_harassment              float64\n 30   pf_religion_restrictions            float64\n 31   pf_religion                         float64\n 32   pf_association_association          float64\n 33   pf_association_assembly             float64\n 34   pf_association_political_establish  float64\n 35   pf_association_political_operate    float64\n 36   pf_association_political            float64\n 37   pf_association_prof_establish       float64\n 38   pf_association_prof_operate         float64\n 39   pf_association_prof                 float64\n 40   pf_association_sport_establish      float64\n 41   pf_association_sport_operate        float64\n 42   pf_association_sport                float64\n 43   pf_association                      float64\n 44   pf_expression_killed                float64\n 45   pf_expression_jailed                float64\n 46   pf_expression_influence             float64\n 47   pf_expression_control               float64\n 48   pf_expression_cable                 float64\n 49   pf_expression_newspapers            float64\n 50   pf_expression_internet              float64\n 51   pf_expression                       float64\n 52   pf_identity_legal                   float64\n 53   pf_identity_parental_marriage       float64\n 54   pf_identity_parental_divorce        float64\n 55   pf_identity_parental                float64\n 56   pf_identity_sex_male                float64\n 57   pf_identity_sex_female              float64\n 58   pf_identity_sex                     float64\n 59   pf_identity_divorce                 float64\n 60   pf_identity                         float64\n 61   pf_score                            float64\n 62   pf_rank                             float64\n 63   ef_government_consumption           float64\n 64   ef_government_transfers             float64\n 65   ef_government_enterprises           float64\n 66   ef_government_tax_income            float64\n 67   ef_government_tax_payroll           float64\n 68   ef_government_tax                   float64\n 69   ef_government                       float64\n 70   ef_legal_judicial                   float64\n 71   ef_legal_courts                     float64\n 72   ef_legal_protection                 float64\n 73   ef_legal_military                   float64\n 74   ef_legal_integrity                  float64\n 75   ef_legal_enforcement                float64\n 76   ef_legal_restrictions               float64\n 77   ef_legal_police                     float64\n 78   ef_legal_crime                      float64\n 79   ef_legal_gender                     float64\n 80   ef_legal                            float64\n 81   ef_money_growth                     float64\n 82   ef_money_sd                         float64\n 83   ef_money_inflation                  float64\n 84   ef_money_currency                   float64\n 85   ef_money                            float64\n 86   ef_trade_tariffs_revenue            float64\n 87   ef_trade_tariffs_mean               float64\n 88   ef_trade_tariffs_sd                 float64\n 89   ef_trade_tariffs                    float64\n 90   ef_trade_regulatory_nontariff       float64\n 91   ef_trade_regulatory_compliance      float64\n 92   ef_trade_regulatory                 float64\n 93   ef_trade_black                      float64\n 94   ef_trade_movement_foreign           float64\n 95   ef_trade_movement_capital           float64\n 96   ef_trade_movement_visit             float64\n 97   ef_trade_movement                   float64\n 98   ef_trade                            float64\n 99   ef_regulation_credit_ownership      float64\n 100  ef_regulation_credit_private        float64\n 101  ef_regulation_credit_interest       float64\n 102  ef_regulation_credit                float64\n 103  ef_regulation_labor_minwage         float64\n 104  ef_regulation_labor_firing          float64\n 105  ef_regulation_labor_bargain         float64\n 106  ef_regulation_labor_hours           float64\n 107  ef_regulation_labor_dismissal       float64\n 108  ef_regulation_labor_conscription    float64\n 109  ef_regulation_labor                 float64\n 110  ef_regulation_business_adm          float64\n 111  ef_regulation_business_bureaucracy  float64\n 112  ef_regulation_business_start        float64\n 113  ef_regulation_business_bribes       float64\n 114  ef_regulation_business_licensing    float64\n 115  ef_regulation_business_compliance   float64\n 116  ef_regulation_business              float64\n 117  ef_regulation                       float64\n 118  ef_score                            float64\n 119  ef_rank                             float64\n 120  hf_score                            float64\n 121  hf_rank                             float64\n 122  hf_quartile                         float64\n 123  mean_hf_score                       float64\n 124  median_hf_score                     float64\n 125  mode_hf_score                       float64\ndtypes: float64(122), int64(1), object(3)\nmemory usage: 1.4+ MB"
  },
  {
    "objectID": "slides/06-preprocessing.html#converting-year-to-datetime",
    "href": "slides/06-preprocessing.html#converting-year-to-datetime",
    "title": "Data preprocessing",
    "section": "Converting year to Datetime",
    "text": "Converting year to Datetime\n\nhfi_copy['year'] = pd.to_datetime(hfi_copy['year'], format='%Y')\nprint(hfi_copy.info(verbose=True))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1458 entries, 0 to 1457\nData columns (total 126 columns):\n #    Column                              Dtype         \n---   ------                              -----         \n 0    year                                datetime64[ns]\n 1    ISO_code                            object        \n 2    countries                           object        \n 3    region                              object        \n 4    pf_rol_procedural                   float64       \n 5    pf_rol_civil                        float64       \n 6    pf_rol_criminal                     float64       \n 7    pf_rol                              float64       \n 8    pf_ss_homicide                      float64       \n 9    pf_ss_disappearances_disap          float64       \n 10   pf_ss_disappearances_violent        float64       \n 11   pf_ss_disappearances_organized      float64       \n 12   pf_ss_disappearances_fatalities     float64       \n 13   pf_ss_disappearances_injuries       float64       \n 14   pf_ss_disappearances                float64       \n 15   pf_ss_women_fgm                     float64       \n 16   pf_ss_women_missing                 float64       \n 17   pf_ss_women_inheritance_widows      float64       \n 18   pf_ss_women_inheritance_daughters   float64       \n 19   pf_ss_women_inheritance             float64       \n 20   pf_ss_women                         float64       \n 21   pf_ss                               float64       \n 22   pf_movement_domestic                float64       \n 23   pf_movement_foreign                 float64       \n 24   pf_movement_women                   float64       \n 25   pf_movement                         float64       \n 26   pf_religion_estop_establish         float64       \n 27   pf_religion_estop_operate           float64       \n 28   pf_religion_estop                   float64       \n 29   pf_religion_harassment              float64       \n 30   pf_religion_restrictions            float64       \n 31   pf_religion                         float64       \n 32   pf_association_association          float64       \n 33   pf_association_assembly             float64       \n 34   pf_association_political_establish  float64       \n 35   pf_association_political_operate    float64       \n 36   pf_association_political            float64       \n 37   pf_association_prof_establish       float64       \n 38   pf_association_prof_operate         float64       \n 39   pf_association_prof                 float64       \n 40   pf_association_sport_establish      float64       \n 41   pf_association_sport_operate        float64       \n 42   pf_association_sport                float64       \n 43   pf_association                      float64       \n 44   pf_expression_killed                float64       \n 45   pf_expression_jailed                float64       \n 46   pf_expression_influence             float64       \n 47   pf_expression_control               float64       \n 48   pf_expression_cable                 float64       \n 49   pf_expression_newspapers            float64       \n 50   pf_expression_internet              float64       \n 51   pf_expression                       float64       \n 52   pf_identity_legal                   float64       \n 53   pf_identity_parental_marriage       float64       \n 54   pf_identity_parental_divorce        float64       \n 55   pf_identity_parental                float64       \n 56   pf_identity_sex_male                float64       \n 57   pf_identity_sex_female              float64       \n 58   pf_identity_sex                     float64       \n 59   pf_identity_divorce                 float64       \n 60   pf_identity                         float64       \n 61   pf_score                            float64       \n 62   pf_rank                             float64       \n 63   ef_government_consumption           float64       \n 64   ef_government_transfers             float64       \n 65   ef_government_enterprises           float64       \n 66   ef_government_tax_income            float64       \n 67   ef_government_tax_payroll           float64       \n 68   ef_government_tax                   float64       \n 69   ef_government                       float64       \n 70   ef_legal_judicial                   float64       \n 71   ef_legal_courts                     float64       \n 72   ef_legal_protection                 float64       \n 73   ef_legal_military                   float64       \n 74   ef_legal_integrity                  float64       \n 75   ef_legal_enforcement                float64       \n 76   ef_legal_restrictions               float64       \n 77   ef_legal_police                     float64       \n 78   ef_legal_crime                      float64       \n 79   ef_legal_gender                     float64       \n 80   ef_legal                            float64       \n 81   ef_money_growth                     float64       \n 82   ef_money_sd                         float64       \n 83   ef_money_inflation                  float64       \n 84   ef_money_currency                   float64       \n 85   ef_money                            float64       \n 86   ef_trade_tariffs_revenue            float64       \n 87   ef_trade_tariffs_mean               float64       \n 88   ef_trade_tariffs_sd                 float64       \n 89   ef_trade_tariffs                    float64       \n 90   ef_trade_regulatory_nontariff       float64       \n 91   ef_trade_regulatory_compliance      float64       \n 92   ef_trade_regulatory                 float64       \n 93   ef_trade_black                      float64       \n 94   ef_trade_movement_foreign           float64       \n 95   ef_trade_movement_capital           float64       \n 96   ef_trade_movement_visit             float64       \n 97   ef_trade_movement                   float64       \n 98   ef_trade                            float64       \n 99   ef_regulation_credit_ownership      float64       \n 100  ef_regulation_credit_private        float64       \n 101  ef_regulation_credit_interest       float64       \n 102  ef_regulation_credit                float64       \n 103  ef_regulation_labor_minwage         float64       \n 104  ef_regulation_labor_firing          float64       \n 105  ef_regulation_labor_bargain         float64       \n 106  ef_regulation_labor_hours           float64       \n 107  ef_regulation_labor_dismissal       float64       \n 108  ef_regulation_labor_conscription    float64       \n 109  ef_regulation_labor                 float64       \n 110  ef_regulation_business_adm          float64       \n 111  ef_regulation_business_bureaucracy  float64       \n 112  ef_regulation_business_start        float64       \n 113  ef_regulation_business_bribes       float64       \n 114  ef_regulation_business_licensing    float64       \n 115  ef_regulation_business_compliance   float64       \n 116  ef_regulation_business              float64       \n 117  ef_regulation                       float64       \n 118  ef_score                            float64       \n 119  ef_rank                             float64       \n 120  hf_score                            float64       \n 121  hf_rank                             float64       \n 122  hf_quartile                         float64       \n 123  mean_hf_score                       float64       \n 124  median_hf_score                     float64       \n 125  mode_hf_score                       float64       \ndtypes: datetime64[ns](1), float64(122), object(3)\nmemory usage: 1.4+ MB\nNone"
  },
  {
    "objectID": "slides/06-preprocessing.html#converting-iso_code-countries-and-region-to-categorical",
    "href": "slides/06-preprocessing.html#converting-iso_code-countries-and-region-to-categorical",
    "title": "Data preprocessing",
    "section": "Converting ISO_code, countries, and region to Categorical",
    "text": "Converting ISO_code, countries, and region to Categorical\n\nhfi_copy['ISO_code'] = hfi_copy['ISO_code'].astype('category')\nhfi_copy['countries'] = hfi_copy['countries'].astype('category')\nhfi_copy['region'] = hfi_copy['region'].astype('category')\nprint(hfi_copy.info(verbose=True))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1458 entries, 0 to 1457\nData columns (total 126 columns):\n #    Column                              Dtype         \n---   ------                              -----         \n 0    year                                datetime64[ns]\n 1    ISO_code                            category      \n 2    countries                           category      \n 3    region                              category      \n 4    pf_rol_procedural                   float64       \n 5    pf_rol_civil                        float64       \n 6    pf_rol_criminal                     float64       \n 7    pf_rol                              float64       \n 8    pf_ss_homicide                      float64       \n 9    pf_ss_disappearances_disap          float64       \n 10   pf_ss_disappearances_violent        float64       \n 11   pf_ss_disappearances_organized      float64       \n 12   pf_ss_disappearances_fatalities     float64       \n 13   pf_ss_disappearances_injuries       float64       \n 14   pf_ss_disappearances                float64       \n 15   pf_ss_women_fgm                     float64       \n 16   pf_ss_women_missing                 float64       \n 17   pf_ss_women_inheritance_widows      float64       \n 18   pf_ss_women_inheritance_daughters   float64       \n 19   pf_ss_women_inheritance             float64       \n 20   pf_ss_women                         float64       \n 21   pf_ss                               float64       \n 22   pf_movement_domestic                float64       \n 23   pf_movement_foreign                 float64       \n 24   pf_movement_women                   float64       \n 25   pf_movement                         float64       \n 26   pf_religion_estop_establish         float64       \n 27   pf_religion_estop_operate           float64       \n 28   pf_religion_estop                   float64       \n 29   pf_religion_harassment              float64       \n 30   pf_religion_restrictions            float64       \n 31   pf_religion                         float64       \n 32   pf_association_association          float64       \n 33   pf_association_assembly             float64       \n 34   pf_association_political_establish  float64       \n 35   pf_association_political_operate    float64       \n 36   pf_association_political            float64       \n 37   pf_association_prof_establish       float64       \n 38   pf_association_prof_operate         float64       \n 39   pf_association_prof                 float64       \n 40   pf_association_sport_establish      float64       \n 41   pf_association_sport_operate        float64       \n 42   pf_association_sport                float64       \n 43   pf_association                      float64       \n 44   pf_expression_killed                float64       \n 45   pf_expression_jailed                float64       \n 46   pf_expression_influence             float64       \n 47   pf_expression_control               float64       \n 48   pf_expression_cable                 float64       \n 49   pf_expression_newspapers            float64       \n 50   pf_expression_internet              float64       \n 51   pf_expression                       float64       \n 52   pf_identity_legal                   float64       \n 53   pf_identity_parental_marriage       float64       \n 54   pf_identity_parental_divorce        float64       \n 55   pf_identity_parental                float64       \n 56   pf_identity_sex_male                float64       \n 57   pf_identity_sex_female              float64       \n 58   pf_identity_sex                     float64       \n 59   pf_identity_divorce                 float64       \n 60   pf_identity                         float64       \n 61   pf_score                            float64       \n 62   pf_rank                             float64       \n 63   ef_government_consumption           float64       \n 64   ef_government_transfers             float64       \n 65   ef_government_enterprises           float64       \n 66   ef_government_tax_income            float64       \n 67   ef_government_tax_payroll           float64       \n 68   ef_government_tax                   float64       \n 69   ef_government                       float64       \n 70   ef_legal_judicial                   float64       \n 71   ef_legal_courts                     float64       \n 72   ef_legal_protection                 float64       \n 73   ef_legal_military                   float64       \n 74   ef_legal_integrity                  float64       \n 75   ef_legal_enforcement                float64       \n 76   ef_legal_restrictions               float64       \n 77   ef_legal_police                     float64       \n 78   ef_legal_crime                      float64       \n 79   ef_legal_gender                     float64       \n 80   ef_legal                            float64       \n 81   ef_money_growth                     float64       \n 82   ef_money_sd                         float64       \n 83   ef_money_inflation                  float64       \n 84   ef_money_currency                   float64       \n 85   ef_money                            float64       \n 86   ef_trade_tariffs_revenue            float64       \n 87   ef_trade_tariffs_mean               float64       \n 88   ef_trade_tariffs_sd                 float64       \n 89   ef_trade_tariffs                    float64       \n 90   ef_trade_regulatory_nontariff       float64       \n 91   ef_trade_regulatory_compliance      float64       \n 92   ef_trade_regulatory                 float64       \n 93   ef_trade_black                      float64       \n 94   ef_trade_movement_foreign           float64       \n 95   ef_trade_movement_capital           float64       \n 96   ef_trade_movement_visit             float64       \n 97   ef_trade_movement                   float64       \n 98   ef_trade                            float64       \n 99   ef_regulation_credit_ownership      float64       \n 100  ef_regulation_credit_private        float64       \n 101  ef_regulation_credit_interest       float64       \n 102  ef_regulation_credit                float64       \n 103  ef_regulation_labor_minwage         float64       \n 104  ef_regulation_labor_firing          float64       \n 105  ef_regulation_labor_bargain         float64       \n 106  ef_regulation_labor_hours           float64       \n 107  ef_regulation_labor_dismissal       float64       \n 108  ef_regulation_labor_conscription    float64       \n 109  ef_regulation_labor                 float64       \n 110  ef_regulation_business_adm          float64       \n 111  ef_regulation_business_bureaucracy  float64       \n 112  ef_regulation_business_start        float64       \n 113  ef_regulation_business_bribes       float64       \n 114  ef_regulation_business_licensing    float64       \n 115  ef_regulation_business_compliance   float64       \n 116  ef_regulation_business              float64       \n 117  ef_regulation                       float64       \n 118  ef_score                            float64       \n 119  ef_rank                             float64       \n 120  hf_score                            float64       \n 121  hf_rank                             float64       \n 122  hf_quartile                         float64       \n 123  mean_hf_score                       float64       \n 124  median_hf_score                     float64       \n 125  mode_hf_score                       float64       \ndtypes: category(3), datetime64[ns](1), float64(122)\nmemory usage: 1.4 MB\nNone"
  },
  {
    "objectID": "slides/06-preprocessing.html#creating-a-boolean-column-from-hf_score",
    "href": "slides/06-preprocessing.html#creating-a-boolean-column-from-hf_score",
    "title": "Data preprocessing",
    "section": "Creating a boolean column from hf_score",
    "text": "Creating a boolean column from hf_score\nLet’s say we want to know whether countries are “free” or not.\n\nthreshold = 8.5\nhfi_copy['hf_score_above_threshold'] = hfi_copy['hf_score'] &gt; threshold"
  },
  {
    "objectID": "slides/06-preprocessing.html#filter-out-true-values",
    "href": "slides/06-preprocessing.html#filter-out-true-values",
    "title": "Data preprocessing",
    "section": "Filter out True values",
    "text": "Filter out True values\n\nfiltered_hfi_copy = hfi_copy[hfi_copy['hf_score_above_threshold'] == True][['hf_score_above_threshold', 'countries']]\n\nprint(filtered_hfi_copy[['hf_score_above_threshold', 'countries']])\n\n      hf_score_above_threshold       countries\n5                         True       Australia\n27                        True          Canada\n41                        True         Denmark\n63                        True       Hong Kong\n70                        True         Ireland\n...                        ...             ...\n1359                      True       Hong Kong\n1403                      True     New Zealand\n1407                      True          Norway\n1436                      True     Switzerland\n1450                      True  United Kingdom\n\n[83 rows x 2 columns]\n\n\n\n\nThat’s a lot of data… 🤔"
  },
  {
    "objectID": "slides/06-preprocessing.html#filter-out-true-values-for-newest-year",
    "href": "slides/06-preprocessing.html#filter-out-true-values-for-newest-year",
    "title": "Data preprocessing",
    "section": "Filter out True values for newest year",
    "text": "Filter out True values for newest year\n\nnewest_year = hfi_copy['year'].max()\nfiltered_hfi_copy = hfi_copy[(hfi_copy['year'] == newest_year) & (hfi_copy['hf_score_above_threshold'] == True)]\n\nresult = filtered_hfi_copy[['hf_score_above_threshold', 'countries']]\n\nprint(result)\n\n     hf_score_above_threshold    countries\n5                        True    Australia\n27                       True       Canada\n41                       True      Denmark\n63                       True    Hong Kong\n70                       True      Ireland\n106                      True  Netherlands\n107                      True  New Zealand\n140                      True  Switzerland"
  },
  {
    "objectID": "slides/06-preprocessing.html#filtering-categories",
    "href": "slides/06-preprocessing.html#filtering-categories",
    "title": "Data preprocessing",
    "section": "Filtering categories",
    "text": "Filtering categories\n\noptions = ['United States', 'India', 'Canada', 'China']\nfiltered_hfi = hfi[hfi['countries'].isin(options)]\nfiltered_hfi['countries'] = filtered_hfi['countries'].cat.remove_unused_categories()\nunique_countries = filtered_hfi['countries'].unique()\nprint(unique_countries)\n\n['Canada', 'China', 'India', 'United States']\nCategories (4, object): ['Canada', 'China', 'India', 'United States']"
  },
  {
    "objectID": "slides/06-preprocessing.html#normalizing",
    "href": "slides/06-preprocessing.html#normalizing",
    "title": "Data preprocessing",
    "section": "Normalizing",
    "text": "Normalizing\n\nStandard deviationZ-scoreMin-maxMaximum absolute\n\n\n\n\nMean: 5\nStandard Deviation: 2\n\n\n\n\n\n\n\n\n\n\n\n\nhfi_copy = hfi.copy()\n\nscaler = StandardScaler()\nhfi_copy[['ef_score_scale', 'pf_score_scale']] = scaler.fit_transform(hfi_copy[['ef_score', 'pf_score']])\n\nhfi_copy[['ef_score_scale', 'pf_score_scale']].describe()\n\n\n\n\n\n\n\n\nef_score_scale\npf_score_scale\n\n\n\n\ncount\n1.378000e+03\n1.378000e+03\n\n\nmean\n4.524683e-16\n2.062533e-17\n\n\nstd\n1.000363e+00\n1.000363e+00\n\n\nmin\n-4.421711e+00\n-3.663087e+00\n\n\n25%\n-6.063870e-01\n-7.303950e-01\n\n\n50%\n1.295064e-01\n-8.926277e-03\n\n\n75%\n7.068997e-01\n9.081441e-01\n\n\nmax\n2.722116e+00\n1.722056e+00\n\n\n\n\n\n\n\n\n\n\nmin_max_scaler = MinMaxScaler()\nhfi_copy[['ef_score_minmax', 'pf_score_minmax']] = min_max_scaler.fit_transform(hfi_copy[['ef_score', 'pf_score']])\n\nhfi_copy[['ef_score_minmax', 'pf_score_minmax']].describe()\n\n\n\n\n\n\n\n\nef_score_minmax\npf_score_minmax\n\n\n\n\ncount\n1378.000000\n1378.000000\n\n\nmean\n0.618956\n0.680221\n\n\nstd\n0.140032\n0.185764\n\n\nmin\n0.000000\n0.000000\n\n\n25%\n0.534073\n0.544589\n\n\n50%\n0.637084\n0.678563\n\n\n75%\n0.717908\n0.848860\n\n\nmax\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\nmax_abs_scaler = MaxAbsScaler()\nhfi_copy[['ef_score_maxabs', 'pf_score_maxabs']] = max_abs_scaler.fit_transform(hfi_copy[['ef_score', 'pf_score']])\n\nhfi_copy[['ef_score_maxabs', 'pf_score_maxabs']].describe()\n\n\n\n\n\n\n\n\nef_score_maxabs\npf_score_maxabs\n\n\n\n\ncount\n1378.000000\n1378.000000\n\n\nmean\n0.738369\n0.752630\n\n\nstd\n0.096148\n0.143700\n\n\nmin\n0.313384\n0.226434\n\n\n25%\n0.680087\n0.647710\n\n\n50%\n0.750816\n0.751348\n\n\n75%\n0.806311\n0.883083\n\n\nmax\n1.000000\n1.000000"
  },
  {
    "objectID": "slides/06-preprocessing.html#visual-comparison",
    "href": "slides/06-preprocessing.html#visual-comparison",
    "title": "Data preprocessing",
    "section": "Visual comparison",
    "text": "Visual comparison"
  },
  {
    "objectID": "slides/06-preprocessing.html#pros-cons-standardscaler",
    "href": "slides/06-preprocessing.html#pros-cons-standardscaler",
    "title": "Data preprocessing",
    "section": "Pros + cons (StandardScaler)",
    "text": "Pros + cons (StandardScaler)\nPros:\n\n\nNormalization of Variance: Centers data around zero with a standard deviation of one, suitable for algorithms assuming normally distributed data (e.g., linear regression, logistic regression, neural networks).\nPreserves Relationships: Maintains ratios and differences between data points.\n\n\nCons:\n\n\nSensitive to Outliers: Outliers can distort scaled values.\nAssumes Normality: Assumes data follows a Gaussian distribution."
  },
  {
    "objectID": "slides/06-preprocessing.html#pros-cons-maxabsscaler",
    "href": "slides/06-preprocessing.html#pros-cons-maxabsscaler",
    "title": "Data preprocessing",
    "section": "Pros + cons (MaxAbsScaler)",
    "text": "Pros + cons (MaxAbsScaler)\nPros:\n\n\nOutlier Resistant: Less sensitive to outliers, scales based on the absolute maximum value.\nPreserves Sparsity: Does not center data, preserving the sparsity pattern.\n\n\nCons:\n\n\nScale Limitation: Scales to the range [-1, 1], which may not suit all algorithms.\nNot Zero-Centered: May be a limitation for algorithms preferring zero-centered data."
  },
  {
    "objectID": "slides/06-preprocessing.html#pros-cons-minmaxscaler",
    "href": "slides/06-preprocessing.html#pros-cons-minmaxscaler",
    "title": "Data preprocessing",
    "section": "Pros + cons (MinMaxScaler)",
    "text": "Pros + cons (MinMaxScaler)\nPros:\n\n\nFixed Range: Scales data to a fixed range (usually [0, 1]), beneficial for algorithms sensitive to feature scales (e.g., neural networks, k-nearest neighbors).\nPreserves Relationships: Maintains relationships between data points.\n\n\nCons:\n\n\nSensitive to Outliers: Outliers can skew scaled values.\nRange Dependence: Scaling depends on the min and max values in the training data, which may not generalize well to new data."
  },
  {
    "objectID": "slides/06-preprocessing.html#normality-test-q-q-plot",
    "href": "slides/06-preprocessing.html#normality-test-q-q-plot",
    "title": "Data preprocessing",
    "section": "Normality test: Q-Q plot",
    "text": "Normality test: Q-Q plot\n\nQ-Q PlotIssues\n\n\n\n\nCode\nhfi_clean = hfi_copy.dropna(subset = ['pf_score'])\n\nsns.set_style(\"white\")\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\nsns.kdeplot(data = hfi_clean, x = \"hf_score\", linewidth = 5, ax = ax1)\nax1.set_title('Human Freedom Score')\n\nsm.qqplot(hfi_clean['hf_score'], line = 's', ax = ax2, dist = stats.norm, fit = True)\nax2.set_title('Human Freedom Score Q-Q plot')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThere were some issues in our plots:\n\n\nLeft Tail: Points deviate downwards from the line, indicating more extreme low values than a normal distribution (negative skewness).\nCentral Section: Points align closely with the line, suggesting the central data is similar to a normal distribution.\nRight Tail: Points curve upwards, showing potential for extreme high values (positive skewness)."
  },
  {
    "objectID": "slides/06-preprocessing.html#correcting-skew",
    "href": "slides/06-preprocessing.html#correcting-skew",
    "title": "Data preprocessing",
    "section": "Correcting skew",
    "text": "Correcting skew\n\nSquare-root transformation. \\(\\sqrt x\\) Used for moderately right-skew (positive skew)\n\n\nCannot handle negative values (but can handle zeros)\n\n\n\n\nLog transformation. \\(log(x + 1)\\) Used for substantial right-skew (positive skew)\n\n\nCannot handle negative or zero values\n\n\n\n\nSquared transformation. \\(x^2\\) Used for moderately left-skew (negative skew)\n\n\nEffective when lower values are densely packed together"
  },
  {
    "objectID": "slides/06-preprocessing.html#comparing-transformations",
    "href": "slides/06-preprocessing.html#comparing-transformations",
    "title": "Data preprocessing",
    "section": "Comparing transformations",
    "text": "Comparing transformations\n\nOriginalSquare-rootLogSquared\n\n\n\n\n\n\n\n\n\n\n\n\nModerate negative skew, no zeros or negative values\n\n\n\n\n\nCode\nhfi_clean['hf_score_sqrt'] = np.sqrt(hfi_clean['hf_score'])\n\ncol = hfi_clean['hf_score_sqrt']\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Square-root Density plot')    \n\nsm.qqplot(col, line = 's', ax = ax2)\nax2.set_title('Square-root Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nhfi_clean['hf_score_log'] = np.log(hfi_clean['hf_score'] + 1)\n\ncol = hfi_clean['hf_score_log']\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Log Density plot')    \n\nsm.qqplot(col, line = 's', ax = ax2)\nax2.set_title('Log Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nhfi_clean['hf_score_square'] = pow(hfi_clean.hf_score, 2)\n\ncol = hfi_clean['hf_score_square']\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Squared Density plot')    \n\nsm.qqplot(col, line = 's', ax = ax2)\nax2.set_title('Squared Q-Q plot')    \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/06-preprocessing.html#what-did-we-learn",
    "href": "slides/06-preprocessing.html#what-did-we-learn",
    "title": "Data preprocessing",
    "section": "What did we learn?",
    "text": "What did we learn?\n\n\nNegative skew excluded all but Squared transformation\n… thus, Squared transformation was the best\nThe data is bimodal, so no transformation is perfect"
  },
  {
    "objectID": "slides/06-preprocessing.html#answering-our-question",
    "href": "slides/06-preprocessing.html#answering-our-question",
    "title": "Data preprocessing",
    "section": "Answering our question",
    "text": "Answering our question\nWhat trends are there within human freedom indices in different regions?\n\n\n\n\n\n\n\n\n\n\nProbably…\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#eigenvalues-and-eigenvectors",
    "href": "slides/23-linear-algebra-II.html#eigenvalues-and-eigenvectors",
    "title": "Linear algebra II",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\n\n\nEigenvalues and eigenvectors decompose a matrix into its fundamental components.\nEigenvalue equation: \\(\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\\)"
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#calculating-eigenvalues-and-eigenvectors",
    "href": "slides/23-linear-algebra-II.html#calculating-eigenvalues-and-eigenvectors",
    "title": "Linear algebra II",
    "section": "Calculating Eigenvalues and Eigenvectors",
    "text": "Calculating Eigenvalues and Eigenvectors\n\nWe will be using Python for this…\n\n\n\nimport numpy as np\nfrom numpy.linalg import eig\nA = np.array([[1, 2], [4, 5]])\neigenvals, eigenvecs = eig(A)\nprint(\"EIGENVALUES:\", eigenvals)\nprint(\"EIGENVECTORS:\", eigenvecs)\n\nEIGENVALUES: [-0.46410162  6.46410162]\nEIGENVECTORS: [[-0.80689822 -0.34372377]\n [ 0.59069049 -0.9390708 ]]"
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#eigen-decomposition",
    "href": "slides/23-linear-algebra-II.html#eigen-decomposition",
    "title": "Linear algebra II",
    "section": "Eigen decomposition",
    "text": "Eigen decomposition\n\nDecomposition formula: \\(\\mathbf{A} = \\mathbf{Q} \\mathbf{L} \\mathbf{Q}^{-1}\\)\n\\(\\mathbf{Q}\\) is the matrix of eigenvectors, \\(\\mathbf{L}\\) is the diagonal matrix of eigenvalues, and \\(\\mathbf{Q}^{-1}\\) is the inverse of \\(\\mathbf{Q}\\)"
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#recomposing-matrices",
    "href": "slides/23-linear-algebra-II.html#recomposing-matrices",
    "title": "Linear algebra II",
    "section": "Recomposing matrices",
    "text": "Recomposing matrices\n\nfrom numpy import diag\nfrom numpy.linalg import inv\nQ = eigenvecs\nL = diag(eigenvals)\nR = inv(Q)\nB = Q @ L @ R\nprint(B)\n\n[[1. 2.]\n [4. 5.]]"
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#applications-in-data-science-and-machine-learning",
    "href": "slides/23-linear-algebra-II.html#applications-in-data-science-and-machine-learning",
    "title": "Linear algebra II",
    "section": "Applications in Data Science and Machine Learning",
    "text": "Applications in Data Science and Machine Learning\n\nPrincipal Component Analysis (PCA): Reduces dimensionality while preserving variance.\nEigenvalues in system stability: Determine stability in control systems and differential equations."
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#special-types-of-matrices",
    "href": "slides/23-linear-algebra-II.html#special-types-of-matrices",
    "title": "Linear algebra II",
    "section": "Special Types of Matrices",
    "text": "Special Types of Matrices\n\n\n\nIdentity Matrix: Diagonal of 1s, other elements are 0s.\n\\[\n\\mathbf{I} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\n\nDiagonal Matrix: Non-zero elements only on the diagonal.\n\\[\n\\mathbf{D} = \\begin{bmatrix}\n4 & 0 & 0 \\\\\n0 & 5 & 0 \\\\\n0 & 0 & 6\n\\end{bmatrix}\n\\]\n\n\n\nTriangular Matrix: Triangular shape of non-zero elements (upper \\(\\mathbf{U}\\), lower \\(\\mathbf{L}\\)).\n\\[\n\\mathbf{U} = \\begin{bmatrix}1 & 2 & 3 \\\\0 & 4 & 5 \\\\0 & 0 & 6\\end{bmatrix} \\\\ \\mathbf{L} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 3 & 0 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/23-linear-algebra-II.html#ae-16-pca",
    "href": "slides/23-linear-algebra-II.html#ae-16-pca",
    "title": "Linear algebra II",
    "section": "ae-16-pca",
    "text": "ae-16-pca\nPrincipal Component Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/08-web-scraping.html#setup",
    "href": "slides/08-web-scraping.html#setup",
    "title": "Web scraping",
    "section": "Setup",
    "text": "Setup\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom afinn import Afinn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport numpy as np\nimport ssl\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "slides/08-web-scraping.html#before-continuing",
    "href": "slides/08-web-scraping.html#before-continuing",
    "title": "Web scraping",
    "section": "Before continuing…",
    "text": "Before continuing…\n\n\nIf you haven’t yet done so: Install a Chrome browser and the SelectorGadget extension:\n\nChrome\nSelectorGadget\n\nGo to your ae repo, commit any remaining changes, push, and then pull for today’s application exercise."
  },
  {
    "objectID": "slides/08-web-scraping.html#reading-the-arizona-daily-wildcat",
    "href": "slides/08-web-scraping.html#reading-the-arizona-daily-wildcat",
    "title": "Web scraping",
    "section": "Reading The Arizona Daily Wildcat",
    "text": "Reading The Arizona Daily Wildcat\n\nHow often do you read The Arizona Daily Wildcat?\n\nEvery day\n3-5 times a week\nOnce a week\nRarely"
  },
  {
    "objectID": "slides/08-web-scraping.html#reading-the-arizona-daily-wildcat-1",
    "href": "slides/08-web-scraping.html#reading-the-arizona-daily-wildcat-1",
    "title": "Web scraping",
    "section": "Reading The Arizona Daily Wildcat",
    "text": "Reading The Arizona Daily Wildcat\n\nWhat do you think is the most common word in the titles of The Arizona Daily Wildcat opinion pieces?"
  },
  {
    "objectID": "slides/08-web-scraping.html#analyzing-the-arizona-daily-wildcat",
    "href": "slides/08-web-scraping.html#analyzing-the-arizona-daily-wildcat",
    "title": "Web scraping",
    "section": "Analyzing The Arizona Daily Wildcat",
    "text": "Analyzing The Arizona Daily Wildcat"
  },
  {
    "objectID": "slides/08-web-scraping.html#analyzing-the-arizona-daily-wildcat-1",
    "href": "slides/08-web-scraping.html#analyzing-the-arizona-daily-wildcat-1",
    "title": "Web scraping",
    "section": "Analyzing The Arizona Daily Wildcat",
    "text": "Analyzing The Arizona Daily Wildcat"
  },
  {
    "objectID": "slides/08-web-scraping.html#all-of-this-analysis-is-done-in-python",
    "href": "slides/08-web-scraping.html#all-of-this-analysis-is-done-in-python",
    "title": "Web scraping",
    "section": "All of this analysis is done in Python!",
    "text": "All of this analysis is done in Python!\n\n(mostly) with tools you already know!"
  },
  {
    "objectID": "slides/08-web-scraping.html#common-works-in-the-arizona-daily-wildcat-titles",
    "href": "slides/08-web-scraping.html#common-works-in-the-arizona-daily-wildcat-titles",
    "title": "Web scraping",
    "section": "Common works in The Arizona Daily Wildcat titles",
    "text": "Common works in The Arizona Daily Wildcat titles\nCode for the earlier plot:\n\nstop_words = set(stopwords.words('english'))\nwildcat['tokens'] = wildcat['title'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.isalpha() and word.lower() not in stop_words])\n\nword_counts = Counter(word for title in wildcat['tokens'] for word in title)\ncommon_words = pd.DataFrame(word_counts.most_common(20), columns=['word', 'count'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='count', y='word', data=common_words, palette='viridis')\nplt.xlabel('Number of mentions')\nplt.ylabel('Word')\nplt.title('Arizona Daily Wildcat - Opinion pieces\\nCommon words in the most recent opinion pieces')\nplt.show()"
  },
  {
    "objectID": "slides/08-web-scraping.html#avg-sentiment-scores-of-titles",
    "href": "slides/08-web-scraping.html#avg-sentiment-scores-of-titles",
    "title": "Web scraping",
    "section": "Avg sentiment scores of titles",
    "text": "Avg sentiment scores of titles\nCode for the earlier plot:\n\nstop_words = set(stopwords.words('english'))\nwildcat['tokens'] = wildcat['title'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.isalpha() and word.lower() not in stop_words])\n\nafinn = Afinn()\n\nwildcat['sentiment'] = wildcat['title'].apply(lambda x: afinn.score(x))\nauthor_sentiment = wildcat.groupby(['author', 'title'])['sentiment'].sum().reset_index()\n\nauthor_summary = author_sentiment.groupby('author').agg(n_articles=('title', 'count'), avg_sentiment=('sentiment', 'mean')).reset_index()\nauthor_summary = author_summary[(author_summary['n_articles'] &gt; 1) & (author_summary['author'].notna())]\n\ntop_positive = author_summary.nlargest(10, 'avg_sentiment')\ntop_negative = author_summary.nsmallest(10, 'avg_sentiment')\ntop_authors = pd.concat([top_positive, top_negative])\n\ntop_authors['neg_pos'] = np.where(top_authors['avg_sentiment'] &lt; 0, 'neg', 'pos')\ntop_authors['label_position'] = np.where(top_authors['neg_pos'] == 'neg', 0.25, -0.25)\ntop_authors = top_authors.sort_values(by='avg_sentiment', ascending=True)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='avg_sentiment', y='author', data=top_authors, hue='neg_pos', dodge=False, palette={'neg': '#4d4009', 'pos': '#FF4B91'})\nplt.xlabel('negative  ←     Average sentiment score (AFINN)     →  positive')\nplt.ylabel(None)\nplt.title('The Arizona Daily Wildcat - Opinion pieces\\nAverage sentiment scores of titles by author')\nplt.legend([], [], frameon=False)\nplt.xlim(-5, 5)\nplt.grid(False)\nplt.gca().invert_yaxis()\nplt.show()"
  },
  {
    "objectID": "slides/08-web-scraping.html#where-is-this-data-coming-from",
    "href": "slides/08-web-scraping.html#where-is-this-data-coming-from",
    "title": "Web scraping",
    "section": "Where is this data coming from?",
    "text": "Where is this data coming from?\nhttps://wildcat.arizona.edu/category/opinions/"
  },
  {
    "objectID": "slides/08-web-scraping.html#where-is-this-data-coming-from-1",
    "href": "slides/08-web-scraping.html#where-is-this-data-coming-from-1",
    "title": "Web scraping",
    "section": "Where is this data coming from?",
    "text": "Where is this data coming from?\n\n\n\n\n\n\n\n\n\nprint(wildcat)\n\n                                                 title            author  \\\n0    BOOK REVIEW: ‘Fresh Fruit, Broken Bodies’ by D...    Andres F. Diaz   \n1    OPINION: The first presidential debate lacked ...       Luke Lawson   \n2    OPINION: College WBB favorites and sleeper pic...  Melisa Guzeloglu   \n3    OPINION: College MBB favorites and sleeper pic...   Nathaniel Levin   \n4    EDITORIAL: A desk altered but opinions thrive ...   Editor-in-Chief   \n..                                                 ...               ...   \n995                      Here’s how to best help Nepal    Hailey Dickson   \n996                        Adderall abuse not harmless    Maddie Pickens   \n997                     Court rule is legitimate judge   Jacob Winkelman   \n998                 Letters to the editor: May 4, 2015  Gabriel Schivone   \n999            Capability imperfectly captured by TCEs    Maddie Pickens   \n\n               date  abstract   column  \\\n0     July 22, 2024       NaN  Opinion   \n1      July 3, 2024       NaN  Opinion   \n2    March 15, 2024       NaN  Opinion   \n3    March 15, 2024       NaN  Opinion   \n4    March 15, 2024       NaN  Opinion   \n..              ...       ...      ...   \n995     May 5, 2015       NaN  Opinion   \n996     May 5, 2015       NaN  Opinion   \n997     May 4, 2015       NaN  Opinion   \n998     May 4, 2015       NaN  Opinion   \n999     May 4, 2015       NaN  Opinion   \n\n                                                   url  \\\n0    https://wildcat.arizona.edu/155604/opinions/bo...   \n1    https://wildcat.arizona.edu/155594/opinions/op...   \n2    https://wildcat.arizona.edu/154146/opinions/s-...   \n3    https://wildcat.arizona.edu/154116/opinions/s-...   \n4    https://wildcat.arizona.edu/154126/opinions/ed...   \n..                                                 ...   \n995  https://wildcat.arizona.edu/123054/opinions/he...   \n996  https://wildcat.arizona.edu/102511/opinions/ad...   \n997  https://wildcat.arizona.edu/127949/opinions/co...   \n998  https://wildcat.arizona.edu/142548/opinions/le...   \n999  https://wildcat.arizona.edu/100073/opinions/ca...   \n\n                                                tokens  sentiment  \n0    [book, review, fresh, fruit, broken, bodies, s...        0.0  \n1    [opinion, first, presidential, debate, lacked,...        0.0  \n2    [opinion, college, wbb, favorites, sleeper, pi...       -1.0  \n3    [opinion, college, mbb, favorites, sleeper, pi...       -1.0  \n4    [editorial, desk, altered, opinions, thrive, w...        0.0  \n..                                                 ...        ...  \n995                                [best, help, nepal]        5.0  \n996                        [adderall, abuse, harmless]       -3.0  \n997                   [court, rule, legitimate, judge]        0.0  \n998                             [letters, editor, may]        0.0  \n999          [capability, imperfectly, captured, tces]        1.0  \n\n[1000 rows x 8 columns]"
  },
  {
    "objectID": "slides/08-web-scraping.html#scraping-the-web-what-why",
    "href": "slides/08-web-scraping.html#scraping-the-web-what-why",
    "title": "Web scraping",
    "section": "Scraping the web: what? why?",
    "text": "Scraping the web: what? why?\n\nIncreasing amount of data is available on the web\nThese data are provided in an unstructured format: you can always copy&paste, but it’s time-consuming and prone to errors\nWeb scraping is the process of extracting this information automatically and transform it into a structured dataset\nTwo different scenarios:\n\nScreen scraping: extract data from source code of website, with html parser (easy) or regular expression matching (less easy).\nWeb APIs (application programming interface): website offers a set of structured http requests that return JSON or XML files."
  },
  {
    "objectID": "slides/08-web-scraping.html#hypertext-markup-language",
    "href": "slides/08-web-scraping.html#hypertext-markup-language",
    "title": "Web scraping",
    "section": "Hypertext Markup Language",
    "text": "Hypertext Markup Language\nMost of the data on the web is still largely available as HTML - while it is structured (hierarchical) it often is not available in a form useful for analysis (flat / tidy).\n\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;This is a title&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p align=\"center\"&gt;Hello world!&lt;/p&gt;\n    &lt;br/&gt;\n    &lt;div class=\"name\" id=\"first\"&gt;John&lt;/div&gt;\n    &lt;div class=\"name\" id=\"last\"&gt;Doe&lt;/div&gt;\n    &lt;div class=\"contact\"&gt;\n      &lt;div class=\"home\"&gt;555-555-1234&lt;/div&gt;\n      &lt;div class=\"home\"&gt;555-555-2345&lt;/div&gt;\n      &lt;div class=\"work\"&gt;555-555-9999&lt;/div&gt;\n      &lt;div class=\"fax\"&gt;555-555-8888&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "slides/08-web-scraping.html#beautifulsoup",
    "href": "slides/08-web-scraping.html#beautifulsoup",
    "title": "Web scraping",
    "section": "BeautifulSoup",
    "text": "BeautifulSoup\n\n\n\nThe BeautifulSoup package makes basic processing and manipulation of HTML data straight forward\nbeautiful-soup-4.readthedocs.io\n\n\nfrom bs4 import BeautifulSoup"
  },
  {
    "objectID": "slides/08-web-scraping.html#beautifulsoup-1",
    "href": "slides/08-web-scraping.html#beautifulsoup-1",
    "title": "Web scraping",
    "section": "BeautifulSoup",
    "text": "BeautifulSoup\nCore functions:\n\nrequests.get(url) - send an HTTP GET request to a URL\nBeautifulSoup(html, 'html.parser') - parse HTML data from a string\nsoup.select('selector') - select specified elements from the HTML document using CSS selectors\nelement.get_text() - extract text content from an element\nelement['attribute'] - extract attribute value from an element"
  },
  {
    "objectID": "slides/08-web-scraping.html#html-beautifulsoup-requests",
    "href": "slides/08-web-scraping.html#html-beautifulsoup-requests",
    "title": "Web scraping",
    "section": "HTML, BeautifulSoup, & requests",
    "text": "HTML, BeautifulSoup, & requests\n\nhtml = '''\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;This is a title&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;p align=\"center\"&gt;Hello world!&lt;/p&gt;\n    &lt;br/&gt;\n    &lt;div class=\"name\" id=\"first\"&gt;John&lt;/div&gt;\n    &lt;div class=\"name\" id=\"last\"&gt;Doe&lt;/div&gt;\n    &lt;div class=\"contact\"&gt;\n      &lt;div class=\"home\"&gt;555-555-1234&lt;/div&gt;\n      &lt;div class=\"home\"&gt;555-555-2345&lt;/div&gt;\n      &lt;div class=\"work\"&gt;555-555-9999&lt;/div&gt;\n      &lt;div class=\"fax\"&gt;555-555-8888&lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n'''\n\n\n\nsoup = BeautifulSoup(html, 'html.parser')"
  },
  {
    "objectID": "slides/08-web-scraping.html#selecting-elements",
    "href": "slides/08-web-scraping.html#selecting-elements",
    "title": "Web scraping",
    "section": "Selecting elements",
    "text": "Selecting elements\n\np_elements = soup.select('p')\nprint(p_elements)\n\n[&lt;p align=\"center\"&gt;Hello world!&lt;/p&gt;]\n\n\n\n\np_text = [p.get_text() for p in p_elements]\nprint(p_text)\n\n['Hello world!']\n\n\n\n\n\np_names = [p.name for p in p_elements]\nprint(p_names)\n\n['p']\n\n\n\n\n\np_attrs = [p.attrs for p in p_elements]\nprint(p_attrs)\n\n[{'align': 'center'}]\n\n\n\n\n\np_align = [p['align'] for p in p_elements if 'align' in p.attrs]\nprint(p_align)\n\n['center']"
  },
  {
    "objectID": "slides/08-web-scraping.html#more-selecting-tags",
    "href": "slides/08-web-scraping.html#more-selecting-tags",
    "title": "Web scraping",
    "section": "More selecting tags",
    "text": "More selecting tags\n\n\ndiv_elements = soup.select('div')\nprint(div_elements)\n\n[&lt;div class=\"name\" id=\"first\"&gt;John&lt;/div&gt;, &lt;div class=\"name\" id=\"last\"&gt;Doe&lt;/div&gt;, &lt;div class=\"contact\"&gt;\n&lt;div class=\"home\"&gt;555-555-1234&lt;/div&gt;\n&lt;div class=\"home\"&gt;555-555-2345&lt;/div&gt;\n&lt;div class=\"work\"&gt;555-555-9999&lt;/div&gt;\n&lt;div class=\"fax\"&gt;555-555-8888&lt;/div&gt;\n&lt;/div&gt;, &lt;div class=\"home\"&gt;555-555-1234&lt;/div&gt;, &lt;div class=\"home\"&gt;555-555-2345&lt;/div&gt;, &lt;div class=\"work\"&gt;555-555-9999&lt;/div&gt;, &lt;div class=\"fax\"&gt;555-555-8888&lt;/div&gt;]\n\n\n\n\n\n\ndiv_text = [div.get_text() for div in div_elements]\nprint(div_text)\n\n['John', 'Doe', '\\n555-555-1234\\n555-555-2345\\n555-555-9999\\n555-555-8888\\n', '555-555-1234', '555-555-2345', '555-555-9999', '555-555-8888']"
  },
  {
    "objectID": "slides/08-web-scraping.html#css-selectors",
    "href": "slides/08-web-scraping.html#css-selectors",
    "title": "Web scraping",
    "section": "CSS selectors",
    "text": "CSS selectors\n\nWe will use a tool called SelectorGadget to help us identify the HTML elements of interest by constructing a CSS selector which can be used to subset the HTML document.\nSome examples of basic selector syntax is below,\n\n\n\n\n\n\n\n\n\n\nSelector\nExample\nDescription\n\n\n\n\n.class\n.title\nSelect all elements with class=“title”\n\n\n#id\n#name\nSelect all elements with id=“name”\n\n\nelement\np\nSelect all &lt;p&gt; elements\n\n\nelement element\ndiv p\nSelect all &lt;p&gt; elements inside a &lt;div&gt; element\n\n\nelement&gt;element\ndiv &gt; p\nSelect all &lt;p&gt; elements with &lt;div&gt; as a parent\n\n\n[attribute]\n[class]\nSelect all elements with a class attribute\n\n\n[attribute=value]\n[class=title]\nSelect all elements with class=“title”"
  },
  {
    "objectID": "slides/08-web-scraping.html#css-classes-and-ids",
    "href": "slides/08-web-scraping.html#css-classes-and-ids",
    "title": "Web scraping",
    "section": "CSS classes and ids",
    "text": "CSS classes and ids\n\nname_elements = soup.select('.name')\nprint(name_elements)\n\n[&lt;div class=\"name\" id=\"first\"&gt;John&lt;/div&gt;, &lt;div class=\"name\" id=\"last\"&gt;Doe&lt;/div&gt;]\n\n\n\n\nfirst_name = soup.select('#first')\nprint(first_name)\n\n[&lt;div class=\"name\" id=\"first\"&gt;John&lt;/div&gt;]"
  },
  {
    "objectID": "slides/08-web-scraping.html#text-with-get_text",
    "href": "slides/08-web-scraping.html#text-with-get_text",
    "title": "Web scraping",
    "section": "Text with get_text()",
    "text": "Text with get_text()\n\nhtml = '''\n&lt;p&gt;  \n  This is the first sentence in the paragraph.\n  This is the second sentence that should be on the same line as the first sentence.&lt;br&gt;This third sentence should start on a new line.\n&lt;/p&gt;\n'''\n\n\n\nsoup = BeautifulSoup(html, 'html.parser')\ntext = soup.get_text()\nprint(text)\n\n\n  \n  This is the first sentence in the paragraph.\n  This is the second sentence that should be on the same line as the first sentence.This third sentence should start on a new line."
  },
  {
    "objectID": "slides/08-web-scraping.html#html-tables-with-read_html",
    "href": "slides/08-web-scraping.html#html-tables-with-read_html",
    "title": "Web scraping",
    "section": "HTML tables with read_html()",
    "text": "HTML tables with read_html()\n\nhtml_table = '''\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;This is a title&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;table&gt;\n      &lt;tr&gt; &lt;th&gt;a&lt;/th&gt; &lt;th&gt;b&lt;/th&gt; &lt;th&gt;c&lt;/th&gt; &lt;/tr&gt;\n      &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt;\n      &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt;\n      &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;/tr&gt;\n    &lt;/table&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n'''\n\n\n\nsoup = BeautifulSoup(html_table, 'html.parser')\ntable = pd.read_html(str(soup.select('table')[0]))[0]\nprint(table)\n\n   a  b  c\n0  1  2  3\n1  2  3  4\n2  3  4  5"
  },
  {
    "objectID": "slides/08-web-scraping.html#selectorgadget",
    "href": "slides/08-web-scraping.html#selectorgadget",
    "title": "Web scraping",
    "section": "SelectorGadget",
    "text": "SelectorGadget\nSelectorGadget (selectorgadget.com) is a javascript based tool that helps you interactively build an appropriate CSS selector for the content you are interested in."
  },
  {
    "objectID": "slides/08-web-scraping.html#opinion-articles-in-the-arizona-daily-wildcat",
    "href": "slides/08-web-scraping.html#opinion-articles-in-the-arizona-daily-wildcat",
    "title": "Web scraping",
    "section": "Opinion articles in The Arizona Daily Wildcat",
    "text": "Opinion articles in The Arizona Daily Wildcat\nGo to https://wildcat.arizona.edu/category/opinions/.\n\nHow many articles are on the page?"
  },
  {
    "objectID": "slides/08-web-scraping.html#goal",
    "href": "slides/08-web-scraping.html#goal",
    "title": "Web scraping",
    "section": "Goal",
    "text": "Goal\n\n\n\nScrape data and organize it in a tabular format in Python\nPerform light text parsing to clean data\nSummarize and visualze the data"
  },
  {
    "objectID": "slides/08-web-scraping.html#ae-06",
    "href": "slides/08-web-scraping.html#ae-06",
    "title": "Web scraping",
    "section": "ae-06",
    "text": "ae-06\n\n\nOpen a new window in VS Code (File &gt; New Window) and open the project called ae.\nIf there are any uncommitted files, commit them, and then click Pull.\nOpen the file called wildcat-scrape.py and follow along."
  },
  {
    "objectID": "slides/08-web-scraping.html#recap",
    "href": "slides/08-web-scraping.html#recap",
    "title": "Web scraping",
    "section": "Recap",
    "text": "Recap\n\nUse the SelectorGadget identify tags for elements you want to grab\nUse BeautifulSoup to first read the whole page (into Python) and then parse the object you’ve read in to the elements you’re interested in\nPut the components together in a data frame and analyze it like you analyze any other data"
  },
  {
    "objectID": "slides/08-web-scraping.html#a-new-python-workflow",
    "href": "slides/08-web-scraping.html#a-new-python-workflow",
    "title": "Web scraping",
    "section": "A new Python workflow",
    "text": "A new Python workflow\n\nWhen working in a Jupyter notebook, your analysis is re-run each time you execute the notebook\nIf web scraping in a notebook, you’d be re-scraping the data each time you run the notebook, which is undesirable (and not nice)!\nAn alternative workflow:\n\nUse a Python script to save your code\nSaving interim data scraped using the code in the script as CSV or pickle files\nUse the saved data in your analysis in your notebook"
  },
  {
    "objectID": "slides/08-web-scraping.html#ethics-can-you-vs-should-you",
    "href": "slides/08-web-scraping.html#ethics-can-you-vs-should-you",
    "title": "Web scraping",
    "section": "Ethics: “Can you?” vs “Should you?”",
    "text": "Ethics: “Can you?” vs “Should you?”\n\n\n\n\n\n\n\nSource: Brian Resnick, Researchers just released profile data on 70,000 OkCupid users without permission, Vox."
  },
  {
    "objectID": "slides/08-web-scraping.html#can-you-vs-should-you",
    "href": "slides/08-web-scraping.html#can-you-vs-should-you",
    "title": "Web scraping",
    "section": "“Can you?” vs “Should you?”",
    "text": "“Can you?” vs “Should you?”"
  },
  {
    "objectID": "slides/08-web-scraping.html#challenges-unreliable-formatting",
    "href": "slides/08-web-scraping.html#challenges-unreliable-formatting",
    "title": "Web scraping",
    "section": "Challenges: Unreliable formatting",
    "text": "Challenges: Unreliable formatting\n\n\n\n\n\n\n\nalumni.arizona.edu/celebrate-arizona/notable-alumni"
  },
  {
    "objectID": "slides/08-web-scraping.html#challenges-data-broken-into-many-pages",
    "href": "slides/08-web-scraping.html#challenges-data-broken-into-many-pages",
    "title": "Web scraping",
    "section": "Challenges: Data broken into many pages",
    "text": "Challenges: Data broken into many pages"
  },
  {
    "objectID": "slides/08-web-scraping.html#workflow-screen-scraping-vs.-apis",
    "href": "slides/08-web-scraping.html#workflow-screen-scraping-vs.-apis",
    "title": "Web scraping",
    "section": "Workflow: Screen scraping vs. APIs",
    "text": "Workflow: Screen scraping vs. APIs\nTwo different scenarios for web scraping:\n\nScreen scraping: extract data from source code of website, with html parser (easy) or regular expression matching (less easy)\nWeb APIs (application programming interface): website offers a set of structured http requests that return JSON or XML files\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/14-sampling-inference.html#setup",
    "href": "slides/14-sampling-inference.html#setup",
    "title": "Sampling distributions + inference",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats \nimport seaborn as sns\n\nnp.random.seed(1)"
  },
  {
    "objectID": "slides/14-sampling-inference.html#variability-of-sample-statistics",
    "href": "slides/14-sampling-inference.html#variability-of-sample-statistics",
    "title": "Sampling distributions + inference",
    "section": "Variability of sample statistics",
    "text": "Variability of sample statistics\n\n\nWe’ve seen that each sample from the population yields a slightly different sample statistic (sample mean, sample proportion, etc.)\nPreviously we’ve quantified this value via simulation\nToday we talk about some of the theory underlying sampling distributions, particularly as they relate to sample means."
  },
  {
    "objectID": "slides/14-sampling-inference.html#statistical-inference",
    "href": "slides/14-sampling-inference.html#statistical-inference",
    "title": "Sampling distributions + inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical inference is the act of generalizing from a sample in order to make conclusions regarding a population.\nWe are interested in population parameters, which we do not observe. Instead, we must calculate statistics from our sample in order to learn about them.\nAs part of this process, we must quantify the degree of uncertainty in our sample statistic."
  },
  {
    "objectID": "slides/14-sampling-inference.html#sampling-distribution-of-the-mean",
    "href": "slides/14-sampling-inference.html#sampling-distribution-of-the-mean",
    "title": "Sampling distributions + inference",
    "section": "Sampling distribution of the mean",
    "text": "Sampling distribution of the mean\nSuppose we’re interested in the mean resting heart rate of students at U of A, and are able to do the following:\n\n\nTake a random sample of size \\(n\\) from this population, and calculate the mean resting heart rate in this sample, \\(\\bar{X}_1\\)\nPut the sample back, take a second random sample of size \\(n\\), and calculate the mean resting heart rate from this new sample, \\(\\bar{X}_2\\)\nPut the sample back, take a third random sample of size \\(n\\), and calculate the mean resting heart rate from this sample, too…\n\n\n\n…and so on."
  },
  {
    "objectID": "slides/14-sampling-inference.html#sampling-distribution-of-the-mean-1",
    "href": "slides/14-sampling-inference.html#sampling-distribution-of-the-mean-1",
    "title": "Sampling distributions + inference",
    "section": "Sampling distribution of the mean",
    "text": "Sampling distribution of the mean\nAfter repeating this many times, we have a data set that has the sample means from the population: \\(\\bar{X}_1\\), \\(\\bar{X}_2\\), \\(\\cdots\\), \\(\\bar{X}_K\\) (assuming we took \\(K\\) total samples).\n\n\nCan we say anything about the distribution of these sample means (that is, the sampling distribution of the mean?)\n\n\n\n(Keep in mind, we don’t know what the underlying distribution of mean resting heart rate of U of A students looks like!)"
  },
  {
    "objectID": "slides/14-sampling-inference.html#the-central-limit-theorem",
    "href": "slides/14-sampling-inference.html#the-central-limit-theorem",
    "title": "Sampling distributions + inference",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nA quick caveat…\n\nFor now, let’s assume we know the underlying standard deviation, \\(\\sigma\\), from our distribution"
  },
  {
    "objectID": "slides/14-sampling-inference.html#the-central-limit-theorem-1",
    "href": "slides/14-sampling-inference.html#the-central-limit-theorem-1",
    "title": "Sampling distributions + inference",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nFor a population with a well-defined mean \\(\\mu\\) and standard deviation \\(\\sigma\\), these three properties hold for the distribution of sample mean \\(\\bar{X}\\), assuming certain conditions hold:\n\n\nThe mean of the sampling distribution of the mean is identical to the population mean \\(\\mu\\).\nThe standard deviation of the distribution of the sample means is \\(\\sigma/\\sqrt{n}\\).\n\n\nThis is called the standard error (SE) of the mean.\n\n\nFor \\(n\\) large enough, the shape of the sampling distribution of means is approximately normally distributed."
  },
  {
    "objectID": "slides/14-sampling-inference.html#the-normal-gaussian-distribution",
    "href": "slides/14-sampling-inference.html#the-normal-gaussian-distribution",
    "title": "Sampling distributions + inference",
    "section": "The normal (Gaussian) distribution",
    "text": "The normal (Gaussian) distribution\nThe normal distribution is unimodal and symmetric and is described by its density function:\n\nIf a random variable \\(X\\) follows the normal distribution, then \\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2} \\right\\}\\] where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance \\((\\sigma \\text{ is the standard deviation})\\)\n\n\n\n\n\n\n\n\nWarning\n\n\nWe often write \\(N(\\mu, \\sigma)\\) to describe this distribution."
  },
  {
    "objectID": "slides/14-sampling-inference.html#the-normal-distribution-graphically",
    "href": "slides/14-sampling-inference.html#the-normal-distribution-graphically",
    "title": "Sampling distributions + inference",
    "section": "The normal distribution (graphically)",
    "text": "The normal distribution (graphically)"
  },
  {
    "objectID": "slides/14-sampling-inference.html#wait-any-distribution",
    "href": "slides/14-sampling-inference.html#wait-any-distribution",
    "title": "Sampling distributions + inference",
    "section": "Wait, any distribution?",
    "text": "Wait, any distribution?\nThe central limit theorem tells us that sample means are normally distributed, if we have enough data and certain assumptions hold.\n\nThis is true even if our original variables are not normally distributed.\n\n\nClick here to see an interactive demonstration of this idea."
  },
  {
    "objectID": "slides/14-sampling-inference.html#conditions-for-clt",
    "href": "slides/14-sampling-inference.html#conditions-for-clt",
    "title": "Sampling distributions + inference",
    "section": "Conditions for CLT",
    "text": "Conditions for CLT\nWe need to check two conditions for CLT to hold: independence, sample size/distribution.\n\n✅ Independence: The sampled observations must be independent. This is difficult to check, but the following are useful guidelines:\n\n\nthe sample must be randomly taken\nif sampling without replacement, sample size must be less than 10% of the population size\n\n\n\n\nIf samples are independent, then by definition one sample’s value does not “influence” another sample’s value."
  },
  {
    "objectID": "slides/14-sampling-inference.html#conditions-for-clt-1",
    "href": "slides/14-sampling-inference.html#conditions-for-clt-1",
    "title": "Sampling distributions + inference",
    "section": "Conditions for CLT",
    "text": "Conditions for CLT\n✅ Sample size / distribution:\n\n\nif data are numerical, usually n &gt; 30 is considered a large enough sample for the CLT to apply\nif we know for sure that the underlying data are normally distributed, then the distribution of sample means will also be exactly normal, regardless of the sample size\nif data are categorical, at least 10 successes and 10 failures."
  },
  {
    "objectID": "slides/14-sampling-inference.html#underlying-population-not-observed-in-real-life",
    "href": "slides/14-sampling-inference.html#underlying-population-not-observed-in-real-life",
    "title": "Sampling distributions + inference",
    "section": "Underlying population (not observed in real life!)",
    "text": "Underlying population (not observed in real life!)\n\nrs_pop = pd.DataFrame({'x': np.random.beta(a=1, b=5, size=100000) * 100})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe true population parameters\n\n\n(16.6681176684249, 14.070251294281247)"
  },
  {
    "objectID": "slides/14-sampling-inference.html#sampling-from-the-population---1",
    "href": "slides/14-sampling-inference.html#sampling-from-the-population---1",
    "title": "Sampling distributions + inference",
    "section": "Sampling from the population - 1",
    "text": "Sampling from the population - 1\n\nnp.random.seed(1)\nsamp_1 = rs_pop.sample(n=50)\nsamp_1_mean = samp_1['x'].mean()\n\n\n\nsamp_1_mean\n\n15.459323508748778"
  },
  {
    "objectID": "slides/14-sampling-inference.html#sampling-from-the-population---2",
    "href": "slides/14-sampling-inference.html#sampling-from-the-population---2",
    "title": "Sampling distributions + inference",
    "section": "Sampling from the population - 2",
    "text": "Sampling from the population - 2\n\nnp.random.seed(2)\nsamp_2 = rs_pop.sample(n=50)\nsamp_2_mean = samp_2['x'].mean()\n\n\n\nsamp_2_mean\n\n16.54881766128449"
  },
  {
    "objectID": "slides/14-sampling-inference.html#sampling-from-the-population---3",
    "href": "slides/14-sampling-inference.html#sampling-from-the-population---3",
    "title": "Sampling distributions + inference",
    "section": "Sampling from the population - 3",
    "text": "Sampling from the population - 3\n\nnp.random.seed(3)\nsamp_3 = rs_pop.sample(n=50)\nsamp_3_mean = samp_3['x'].mean()\n\n\n\nsamp_3_mean\n\n18.232335628488613\n\n\n\nkeep repeating…"
  },
  {
    "objectID": "slides/14-sampling-inference.html#sampling-distribution",
    "href": "slides/14-sampling-inference.html#sampling-distribution",
    "title": "Sampling distributions + inference",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nnp.random.seed(92620)\nsampling_means = [rs_pop.sample(n=50, replace=True)['x'].mean() for _ in range(5000)]\n\nsampling_df = pd.DataFrame({'xbar': sampling_means})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sample statistics\n\n\n(16.71372258467488, 1.9996839510659443)"
  },
  {
    "objectID": "slides/14-sampling-inference.html#section",
    "href": "slides/14-sampling-inference.html#section",
    "title": "Sampling distributions + inference",
    "section": "",
    "text": "Compare the shapes, centers, and spreads of these distributions:\n\n\n\nThe true population\n\n\n\n\n\n\n\n\n\n\nThe sample statistics"
  },
  {
    "objectID": "slides/14-sampling-inference.html#recap",
    "href": "slides/14-sampling-inference.html#recap",
    "title": "Sampling distributions + inference",
    "section": "Recap",
    "text": "Recap\n\n\nIf certain assumptions are satisfied, regardless of the shape of the population distribution, the sampling distribution of the mean follows an approximately normal distribution.\nThe center of the sampling distribution is at the center of the population distribution.\nThe sampling distribution is less variable than the population distribution (and we can quantify by how much).\n\n\n\n\nWhat is the standard error, and how are the standard error and sample size related? What does that say about how the spread of the sampling distribution changes as \\(n\\) increases?"
  },
  {
    "objectID": "slides/14-sampling-inference.html#why-do-we-care",
    "href": "slides/14-sampling-inference.html#why-do-we-care",
    "title": "Sampling distributions + inference",
    "section": "Why do we care?",
    "text": "Why do we care?\nKnowing the distribution of the sample statistic \\(\\bar{X}\\) can help us\n\n\nEstimate a population parameter as point estimate \\(\\boldsymbol{\\pm}\\) margin of error\nThe margin of error is comprised of a measure of how confident we want to be and how variable the sample statistic is\n\n\n\nTest for a population parameter by evaluating how likely it is to obtain to observed sample statistic when assuming that the null hypothesis is true\n\nThis probability will depend on how variable the sampling distribution is"
  },
  {
    "objectID": "slides/14-sampling-inference.html#inference-based-on-the-clt-1",
    "href": "slides/14-sampling-inference.html#inference-based-on-the-clt-1",
    "title": "Sampling distributions + inference",
    "section": "Inference based on the CLT",
    "text": "Inference based on the CLT\nIf necessary conditions are met, we can also use inference methods based on the CLT. Suppose we know the true population standard deviation, \\(\\sigma\\).\n\nThen the CLT tells us that \\(\\bar{X}\\) approximately has the distribution \\(N\\left(\\mu, \\sigma/\\sqrt{n}\\right)\\).\nThat is,\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/14-sampling-inference.html#t-distribution",
    "href": "slides/14-sampling-inference.html#t-distribution",
    "title": "Sampling distributions + inference",
    "section": "T distribution",
    "text": "T distribution\nIn practice, we never know the true value of \\(\\sigma\\), and so we estimate it from our data with \\(s\\).\nWe can make the following test statistic for testing a single sample’s population mean, which has a t-distribution with n-1 degrees of freedom:\n\n\n\\[ T = \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\\]"
  },
  {
    "objectID": "slides/14-sampling-inference.html#t-distribution-1",
    "href": "slides/14-sampling-inference.html#t-distribution-1",
    "title": "Sampling distributions + inference",
    "section": "T distribution",
    "text": "T distribution\n\n\nThe t-distribution is also unimodal and symmetric, and is centered at 0\nIt has thicker tails than the normal distribution\n\nThis is to make up for additional variability introduced by using \\(s\\) instead of \\(\\sigma\\) in calculation of the SE\n\nIt is defined by the degrees of freedom"
  },
  {
    "objectID": "slides/14-sampling-inference.html#t-vs-z-distributions",
    "href": "slides/14-sampling-inference.html#t-vs-z-distributions",
    "title": "Sampling distributions + inference",
    "section": "T vs Z distributions",
    "text": "T vs Z distributions"
  },
  {
    "objectID": "slides/14-sampling-inference.html#t-distribution-2",
    "href": "slides/14-sampling-inference.html#t-distribution-2",
    "title": "Sampling distributions + inference",
    "section": "T distribution",
    "text": "T distribution\n\n\nFinding probabilities under the t curve:\n\n# P(t &lt; -1.96) for df = 9\np_less = stats.t.cdf(-1.96, df=9)\np_less\n\n0.04082220273020832\n\n\n\n\n# P(t &gt; -1.96) for df = 9\np_greater = stats.t.sf(-1.96, df=9)\np_greater\n\n0.9591777972697917\n\n\n\nFinding cutoff values under the t curve:\n\n# Find Q1 (25th percentile) for df = 9\nq1 = stats.t.ppf(0.25, df=9)\nq1\n\n-0.7027221467513188\n\n\n\n\n# Find Q3 (75th percentile) for df = 9\nq3 = stats.t.ppf(0.75, df=9)\nq3\n\n0.7027221467513188"
  },
  {
    "objectID": "slides/14-sampling-inference.html#confidence-interval-for-a-mean",
    "href": "slides/14-sampling-inference.html#confidence-interval-for-a-mean",
    "title": "Sampling distributions + inference",
    "section": "Confidence interval for a mean",
    "text": "Confidence interval for a mean\n\n\n\n\n\n\nWarning\n\n\nGeneral form of the confidence interval\n\\[point~estimate \\pm critical~value \\times SE\\]\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nConfidence interval for the mean\n\\[\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/14-sampling-inference.html#durham-nc-resident-satisfaction",
    "href": "slides/14-sampling-inference.html#durham-nc-resident-satisfaction",
    "title": "Sampling distributions + inference",
    "section": "Durham NC Resident Satisfaction",
    "text": "Durham NC Resident Satisfaction\ndurham_survey contains resident responses to a survey given by the City of Durham in 2018. These are a randomly selected, representative sample of Durham residents.\nQuestions were rated 1 - 5, with 1 being “highly dissatisfied” and 5 being “highly satisfied.”"
  },
  {
    "objectID": "slides/14-sampling-inference.html#exploratory-data-analysis",
    "href": "slides/14-sampling-inference.html#exploratory-data-analysis",
    "title": "Sampling distributions + inference",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n\n\ndurham = pd.read_csv(\"data/durham_survey.csv\")\ndurham_filtered = durham[durham['quality_library'] != 9]\n\n\nsummary_stats = durham_filtered['quality_library'].agg(['mean', 'median', 'std', 'count']).rename({\n    'mean': 'x_bar',\n    'median': 'med',\n    'std': 'sd',\n    'count': 'n'\n})\nprint(summary_stats)\n\nx_bar      3.96929\nmed        4.00000\nsd         0.90033\nn        521.00000\nName: quality_library, dtype: float64"
  },
  {
    "objectID": "slides/14-sampling-inference.html#calculate-95-confidence-interval",
    "href": "slides/14-sampling-inference.html#calculate-95-confidence-interval",
    "title": "Sampling distributions + inference",
    "section": "Calculate 95% confidence interval",
    "text": "Calculate 95% confidence interval\n\\[\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}\\]\n\n\npoint_est = summary_stats['x_bar'] # Point estimate\nse = summary_stats['sd'] / (summary_stats['n'] ** 0.5) # SE\ndf = summary_stats['n'] - 1 # Degrees of freedom\n\n\n\n\nt_star = stats.t.ppf(0.975, df) # Critical value \n\n\n\n\n# Confidence interval\nCI = point_est + np.array([-1, 1]) * t_star * se\nCI_rounded = np.round(CI, 2)\nCI_rounded\n\narray([3.89, 4.05])"
  },
  {
    "objectID": "slides/14-sampling-inference.html#interpret-95-confidence-interval",
    "href": "slides/14-sampling-inference.html#interpret-95-confidence-interval",
    "title": "Sampling distributions + inference",
    "section": "Interpret 95% confidence interval",
    "text": "Interpret 95% confidence interval\nThe 95% confidence interval is 3.89 to 4.05.\n\n\nInterpret this interval in context of the data.\n\n\n\nWe are 95% confident that the true mean rating for Durham residents’ satisfaction with the library system is between 3.89 and 4.05.\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#what-is-numpy",
    "href": "slides/03-numpy-pandas.html#what-is-numpy",
    "title": "Intro to Numpy + Pandas",
    "section": "What is NumPy?",
    "text": "What is NumPy?\n\n\nNumPy = Numerical Python\nFoundational package for scientific computing\nHigh-performance multidimensional arrays\nTools for working with arrays\n\n\n\nStart with\npip install numpy"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#why-numpy-for-data-science",
    "href": "slides/03-numpy-pandas.html#why-numpy-for-data-science",
    "title": "Intro to Numpy + Pandas",
    "section": "Why NumPy for data science?",
    "text": "Why NumPy for data science?\n\n\nEssential for data processing, manipulation, and analysis.\nUnderpins advanced data science algorithms implemented in Python.\nFast and memory-efficient with powerful data structures."
  },
  {
    "objectID": "slides/03-numpy-pandas.html#creating-arrays",
    "href": "slides/03-numpy-pandas.html#creating-arrays",
    "title": "Intro to Numpy + Pandas",
    "section": "Creating arrays",
    "text": "Creating arrays\n\nCodeOutput\n\n\n\nimport numpy as np\n\n# Creating a simple NumPy array\narr = np.array([1, 2, 3, 4])\n\n# Multidimensional array\nmulti_arr = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Range of values\nrange_arr = np.arange(10)\n\n# Array of zeros\nzeros_arr = np.zeros((3, 3))\n\n# Array of ones\nones_arr = np.ones((2, 2))\n\n# Identity matrix\nidentity_matrix = np.eye(3)\n\n\n\n\n\narr: [1 2 3 4] \n\nmulti_arr: [[1 2 3]\n [4 5 6]] \n\nrange_arr: [0 1 2 3 4 5 6 7 8 9] \n\nzeros_arr: [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]] \n\nones_arr: [[1. 1.]\n [1. 1.]] \n\nidentity_matrix: [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#array-attributes",
    "href": "slides/03-numpy-pandas.html#array-attributes",
    "title": "Intro to Numpy + Pandas",
    "section": "Array attributes",
    "text": "Array attributes\n\n# Array dimensions\nprint(\"Dimensions:\", multi_arr.ndim)\n\n# Shape of array\nprint(\"Shape:\", multi_arr.shape)\n\n# Size of array\nprint(\"Size:\", multi_arr.size)\n\n# Data type of array elements\nprint(\"Data Type:\", multi_arr.dtype)\n\nDimensions: 2\nShape: (2, 3)\nSize: 6\nData Type: int64"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#arithmetic-operations",
    "href": "slides/03-numpy-pandas.html#arithmetic-operations",
    "title": "Intro to Numpy + Pandas",
    "section": "Arithmetic operations",
    "text": "Arithmetic operations\n\n# Element-wise addition\naddition = arr + 2\n\n# Element-wise subtraction\nsubtraction = arr - 2\n\n# Element-wise multiplication\nmultiplication = arr * 2\n\n# Element-wise division\ndivision = arr / 2\n\n\n\naddition: [3 4 5 6] \n\nsubtraction: [-1  0  1  2] \n\nmultiplication: [2 4 6 8] \n\ndivision: [0.5 1.  1.5 2. ]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#aside",
    "href": "slides/03-numpy-pandas.html#aside",
    "title": "Intro to Numpy + Pandas",
    "section": "Aside",
    "text": "Aside\nWhy do my outputs look than different than Python?\n\nPythonMy slides\n\n\n\nprint(addition)\nprint(subtraction)\nprint(multiplication)\nprint(division)\n\n[3 4 5 6]\n[-1  0  1  2]\n[2 4 6 8]\n[0.5 1.  1.5 2. ]\n\n\n\n\n\nprint(\"addition:\", addition, \"\\n\")\n\nprint(\"subtraction:\", subtraction, \"\\n\")\n\nprint(\"multiplication:\", multiplication, \"\\n\")\n\nprint(\"division:\", division, \"\\n\")\n\naddition: [3 4 5 6] \n\nsubtraction: [-1  0  1  2] \n\nmultiplication: [2 4 6 8] \n\ndivision: [0.5 1.  1.5 2. ]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#statistical-operations",
    "href": "slides/03-numpy-pandas.html#statistical-operations",
    "title": "Intro to Numpy + Pandas",
    "section": "Statistical operations",
    "text": "Statistical operations\n\n# Sum of elements\ntotal = arr.sum()\n\n# Mean of elements\nmean_value = arr.mean()\n\n# Standard deviation\nstd_dev = arr.std()\n\n# Correlation coefficient\ncorr = np.corrcoef(multi_arr)\n\n\n\ntotal: 10 \n\nmean_value: 2.5 \n\nstd_dev: 1.118033988749895 \n\ncorr: [[1. 1.]\n [1. 1.]]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#reshaping-and-transposing",
    "href": "slides/03-numpy-pandas.html#reshaping-and-transposing",
    "title": "Intro to Numpy + Pandas",
    "section": "Reshaping and transposing",
    "text": "Reshaping and transposing\n\n# Reshaping an array\nreshaped = np.reshape(range_arr, (2, 5))\n\n# Transpose of an array\ntransposed = multi_arr.T\n\n\n\nrange_arr: [0 1 2 3 4 5 6 7 8 9] \n\nreshaped: [[0 1 2 3 4]\n [5 6 7 8 9]] \n\nmulti_arr: [[1 2 3]\n [4 5 6]] \n\ntransposed: [[1 4]\n [2 5]\n [3 6]]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#indexing-and-slicing",
    "href": "slides/03-numpy-pandas.html#indexing-and-slicing",
    "title": "Intro to Numpy + Pandas",
    "section": "Indexing and slicing",
    "text": "Indexing and slicing\n\n# Accessing a specific element\nelement = multi_arr[0, 1]\n\n# Slicing a row\nrow = multi_arr[1, :]\n\n# Slicing a column\ncolumn = multi_arr[:, 2]\n\n\n\nmulti_arr: [[1 2 3]\n [4 5 6]] \n\nelement: [[1 2 3]\n [4 5 6]] \n\nrow: [4 5 6] \n\ncolumn: [3 6]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#broadcasting",
    "href": "slides/03-numpy-pandas.html#broadcasting",
    "title": "Intro to Numpy + Pandas",
    "section": "Broadcasting",
    "text": "Broadcasting\n\n# Broadcasting allows arithmetic operations on arrays of different sizes\nbroadcasted_addition = multi_arr + np.array([1, 0, 1])\n\n\n\nmulti_arr: [[1 2 3]\n [4 5 6]] \n\nbroadcasted_addition: [[2 2 4]\n [5 5 7]]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#matrix-operations",
    "href": "slides/03-numpy-pandas.html#matrix-operations",
    "title": "Intro to Numpy + Pandas",
    "section": "Matrix operations",
    "text": "Matrix operations\nDot product: take two equal-length sequences and return a single number\n2 • (1, 2, 3) = 2x1 = 2; 2x2 = 4; 2x3 = 6\nMatrix multiplication:\n(1, 2, 3) • (7, 9, 11) = (1×7 + 2×9 + 3×11) = 58\n\n# Dot product\ndot_product = np.dot(arr, arr)\n\n# Matrix multiplication\nmatrix_mul = np.dot(multi_arr, identity_matrix)\n\n\n\ndot_product: 30 \n\nmatrix_mul: [[1. 2. 3.]\n [4. 5. 6.]]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#eigenvalues-and-eigenvectors",
    "href": "slides/03-numpy-pandas.html#eigenvalues-and-eigenvectors",
    "title": "Intro to Numpy + Pandas",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\n# Eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(identity_matrix)\n\n\n\neigenvalues: [1. 1. 1.] \n\neigenvectors: [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#numpy-for-data-science",
    "href": "slides/03-numpy-pandas.html#numpy-for-data-science",
    "title": "Intro to Numpy + Pandas",
    "section": "NumPy for data science",
    "text": "NumPy for data science\n\nApplication in Algorithms\n\nNumPy arrays are used in various data science algorithms like clustering, classification, and neural networks.\n\n\n\nPerformance\n\nNumPy operations are implemented in C, which makes them much faster than standard Python."
  },
  {
    "objectID": "slides/03-numpy-pandas.html#conclusion",
    "href": "slides/03-numpy-pandas.html#conclusion",
    "title": "Intro to Numpy + Pandas",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nNumPy is integral to data science and analysis in Python.\nIt provides efficient and fast operations for array and matrix manipulation.\nUnderstanding NumPy is crucial for implementing and customizing data science algorithms."
  },
  {
    "objectID": "slides/03-numpy-pandas.html#what-is-pandas",
    "href": "slides/03-numpy-pandas.html#what-is-pandas",
    "title": "Intro to Numpy + Pandas",
    "section": "What is Pandas?",
    "text": "What is Pandas?\n\n\nHigh-Performance Library: Pandas is a Python library for fast data manipulation.\nCore Structures: It introduces DataFrame and Series for data handling.\nData Processing: Ideal for cleaning and analyzing datasets.\nVersatile I/O: Offers extensive file format compatibility for data I/O."
  },
  {
    "objectID": "slides/03-numpy-pandas.html#why-pandas-for-data-mining",
    "href": "slides/03-numpy-pandas.html#why-pandas-for-data-mining",
    "title": "Intro to Numpy + Pandas",
    "section": "Why Pandas for data mining?",
    "text": "Why Pandas for data mining?\n\n\nStreamlines Data Prep: Optimizes data manipulation for mining readiness.\nBuilt-in Analysis: Includes essential tools for quick data exploration.\nHandles Large Data: Efficiently processes and analyzes big datasets."
  },
  {
    "objectID": "slides/03-numpy-pandas.html#series",
    "href": "slides/03-numpy-pandas.html#series",
    "title": "Intro to Numpy + Pandas",
    "section": "Series",
    "text": "Series\nOne-dimensional array-like object containing a sequence of values with an associated array of labels, its index.\n\nimport pandas as pd\n\n# Creating a Series\nser = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\n\n# Accessing elements\nprint(\"a:\", ser['a'])\n\na: -5"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#dataframe",
    "href": "slides/03-numpy-pandas.html#dataframe",
    "title": "Intro to Numpy + Pandas",
    "section": "DataFrame",
    "text": "DataFrame\nA rectangular table of data with an ordered collection of columns\n\n# Creating a DataFrame\ndata = {\n    'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n    'year': [2000, 2001, 2002, 2001, 2002],\n    'pop': [1.5, 1.7, 3.6, 2.4, 2.9]\n}\nframe = pd.DataFrame(data)\n\n# Selecting columns\nprint(frame['state'])\n\n0      Ohio\n1      Ohio\n2      Ohio\n3    Nevada\n4    Nevada\nName: state, dtype: object"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#index-objects",
    "href": "slides/03-numpy-pandas.html#index-objects",
    "title": "Intro to Numpy + Pandas",
    "section": "Index objects",
    "text": "Index objects\nImmutable, can’t be modified by a user\n\n# Index objects\nobj = pd.Series(range(3), index=['a', 'b', 'c'])\nindex = obj.index\n\nprint(index)\n\nIndex(['a', 'b', 'c'], dtype='object')"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#case-study-average-rent-costs",
    "href": "slides/03-numpy-pandas.html#case-study-average-rent-costs",
    "title": "Intro to Numpy + Pandas",
    "section": "Case study: average rent costs",
    "text": "Case study: average rent costs\n\n\n\n\n\n💰📈\n\n\nSource: Zillow Public Access Data"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#first-why-.csvs",
    "href": "slides/03-numpy-pandas.html#first-why-.csvs",
    "title": "Intro to Numpy + Pandas",
    "section": "First, why .CSVs?",
    "text": "First, why .CSVs?\n\n\nMore reproducible - can see changes on GitHub\nSimple file structure\nStandardized\nNon-proprietary (e.g., Excel)"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#reading-in-.csv-files",
    "href": "slides/03-numpy-pandas.html#reading-in-.csv-files",
    "title": "Intro to Numpy + Pandas",
    "section": "Reading in .CSV files",
    "text": "Reading in .CSV files\n\n# Loading data from CSV\ndf = pd.read_csv('data/rent_avg.csv')\n\ndf\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\n1/31/15\n2/28/15\n3/31/15\n4/30/15\n5/31/15\n...\n2/28/23\n3/31/23\n4/30/23\n5/31/23\n6/30/23\n7/31/23\n8/31/23\n9/30/23\n10/31/23\n11/30/23\n\n\n\n\n0\n102001\n0\nUnited States\ncountry\nNaN\n1266.059583\n1272.748070\n1281.390109\n1291.808026\n1301.544232\n...\n2072.346516\n2084.944938\n2100.570959\n2113.158286\n2123.032953\n2132.040398\n2141.677753\n2147.835795\n2148.939168\n2147.563754\n\n\n1\n394913\n1\nNew York, NY\nmsa\nNY\n2233.133615\n2255.035180\n2272.077073\n2291.645864\n2297.479956\n...\n3336.961840\n3402.179034\n3470.949450\n3492.930681\n3502.786897\n3503.650740\n3509.480232\n3493.537322\n3460.942251\n3445.696877\n\n\n2\n753899\n2\nLos Angeles, CA\nmsa\nCA\n2571.296547\n2586.050819\n2604.348963\n2616.104497\n2637.303435\n...\n4073.810818\n4100.234089\n4134.999768\n4148.832674\n4182.522537\n4202.376930\n4230.003153\n4224.038689\n4213.465460\n4209.636932\n\n\n3\n394463\n3\nChicago, IL\nmsa\nIL\n1504.096116\n1510.879827\n1522.416987\n1534.343702\n1547.516576\n...\n2112.639599\n2123.617285\n2145.692631\n2163.843271\n2183.541315\n2196.506806\n2213.427631\n2225.237153\n2225.798038\n2221.960828\n\n\n4\n394514\n4\nDallas, TX\nmsa\nTX\n1363.557414\n1371.136919\n1381.114797\n1394.643185\n1408.025840\n...\n2222.442779\n2229.474165\n2239.462332\n2253.004851\n2267.408242\n2280.056798\n2285.063932\n2284.569053\n2274.785931\n2277.403767\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n467\n753871\n811\nBreckenridge, CO\nmsa\nCO\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n3848.649613\n4327.112762\n4414.482838\n4438.619592\n4535.664564\n4599.237795\n4558.170264\n4667.043338\n4671.081209\n4472.222222\n\n\n468\n394751\n821\nKirksville, MO\nmsa\nMO\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1000.733430\n981.701456\n951.458333\n\n\n469\n753923\n849\nThe Dalles, OR\nmsa\nOR\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1757.739178\n1788.943167\n1885.904065\n1838.888889\n\n\n470\n394584\n863\nFallon, NV\nmsa\nNV\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1295.000000\n\n\n471\n394996\n915\nPortales, NM\nmsa\nNM\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n1122.195307\n1101.628147\n1057.654564\n1071.572024\n1046.671512\n1099.305556\n\n\n\n\n472 rows × 112 columns"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#inspecting-data",
    "href": "slides/03-numpy-pandas.html#inspecting-data",
    "title": "Intro to Numpy + Pandas",
    "section": "Inspecting data",
    "text": "Inspecting data\nAny issues?\n\nHeadTailData typesDescribeStatsUnique values\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\n1/31/15\n2/28/15\n3/31/15\n4/30/15\n5/31/15\n...\n2/28/23\n3/31/23\n4/30/23\n5/31/23\n6/30/23\n7/31/23\n8/31/23\n9/30/23\n10/31/23\n11/30/23\n\n\n\n\n0\n102001\n0\nUnited States\ncountry\nNaN\n1266.059583\n1272.748070\n1281.390109\n1291.808026\n1301.544232\n...\n2072.346516\n2084.944938\n2100.570959\n2113.158286\n2123.032953\n2132.040398\n2141.677753\n2147.835795\n2148.939168\n2147.563754\n\n\n1\n394913\n1\nNew York, NY\nmsa\nNY\n2233.133615\n2255.035180\n2272.077073\n2291.645864\n2297.479956\n...\n3336.961840\n3402.179034\n3470.949450\n3492.930681\n3502.786897\n3503.650740\n3509.480232\n3493.537322\n3460.942251\n3445.696877\n\n\n2\n753899\n2\nLos Angeles, CA\nmsa\nCA\n2571.296547\n2586.050819\n2604.348963\n2616.104497\n2637.303435\n...\n4073.810818\n4100.234089\n4134.999768\n4148.832674\n4182.522537\n4202.376930\n4230.003153\n4224.038689\n4213.465460\n4209.636932\n\n\n3\n394463\n3\nChicago, IL\nmsa\nIL\n1504.096116\n1510.879827\n1522.416987\n1534.343702\n1547.516576\n...\n2112.639599\n2123.617285\n2145.692631\n2163.843271\n2183.541315\n2196.506806\n2213.427631\n2225.237153\n2225.798038\n2221.960828\n\n\n4\n394514\n4\nDallas, TX\nmsa\nTX\n1363.557414\n1371.136919\n1381.114797\n1394.643185\n1408.025840\n...\n2222.442779\n2229.474165\n2239.462332\n2253.004851\n2267.408242\n2280.056798\n2285.063932\n2284.569053\n2274.785931\n2277.403767\n\n\n\n\n5 rows × 112 columns\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\n1/31/15\n2/28/15\n3/31/15\n4/30/15\n5/31/15\n...\n2/28/23\n3/31/23\n4/30/23\n5/31/23\n6/30/23\n7/31/23\n8/31/23\n9/30/23\n10/31/23\n11/30/23\n\n\n\n\n467\n753871\n811\nBreckenridge, CO\nmsa\nCO\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n3848.649613\n4327.112762\n4414.482838\n4438.619592\n4535.664564\n4599.237795\n4558.170264\n4667.043338\n4671.081209\n4472.222222\n\n\n468\n394751\n821\nKirksville, MO\nmsa\nMO\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1000.733430\n981.701456\n951.458333\n\n\n469\n753923\n849\nThe Dalles, OR\nmsa\nOR\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1757.739178\n1788.943167\n1885.904065\n1838.888889\n\n\n470\n394584\n863\nFallon, NV\nmsa\nNV\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1295.000000\n\n\n471\n394996\n915\nPortales, NM\nmsa\nNM\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n1122.195307\n1101.628147\n1057.654564\n1071.572024\n1046.671512\n1099.305556\n\n\n\n\n5 rows × 112 columns\n\n\n\n\n\n\ndf.dtypes\n\nRegionID        int64\nSizeRank        int64\nRegionName     object\nRegionType     object\nStateName      object\n               ...   \n7/31/23       float64\n8/31/23       float64\n9/30/23       float64\n10/31/23      float64\n11/30/23      float64\nLength: 112, dtype: object\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\n1/31/15\n2/28/15\n3/31/15\n4/30/15\n5/31/15\n6/30/15\n7/31/15\n8/31/15\n...\n2/28/23\n3/31/23\n4/30/23\n5/31/23\n6/30/23\n7/31/23\n8/31/23\n9/30/23\n10/31/23\n11/30/23\n\n\n\n\ncount\n472.000000\n472.000000\n170.000000\n173.000000\n177.000000\n179.000000\n179.000000\n178.000000\n179.000000\n180.000000\n...\n346.000000\n361.000000\n372.000000\n384.000000\n406.000000\n411.000000\n415.000000\n425.000000\n445.000000\n472.000000\n\n\nmean\n415298.207627\n273.686441\n1239.825620\n1240.669841\n1251.146830\n1266.885696\n1276.013223\n1284.256611\n1290.350280\n1291.137595\n...\n1903.384062\n1898.742328\n1908.427805\n1906.267933\n1886.896644\n1892.888767\n1897.265795\n1891.438391\n1877.721153\n1851.628367\n\n\nstd\n89315.652491\n192.924182\n413.889910\n413.782914\n417.808376\n430.279161\n436.704575\n443.645583\n446.426265\n448.112021\n...\n1004.878275\n984.054607\n993.950964\n981.101384\n943.047137\n925.105623\n936.407031\n974.805502\n973.269750\n935.988332\n\n\nmin\n102001.000000\n0.000000\n618.854999\n621.850858\n634.040448\n633.276802\n623.165150\n624.515366\n625.812267\n645.063429\n...\n675.528618\n753.200396\n723.441350\n745.512901\n748.650235\n727.708028\n724.647895\n726.496330\n743.350970\n752.666667\n\n\n25%\n394560.500000\n118.750000\n982.245085\n986.598979\n993.390743\n998.496778\n999.149213\n999.166349\n1002.455958\n1006.227359\n...\n1411.767065\n1409.322755\n1426.260589\n1418.156725\n1410.428037\n1418.889164\n1417.539001\n1414.537646\n1406.329003\n1397.858135\n\n\n50%\n394805.500000\n241.500000\n1119.640946\n1128.291306\n1140.587934\n1149.357569\n1154.063529\n1160.902744\n1167.827881\n1173.401059\n...\n1725.072590\n1707.075023\n1707.509644\n1701.364516\n1700.414955\n1706.087275\n1716.116918\n1708.640942\n1696.614785\n1675.149130\n\n\n75%\n395063.500000\n393.250000\n1338.069919\n1342.565444\n1365.299281\n1379.799386\n1396.227619\n1395.909341\n1407.885537\n1406.385945\n...\n2153.244218\n2139.535628\n2167.521817\n2157.862768\n2130.928236\n2153.958642\n2174.036097\n2151.869189\n2148.939168\n2118.656793\n\n\nmax\n845167.000000\n915.000000\n3079.176287\n3096.936684\n3120.952116\n3176.462957\n3249.296472\n3318.678095\n3347.010915\n3354.373564\n...\n15718.660650\n15404.494040\n15781.904020\n15583.894210\n14849.608200\n14344.093470\n14754.112810\n16015.296260\n16415.672810\n15918.888890\n\n\n\n\n8 rows × 109 columns\n\n\n\n\n\nWe can also extract specific summary stats\n\nmin_value = df['11/30/23'].min()\nmax_value = df['11/30/23'].max()\nmean_value = df['11/30/23'].mean()\nmed_value = df['11/30/23'].median()\nstd_value = df['11/30/23'].std()\ncount_value = df['11/30/23'].count()\n\n\n\nmin: 752.6666667\nmax: 15918.88889\nmean: 1851.6283666211866\nmedian: 1675.1491305\nst. dev: 935.9883320322535\nN: 472\n\n\n\n\n\npd.unique(df['StateName'])\n\narray([nan, 'NY', 'CA', 'IL', 'TX', 'VA', 'PA', 'FL', 'GA', 'MA', 'AZ',\n       'MI', 'WA', 'MN', 'CO', 'MD', 'MO', 'NC', 'OR', 'OH', 'NV', 'IN',\n       'TN', 'RI', 'WI', 'OK', 'KY', 'LA', 'UT', 'CT', 'AL', 'HI', 'NE',\n       'SC', 'NM', 'ID', 'AR', 'IA', 'KS', 'MS', 'ME', 'NH', 'DE', 'AK',\n       'NJ', 'SD', 'WV', 'ND', 'VT', 'MT', 'WY'], dtype=object)"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#melting",
    "href": "slides/03-numpy-pandas.html#melting",
    "title": "Intro to Numpy + Pandas",
    "section": "Melting",
    "text": "Melting\nJumping ahead slightly…\n\ndf2 = df.melt(id_vars = df.columns[0:5], var_name = \"date\", value_name = \"avg_price\")\ndf2.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\ndate\navg_price\n\n\n\n\n0\n102001\n0\nUnited States\ncountry\nNaN\n1/31/15\n1266.059583\n\n\n1\n394913\n1\nNew York, NY\nmsa\nNY\n1/31/15\n2233.133615\n\n\n2\n753899\n2\nLos Angeles, CA\nmsa\nCA\n1/31/15\n2571.296547\n\n\n3\n394463\n3\nChicago, IL\nmsa\nIL\n1/31/15\n1504.096116\n\n\n4\n394514\n4\nDallas, TX\nmsa\nTX\n1/31/15\n1363.557414"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#convert-to-datetime",
    "href": "slides/03-numpy-pandas.html#convert-to-datetime",
    "title": "Intro to Numpy + Pandas",
    "section": "Convert to datetime",
    "text": "Convert to datetime\n\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2.head()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\nRegionName\nRegionType\nStateName\ndate\navg_price\n\n\n\n\n0\n102001\n0\nUnited States\ncountry\nNaN\n2015-01-31\n1266.059583\n\n\n1\n394913\n1\nNew York, NY\nmsa\nNY\n2015-01-31\n2233.133615\n\n\n2\n753899\n2\nLos Angeles, CA\nmsa\nCA\n2015-01-31\n2571.296547\n\n\n3\n394463\n3\nChicago, IL\nmsa\nIL\n2015-01-31\n1504.096116\n\n\n4\n394514\n4\nDallas, TX\nmsa\nTX\n2015-01-31\n1363.557414"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#group-data",
    "href": "slides/03-numpy-pandas.html#group-data",
    "title": "Intro to Numpy + Pandas",
    "section": "Group data",
    "text": "Group data\n\ngrouped_df = df2.groupby('StateName')\nprint(type(grouped_df))\n\n&lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt;"
  },
  {
    "objectID": "slides/03-numpy-pandas.html#grouped-statistics",
    "href": "slides/03-numpy-pandas.html#grouped-statistics",
    "title": "Intro to Numpy + Pandas",
    "section": "Grouped statistics",
    "text": "Grouped statistics\n\nDescribeSingle stat\n\n\n\n# Summary statistics for all numeric columns by sex\ngrouped_df.describe()\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\n...\ndate\navg_price\n\n\n\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\ncount\nmean\n...\nmax\nstd\ncount\nmean\nmin\n25%\n50%\n75%\nmax\nstd\n\n\nStateName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAK\n214.0\n394453.500000\n394327.0\n394327.00\n394453.5\n394580.00\n394580.0\n126.796601\n214.0\n273.000000\n...\n2023-11-30 00:00:00\nNaN\n118.0\n1845.229913\n1634.802345\n1709.827198\n1751.882085\n1959.080017\n2281.395586\n180.825268\n\n\nAL\n1605.0\n448643.533333\n394333.0\n394519.00\n394598.0\n395145.00\n845163.0\n138679.865228\n1605.0\n296.066667\n...\n2023-11-30 00:00:00\nNaN\n837.0\n1196.697249\n830.963294\n971.353373\n1190.838894\n1347.038333\n2032.164480\n254.603330\n\n\nAR\n749.0\n394790.428571\n394590.0\n394609.00\n394728.0\n395042.00\n395077.0\n182.464426\n749.0\n285.142857\n...\n2023-11-30 00:00:00\nNaN\n354.0\n1095.547769\n726.685275\n940.461179\n1068.180400\n1253.028677\n1697.220290\n228.390724\n\n\nAZ\n856.0\n451262.875000\n394595.0\n394923.25\n395096.5\n395187.00\n845160.0\n148966.294950\n856.0\n214.375000\n...\n2023-11-30 00:00:00\nNaN\n502.0\n1557.286231\n975.778114\n1239.202922\n1468.813304\n1825.604726\n2729.174782\n413.841467\n\n\nCA\n3424.0\n428576.656250\n394357.0\n394841.75\n395047.5\n395142.00\n753920.0\n104654.375661\n3424.0\n191.437500\n...\n2023-11-30 00:00:00\nNaN\n2262.0\n2472.280505\n1056.267157\n1832.396777\n2412.471519\n3015.948801\n5280.032047\n801.072889\n\n\nCO\n1284.0\n454493.666667\n394405.0\n394518.50\n394620.5\n394908.75\n753881.0\n133940.119391\n1284.0\n348.833333\n...\n2023-11-30 00:00:00\nNaN\n716.0\n2346.684403\n1058.116385\n1596.669792\n1904.408666\n2271.895581\n16415.672810\n2257.370863\n\n\nCT\n535.0\n394815.600000\n394415.0\n394669.00\n394908.0\n394924.00\n395162.0\n254.108902\n535.0\n119.600000\n...\n2023-11-30 00:00:00\nNaN\n433.0\n2127.777496\n1231.613286\n1599.540968\n1837.233176\n2604.887072\n4416.410828\n748.713311\n\n\nDE\n214.0\n394795.000000\n394539.0\n394539.00\n394795.0\n395051.00\n395051.0\n256.600235\n214.0\n189.000000\n...\n2023-11-30 00:00:00\nNaN\n127.0\n1571.385754\n1258.289127\n1340.570246\n1469.069826\n1812.232209\n2085.191980\n258.317915\n\n\nFL\n2568.0\n424798.708333\n394440.0\n394759.00\n394950.0\n395079.25\n753906.0\n99244.412111\n2568.0\n174.916667\n...\n2023-11-30 00:00:00\nNaN\n2122.0\n1668.835798\n885.529271\n1273.651277\n1522.096523\n1936.250181\n4340.326552\n550.513435\n\n\nGA\n2033.0\n413673.210526\n394306.0\n394423.00\n394813.0\n395126.00\n753893.0\n80210.928282\n2033.0\n326.421053\n...\n2023-11-30 00:00:00\nNaN\n1129.0\n1256.099519\n624.240075\n971.108002\n1184.687231\n1465.110193\n2238.706120\n375.740443\n\n\nHI\n321.0\n514445.000000\n394680.0\n394680.00\n394731.0\n753924.00\n753924.0\n169601.609159\n321.0\n183.666667\n...\n2023-11-30 00:00:00\nNaN\n132.0\n2705.681385\n1941.739389\n2376.900621\n2535.854344\n2903.391210\n4534.504002\n591.932689\n\n\nIA\n749.0\n394623.571429\n394325.0\n394447.00\n394542.0\n394707.00\n395210.0\n263.893527\n749.0\n320.714286\n...\n2023-11-30 00:00:00\nNaN\n413.0\n1343.504147\n924.635859\n1096.610185\n1226.942363\n1525.647707\n2304.133291\n346.240757\n\n\nID\n749.0\n446097.000000\n394399.0\n394480.00\n394988.0\n395173.00\n753916.0\n125750.835880\n749.0\n363.142857\n...\n2023-11-30 00:00:00\nNaN\n289.0\n1576.334430\n995.965187\n1281.801032\n1520.317263\n1750.688649\n2458.333333\n374.149199\n\n\nIL\n1177.0\n468306.000000\n394454.0\n394516.00\n394734.0\n395114.00\n845167.0\n157410.509968\n1177.0\n239.636364\n...\n2023-11-30 00:00:00\nNaN\n629.0\n1220.872101\n645.511180\n1003.992490\n1133.110230\n1406.751015\n2225.798038\n331.832737\n\n\nIN\n1391.0\n422359.384615\n394393.0\n394565.00\n394705.0\n395021.00\n753895.0\n95740.797170\n1391.0\n299.538462\n...\n2023-11-30 00:00:00\nNaN\n730.0\n1105.728925\n643.993545\n831.024770\n1045.852512\n1317.120042\n2085.765612\n324.831936\n\n\nKS\n749.0\n394959.285714\n394701.0\n394778.00\n394981.0\n395161.00\n395224.0\n184.506905\n749.0\n390.571429\n...\n2023-11-30 00:00:00\nNaN\n423.0\n1100.784587\n680.241603\n869.782224\n1063.870894\n1245.758531\n1927.089567\n299.129530\n\n\nKY\n749.0\n394736.285714\n394406.0\n394563.00\n394792.0\n394949.00\n395023.0\n203.545232\n749.0\n267.857143\n...\n2023-11-30 00:00:00\nNaN\n272.0\n1201.673804\n915.665260\n1061.281701\n1172.542508\n1322.833020\n1628.296815\n170.093385\n\n\nLA\n1177.0\n394735.090909\n394314.0\n394608.00\n394761.0\n394910.00\n395096.0\n236.021094\n1177.0\n260.545455\n...\n2023-11-30 00:00:00\nNaN\n558.0\n1263.411616\n897.251299\n1101.569579\n1279.399917\n1393.830133\n1839.146198\n218.119976\n\n\nMA\n428.0\n394779.500000\n394361.0\n394393.25\n394759.5\n395145.75\n395238.0\n400.131638\n428.0\n88.750000\n...\n2023-11-30 00:00:00\nNaN\n274.0\n2246.404623\n1471.260619\n1762.759314\n2264.264095\n2598.178490\n3533.185405\n522.650962\n\n\nMD\n428.0\n484350.750000\n394358.0\n394474.25\n394586.5\n484463.00\n753872.0\n155790.307335\n428.0\n240.000000\n...\n2023-11-30 00:00:00\nNaN\n247.0\n1552.644194\n880.592353\n1210.839158\n1589.107929\n1754.226876\n2184.235363\n315.788335\n\n\nME\n214.0\n394678.000000\n394359.0\n394359.00\n394678.0\n394997.00\n394997.0\n319.747949\n214.0\n196.500000\n...\n2023-11-30 00:00:00\nNaN\n78.0\n2134.470922\n1547.912828\n1834.919379\n2041.777850\n2482.006422\n2775.031672\n378.392726\n\n\nMI\n1712.0\n417161.437500\n394302.0\n394580.00\n394751.0\n394901.25\n753890.0\n86968.672381\n1712.0\n247.875000\n...\n2023-11-30 00:00:00\nNaN\n704.0\n1444.416460\n618.854999\n997.830541\n1227.899830\n1599.409474\n3644.308459\n684.450686\n\n\nMN\n642.0\n394836.000000\n394543.0\n394637.00\n394844.0\n395030.00\n395118.0\n201.659805\n642.0\n231.166667\n...\n2023-11-30 00:00:00\nNaN\n310.0\n1588.607379\n1197.459955\n1375.809178\n1548.545456\n1733.734201\n2231.628451\n247.955226\n\n\nMO\n1498.0\n394825.571429\n394411.0\n394604.00\n394743.0\n395116.00\n395204.0\n271.321557\n1498.0\n396.000000\n...\n2023-11-30 00:00:00\nNaN\n509.0\n1122.500158\n762.926459\n1000.200109\n1092.990495\n1252.354572\n1547.110695\n188.219653\n\n\nMS\n642.0\n394880.833333\n394658.0\n394671.00\n394831.0\n395124.00\n395170.0\n212.375595\n642.0\n308.000000\n...\n2023-11-30 00:00:00\nNaN\n225.0\n1192.876828\n937.793923\n1068.174125\n1144.629497\n1317.754833\n2279.597698\n195.298729\n\n\nMT\n642.0\n394577.833333\n394386.0\n394407.00\n394537.0\n394733.00\n394867.0\n182.675809\n642.0\n421.666667\n...\n2023-11-30 00:00:00\nNaN\n177.0\n1793.224137\n1094.019889\n1216.021805\n1627.453364\n2329.398684\n3136.530533\n591.359944\n\n\nNC\n2354.0\n411102.045455\n394338.0\n394562.00\n394697.5\n395063.00\n753912.0\n74823.677625\n2354.0\n274.500000\n...\n2023-11-30 00:00:00\nNaN\n1286.0\n1366.269717\n846.562543\n1106.111669\n1331.843142\n1556.471201\n2288.942284\n324.147384\n\n\nND\n321.0\n394892.666667\n394585.0\n394585.00\n394866.0\n395227.00\n395227.0\n263.183083\n321.0\n455.666667\n...\n2023-11-30 00:00:00\nNaN\n22.0\n1651.132776\n1395.000000\n1519.067443\n1712.143286\n1755.955326\n1808.444444\n132.405607\n\n\nNE\n428.0\n394817.750000\n394617.0\n394751.25\n394858.0\n394924.50\n394938.0\n128.299053\n428.0\n425.000000\n...\n2023-11-30 00:00:00\nNaN\n216.0\n1320.009482\n940.764327\n1186.933246\n1262.477897\n1451.544329\n1754.718239\n189.568181\n\n\nNH\n214.0\n619992.000000\n394820.0\n394820.00\n619992.0\n845164.00\n845164.0\n225699.953831\n214.0\n171.000000\n...\n2023-11-30 00:00:00\nNaN\n22.0\n2582.894543\n2325.537810\n2480.164956\n2562.927550\n2709.069839\n2791.183412\n139.749875\n\n\nNJ\n214.0\n394756.000000\n394348.0\n394348.00\n394756.0\n395164.00\n395164.0\n408.956625\n214.0\n161.000000\n...\n2023-11-30 00:00:00\nNaN\n214.0\n1693.799261\n1108.055046\n1402.009299\n1642.693619\n1931.930719\n2543.122507\n353.021462\n\n\nNM\n963.0\n394666.777778\n394305.0\n394443.00\n394588.0\n394996.00\n395066.0\n292.082726\n963.0\n440.444444\n...\n2023-11-30 00:00:00\nNaN\n314.0\n1472.158264\n987.763290\n1157.028714\n1301.372585\n1634.597233\n2925.870370\n453.840507\n\n\nNV\n642.0\n394673.666667\n394444.0\n394584.00\n394610.0\n394775.00\n395019.0\n182.289096\n642.0\n452.833333\n...\n2023-11-30 00:00:00\nNaN\n264.0\n1784.668225\n1212.685378\n1484.060443\n1755.211957\n2081.924961\n2817.571625\n370.648528\n\n\nNY\n1070.0\n439881.200000\n394308.0\n394693.00\n394972.0\n395179.00\n845159.0\n135156.094569\n1070.0\n163.300000\n...\n2023-11-30 00:00:00\nNaN\n641.0\n1790.942113\n997.328276\n1343.320969\n1602.835366\n2157.428456\n3509.480232\n596.847035\n\n\nOH\n1284.0\n432299.833333\n394304.0\n394472.75\n394760.5\n395175.25\n845158.0\n124530.356368\n1284.0\n141.750000\n...\n2023-11-30 00:00:00\nNaN\n923.0\n1046.700170\n680.642040\n839.625107\n987.004712\n1188.448704\n1947.044776\n265.814765\n\n\nOK\n1177.0\n394820.000000\n394300.0\n394548.00\n394935.0\n395133.00\n395169.0\n309.974483\n1177.0\n465.454545\n...\n2023-11-30 00:00:00\nNaN\n410.0\n1062.128302\n758.667864\n973.232481\n1038.672842\n1176.785601\n1458.135575\n182.618956\n\n\nOR\n1498.0\n471653.571429\n394307.0\n394505.00\n394800.0\n395048.00\n753923.0\n147450.415722\n1498.0\n379.928571\n...\n2023-11-30 00:00:00\nNaN\n600.0\n1781.272373\n1052.830183\n1532.939293\n1757.701867\n1985.272415\n2611.379315\n360.962236\n\n\nPA\n2033.0\n394875.421053\n394318.0\n394666.00\n394974.0\n395106.00\n395244.0\n262.799913\n2033.0\n224.368421\n...\n2023-11-30 00:00:00\nNaN\n875.0\n1347.330330\n789.815931\n1149.120217\n1306.885412\n1518.000385\n2094.027778\n277.309547\n\n\nRI\n107.0\n395005.000000\n395005.0\n395005.00\n395005.0\n395005.00\n395005.0\n0.000000\n107.0\n39.000000\n...\n2023-11-30 00:00:00\nNaN\n107.0\n2118.407898\n1598.141501\n1805.755429\n2012.306431\n2439.407877\n2912.998572\n389.954589\n\n\nSC\n1070.0\n394772.600000\n394457.0\n394597.00\n394667.0\n395085.00\n395139.0\n248.911116\n1070.0\n225.800000\n...\n2023-11-30 00:00:00\nNaN\n743.0\n1396.031749\n853.147291\n1118.728682\n1338.169915\n1577.549159\n2620.224277\n357.345479\n\n\nSD\n321.0\n394845.000000\n394419.0\n394419.00\n395013.0\n395103.00\n395103.0\n303.933833\n321.0\n424.333333\n...\n2023-11-30 00:00:00\nNaN\n106.0\n1384.547508\n1049.505973\n1162.543829\n1311.456545\n1602.649611\n1872.468058\n230.479838\n\n\nTN\n1177.0\n394705.181818\n394460.0\n394474.00\n394726.0\n394849.00\n395168.0\n211.301360\n1177.0\n192.636364\n...\n2023-11-30 00:00:00\nNaN\n667.0\n1371.017577\n935.476460\n1104.347806\n1363.832948\n1532.375111\n2270.663942\n298.293853\n\n\nTX\n3317.0\n409264.806452\n394299.0\n394502.00\n394746.0\n395054.00\n845162.0\n79596.074231\n3317.0\n259.967742\n...\n2023-11-30 00:00:00\nNaN\n2088.0\n1438.496827\n765.245138\n1152.171443\n1396.188630\n1668.594537\n2474.220726\n357.363631\n\n\nUT\n856.0\n394902.000000\n394446.0\n394768.50\n394968.5\n395069.50\n395187.0\n232.809618\n856.0\n320.750000\n...\n2023-11-30 00:00:00\nNaN\n458.0\n1711.807021\n1109.188006\n1408.681922\n1591.513180\n1930.749340\n4311.466765\n506.007518\n\n\nVA\n1070.0\n394914.300000\n394392.0\n394668.00\n395025.0\n395194.00\n395232.0\n298.071878\n1070.0\n188.400000\n...\n2023-11-30 00:00:00\nNaN\n851.0\n1536.584520\n782.641195\n1268.882352\n1451.721042\n1780.951306\n2880.797205\n441.327399\n\n\nVT\n107.0\n394429.000000\n394429.0\n394429.00\n394429.0\n394429.00\n394429.0\n0.000000\n107.0\n206.000000\n...\n2023-11-30 00:00:00\nNaN\n11.0\n2900.941104\n2639.431318\n2837.553193\n2958.561667\n2986.743498\n3027.025463\n125.568309\n\n\nWA\n1605.0\n394891.866667\n394378.0\n394741.00\n394925.0\n395113.00\n395240.0\n264.588738\n1605.0\n303.733333\n...\n2023-11-30 00:00:00\nNaN\n844.0\n1852.587674\n1025.907447\n1514.166912\n1812.799384\n2157.861092\n3104.024269\n428.180994\n\n\nWI\n1177.0\n394791.727273\n394334.0\n394646.00\n394816.0\n394944.00\n395215.0\n224.222302\n1177.0\n247.727273\n...\n2023-11-30 00:00:00\nNaN\n323.0\n1514.551976\n907.936037\n1130.945896\n1419.831933\n1827.497187\n2671.202919\n434.493367\n\n\nWV\n321.0\n394601.000000\n394455.0\n394455.00\n394469.0\n394879.00\n394879.0\n196.965797\n321.0\n306.666667\n...\n2023-11-30 00:00:00\nNaN\n123.0\n1340.545489\n905.843432\n1095.441407\n1383.271660\n1485.772036\n1751.850634\n245.916508\n\n\nWY\n321.0\n394512.666667\n394445.0\n394445.00\n394462.0\n394631.00\n394631.0\n84.092719\n321.0\n458.666667\n...\n2023-11-30 00:00:00\nNaN\n92.0\n1386.686876\n1182.758469\n1252.376297\n1359.626203\n1525.037930\n1678.124614\n157.596041\n\n\n\n\n50 rows × 32 columns\n\n\n\n\n\n\n# Provide the mean for each numeric column by sex\ngrouped_df.mean(numeric_only = True)\n\n\n\n\n\n\n\n\nRegionID\nSizeRank\navg_price\n\n\nStateName\n\n\n\n\n\n\n\nAK\n394453.500000\n273.000000\n1845.229913\n\n\nAL\n448643.533333\n296.066667\n1196.697249\n\n\nAR\n394790.428571\n285.142857\n1095.547769\n\n\nAZ\n451262.875000\n214.375000\n1557.286231\n\n\nCA\n428576.656250\n191.437500\n2472.280505\n\n\nCO\n454493.666667\n348.833333\n2346.684403\n\n\nCT\n394815.600000\n119.600000\n2127.777496\n\n\nDE\n394795.000000\n189.000000\n1571.385754\n\n\nFL\n424798.708333\n174.916667\n1668.835798\n\n\nGA\n413673.210526\n326.421053\n1256.099519\n\n\nHI\n514445.000000\n183.666667\n2705.681385\n\n\nIA\n394623.571429\n320.714286\n1343.504147\n\n\nID\n446097.000000\n363.142857\n1576.334430\n\n\nIL\n468306.000000\n239.636364\n1220.872101\n\n\nIN\n422359.384615\n299.538462\n1105.728925\n\n\nKS\n394959.285714\n390.571429\n1100.784587\n\n\nKY\n394736.285714\n267.857143\n1201.673804\n\n\nLA\n394735.090909\n260.545455\n1263.411616\n\n\nMA\n394779.500000\n88.750000\n2246.404623\n\n\nMD\n484350.750000\n240.000000\n1552.644194\n\n\nME\n394678.000000\n196.500000\n2134.470922\n\n\nMI\n417161.437500\n247.875000\n1444.416460\n\n\nMN\n394836.000000\n231.166667\n1588.607379\n\n\nMO\n394825.571429\n396.000000\n1122.500158\n\n\nMS\n394880.833333\n308.000000\n1192.876828\n\n\nMT\n394577.833333\n421.666667\n1793.224137\n\n\nNC\n411102.045455\n274.500000\n1366.269717\n\n\nND\n394892.666667\n455.666667\n1651.132776\n\n\nNE\n394817.750000\n425.000000\n1320.009482\n\n\nNH\n619992.000000\n171.000000\n2582.894543\n\n\nNJ\n394756.000000\n161.000000\n1693.799261\n\n\nNM\n394666.777778\n440.444444\n1472.158264\n\n\nNV\n394673.666667\n452.833333\n1784.668225\n\n\nNY\n439881.200000\n163.300000\n1790.942113\n\n\nOH\n432299.833333\n141.750000\n1046.700170\n\n\nOK\n394820.000000\n465.454545\n1062.128302\n\n\nOR\n471653.571429\n379.928571\n1781.272373\n\n\nPA\n394875.421053\n224.368421\n1347.330330\n\n\nRI\n395005.000000\n39.000000\n2118.407898\n\n\nSC\n394772.600000\n225.800000\n1396.031749\n\n\nSD\n394845.000000\n424.333333\n1384.547508\n\n\nTN\n394705.181818\n192.636364\n1371.017577\n\n\nTX\n409264.806452\n259.967742\n1438.496827\n\n\nUT\n394902.000000\n320.750000\n1711.807021\n\n\nVA\n394914.300000\n188.400000\n1536.584520\n\n\nVT\n394429.000000\n206.000000\n2900.941104\n\n\nWA\n394891.866667\n303.733333\n1852.587674\n\n\nWI\n394791.727273\n247.727273\n1514.551976\n\n\nWV\n394601.000000\n306.666667\n1340.545489\n\n\nWY\n394512.666667\n458.666667\n1386.686876"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#announcements",
    "href": "slides/01-meet-the-toolkit.html#announcements",
    "title": "Meet the toolkit",
    "section": "Announcements",
    "text": "Announcements\n\nIf you have not yet completed the Getting to know you survey, please do so asap!\nIf you have not yet accepted the invite to join the course GitHub Organization, please do so asap!\nOffice hours linked at https://datasciaz.netlify.app/course-team.html"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#course-homepage",
    "href": "slides/01-meet-the-toolkit.html#course-homepage",
    "title": "Meet the toolkit",
    "section": "Course homepage",
    "text": "Course homepage\nLet’s take a tour!"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#collaboration-policy",
    "href": "slides/01-meet-the-toolkit.html#collaboration-policy",
    "title": "Meet the toolkit",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#sharing-reusing-code-policy",
    "href": "slides/01-meet-the-toolkit.html#sharing-reusing-code-policy",
    "title": "Meet the toolkit",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#use-of-generative-ai",
    "href": "slides/01-meet-the-toolkit.html#use-of-generative-ai",
    "title": "Meet the toolkit",
    "section": "Use of generative AI",
    "text": "Use of generative AI\n\nTreat generative AI, such as ChatGPT, as an online resource.\nGuiding principles:\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code.\n❌ AI tools for narrative: Unless instructed, you may not use generative AI to write narrative on assignments. You may use generative AI as a resource as you complete assignments but not for answers."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#academic-integrity",
    "href": "slides/01-meet-the-toolkit.html#academic-integrity",
    "title": "Meet the toolkit",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the UArizona iSchool Community Standard:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#most-importantly",
    "href": "slides/01-meet-the-toolkit.html#most-importantly",
    "title": "Meet the toolkit",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#five-tips-for-success",
    "href": "slides/01-meet-the-toolkit.html#five-tips-for-success",
    "title": "Meet the toolkit",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class.\nAsk questions.\nDo the readings.\nDo the lab.\nDon’t procrastinate – at least on a weekly basis!"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#course-toolkit-1",
    "href": "slides/01-meet-the-toolkit.html#course-toolkit-1",
    "title": "Meet the toolkit",
    "section": "Course toolkit",
    "text": "Course toolkit\n\n\nCourse operation\n\nMaterials: datasciaz.netlify.app\nSubmission: GitHub\nDiscussion: Slack\nGradebook: D2L\n\n\nDoing data science\n\nComputing:\n\nPython\nVS Code\nQuarto/Jupyter\n\nVersion control and collaboration:\n\nGit\nGitHub"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#learning-goals",
    "href": "slides/01-meet-the-toolkit.html#learning-goals",
    "title": "Meet the toolkit",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the course, you will be able to…\n\n\ngain insight from data\ngain insight from data, reproducibly\ngain insight from data, reproducibly, using modern programming tools and techniques\ngain insight from data, reproducibly and collaboratively, using modern programming tools and techniques\ngain insight from data, reproducibly (with literate programming and version control) and collaboratively, using modern programming tools and techniques"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#reproducibility-checklist",
    "href": "slides/01-meet-the-toolkit.html#reproducibility-checklist",
    "title": "Meet the toolkit",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for a data analysis to be “reproducible”?\n\n\nShort-term goals:\n\nAre the tables and figures reproducible from the code and data?\nDoes the code actually do what you think it does?\nIn addition to what was done, is it clear why it was done?\n\n\n\nLong-term goals:\n\nCan the code be used for other data?\nCan you extend the code to do other things?"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#toolkit-for-reproducibility",
    "href": "slides/01-meet-the-toolkit.html#toolkit-for-reproducibility",
    "title": "Meet the toolkit",
    "section": "Toolkit for reproducibility",
    "text": "Toolkit for reproducibility\n\nScriptability \\(\\rightarrow\\) Python\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto/Jupyter\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#python-and-jupyter-1",
    "href": "slides/01-meet-the-toolkit.html#python-and-jupyter-1",
    "title": "Meet the toolkit",
    "section": "Python and Jupyter",
    "text": "Python and Jupyter\n\n\n\n\n\n\n\n\nPython is an open-source general purpose programming language\nPython is also an environment for statistical computing and graphics\nIt’s easily extensible with packages\n\n\n\n\n\n\n\n\nJupyter is a convenient interface for Python called an IDE (integrated development environment), e.g. “I write Python code in the Jupyter IDE”\n\n\n\nJupyter is not a requirement for programming with Python, but it’s very commonly used by Python programmers and data scientists"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#python-vs.-jupyter",
    "href": "slides/01-meet-the-toolkit.html#python-vs.-jupyter",
    "title": "Meet the toolkit",
    "section": "Python vs. Jupyter",
    "text": "Python vs. Jupyter\n\n\n\n\n\n\n\nSource: Modern Dive (modified)."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#python-packages",
    "href": "slides/01-meet-the-toolkit.html#python-packages",
    "title": "Meet the toolkit",
    "section": "Python packages",
    "text": "Python packages\n\n\nPackages: Fundamental units of reproducible Python code, including reusable Python modules/functions, the documentation that describes how to use them, and sample data1\nAs of 23 July 2024, there are 557,005 Python packages (projects) available on PyPI (the Python Package Index)2\nWe’re going to work with a small (but important) subset of these!\n\n\n\n\n1 Beuzen and Timbers, Python Packages.\n2 PyPI contributed packages."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#tour-python-jupyter-via-vs-code",
    "href": "slides/01-meet-the-toolkit.html#tour-python-jupyter-via-vs-code",
    "title": "Meet the toolkit",
    "section": "Tour: Python + Jupyter (via VS Code)",
    "text": "Tour: Python + Jupyter (via VS Code)\n\n\n\nOption 1:\nSit back and enjoy the show!\n\n\n\nOption 2:\nClone the corresponding application exercise repo and follow along.\n ae-01-meet-the-penguins\nGo to the course GitHub organization and clone ae-01-meet-the-penguins to your environment."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#tour-recap-python-jupyter-via-vs-code",
    "href": "slides/01-meet-the-toolkit.html#tour-recap-python-jupyter-via-vs-code",
    "title": "Meet the toolkit",
    "section": "Tour recap: Python + Jupyter (via VS Code)",
    "text": "Tour recap: Python + Jupyter (via VS Code)"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#a-short-list-for-now-of-python-essentials",
    "href": "slides/01-meet-the-toolkit.html#a-short-list-for-now-of-python-essentials",
    "title": "Meet the toolkit",
    "section": "A short list (for now) of Python essentials",
    "text": "A short list (for now) of Python essentials\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses:\n\n\nto_this.do_this()\nto_that.do_that(to_this, with_those)\n\n\n\nPackages are installed with the pip install function (via the terminal)…\n\npip install package_name\n\n… and loaded with the import function, once per session (usually with a shorthand “nickname”):\n\n\nimport package_name as pkg"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#python-essentials-continued",
    "href": "slides/01-meet-the-toolkit.html#python-essentials-continued",
    "title": "Meet the toolkit",
    "section": "Python essentials (continued)",
    "text": "Python essentials (continued)\n\nColumns (variables) in data frames are accessed with ['']:\n\n\ndataframe['var_name']\n\n\n\nObject documentation can be accessed with help()\n\n\nhelp(pd.Series.mean)"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#pandas",
    "href": "slides/01-meet-the-toolkit.html#pandas",
    "title": "Meet the toolkit",
    "section": "pandas",
    "text": "pandas\n\npandas.pydata.org\n\nPandas is a quintessential package designed for data analysis"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#jupyter-notebooks-1",
    "href": "slides/01-meet-the-toolkit.html#jupyter-notebooks-1",
    "title": "Meet the toolkit",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\n\n\nFully reproducible reports – each time you run the analysis is ran from the beginning\nCode goes in code chunks narrative goes in markdown chunks\nA visual editor for a familiar / Google docs-like editing experience"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#tour-jupyter-notebooks",
    "href": "slides/01-meet-the-toolkit.html#tour-jupyter-notebooks",
    "title": "Meet the toolkit",
    "section": "Tour: Jupyter Notebooks",
    "text": "Tour: Jupyter Notebooks\n\n\n\nOption 1:\nSit back and enjoy the show!\n\n\n\nOption 2:\nClone the corresponding application exercise repo and follow along.\n ae-01-meet-the-penguins\nGo to the course GitHub organization and clone ae-01-meet-the-penguins to your environment."
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#tour-recap-jupyter-notebooks",
    "href": "slides/01-meet-the-toolkit.html#tour-recap-jupyter-notebooks",
    "title": "Meet the toolkit",
    "section": "Tour recap: Jupyter Notebooks",
    "text": "Tour recap: Jupyter Notebooks"
  },
  {
    "objectID": "slides/01-meet-the-toolkit.html#how-will-we-use-jupyter-notebooks",
    "href": "slides/01-meet-the-toolkit.html#how-will-we-use-jupyter-notebooks",
    "title": "Meet the toolkit",
    "section": "How will we use Jupyter Notebooks?",
    "text": "How will we use Jupyter Notebooks?\n\nEvery application exercise, lab, project, etc. is a Jupyter notebook\n\nHowever, projects will be built with Quarto Websites (more later)\n\nYou’ll always have a template Jupyter notebook to start with\nThe amount of scaffolding in the template will decrease over the semester\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#linear-algebra-1",
    "href": "slides/22-linear-algebra-I.html#linear-algebra-1",
    "title": "Linear algebra I",
    "section": "Linear algebra",
    "text": "Linear algebra\n\n\n\nLinear algebra is the study of vectors, vector spaces, and linear transformations.\nFundamental to many fields including data science, machine learning, and statistics."
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#vectors-1",
    "href": "slides/22-linear-algebra-I.html#vectors-1",
    "title": "Linear algebra I",
    "section": "Vectors",
    "text": "Vectors\n\nDefinition:\n\n\nVectors are objects that can be added together and multiplied by scalars to form new vectors. In a data science context, vectors are often used to represent numeric data.\n\n\n\n\nExamples:\n\n\nThree-dimensional vector: [height, weight, age] = [70, 170, 40]\nFour-dimensional vector: [exam1, exam2, exam3, exam4] = [95, 80, 75, 62]"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#vectors-in-python",
    "href": "slides/22-linear-algebra-I.html#vectors-in-python",
    "title": "Linear algebra I",
    "section": "Vectors in Python",
    "text": "Vectors in Python\n\nNumpyVisual\n\n\n\nimport numpy as np\nv = np.array([3, 2])\nprint(v)\n\n[3 2]\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nv = np.array([3, 2])\norigin = np.array([0, 0])\n\nplt.quiver(*origin, *v, scale=1, scale_units='xy', angles='xy')\nplt.xlim(0, 4)\nplt.ylim(0, 3)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#vector-addition",
    "href": "slides/22-linear-algebra-I.html#vector-addition",
    "title": "Linear algebra I",
    "section": "Vector addition",
    "text": "Vector addition\n\nExamplePythonVisual\n\n\n\n\nVectors of the same length can be added or subtracted componentwise.\nExample: \\(\\mathbf{v} = [3,2]\\) and \\(\\mathbf{w}=[2,-1]\\)\nResult: \\(\\mathbf{v}+\\mathbf{w}=[5,1]\\)\n\n\n\n\n\nv = np.array([3, 2])\nw = np.array([2, -1])\nv_plus_w = v + w\nprint(v_plus_w) \n\n[5 1]\n\n\n\n\n\n\nCode\nv = np.array([3, 2])\nw = np.array([2, -1])\nv_plus_w = v + w\n\nplt.quiver(*origin, *v, color='r', scale=1, scale_units='xy', angles='xy')\nplt.quiver(*origin, *w, color='b', scale=1, scale_units='xy', angles='xy')\nplt.quiver(*origin, *v_plus_w, color='g', scale=1, scale_units='xy', angles='xy')\nplt.xlim(0, 6)\nplt.ylim(-2, 3)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#vector-subtraction",
    "href": "slides/22-linear-algebra-I.html#vector-subtraction",
    "title": "Linear algebra I",
    "section": "Vector subtraction",
    "text": "Vector subtraction\n\nExamplePythonVisual\n\n\n\n\nVectors of the same length can be added or subtracted componentwise.\nExample: \\(\\mathbf{v} = [3,2]\\) and \\(\\mathbf{w}=[2,-1]\\)\nResult: \\(\\mathbf{v}-\\mathbf{w}=[1,3]\\)\n\n\n\n\n\nv = np.array([3, 2])\nw = np.array([2, -1])\nv_plus_w = v - w\nprint(v_plus_w) \n\n[1 3]\n\n\n\n\n\n\nCode\nv = np.array([3, 2])\nw = np.array([2, -1])\nv_plus_w = v - w\n\nplt.quiver(*origin, *v, color='r', scale=1, scale_units='xy', angles='xy')\nplt.quiver(*origin, *w, color='b', scale=1, scale_units='xy', angles='xy')\nplt.quiver(*origin, *v_plus_w, color='g', scale=1, scale_units='xy', angles='xy')\nplt.xlim(0, 6)\nplt.ylim(-2, 3)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#vector-scaling",
    "href": "slides/22-linear-algebra-I.html#vector-scaling",
    "title": "Linear algebra I",
    "section": "Vector scaling",
    "text": "Vector scaling\n\nExamplePythonVisual\n\n\n\n\nScaling vector \\(\\mathbf{v}=[3,2]\\) by \\(2\\)\nResult: \\([6,4]\\)\n\n\n\n\n\nv = np.array([3, 2])\nscaled_v = 2.0 * v\nprint(scaled_v)\n\n[6. 4.]\n\n\n\n\n\n\nCode\nscaled_v = 2 * v\n\nplt.quiver(*origin, *v, color='r', scale=1, scale_units='xy', angles='xy')\nplt.quiver(*origin, *scaled_v, color='g', scale=1, scale_units='xy', angles='xy', alpha=0.75)\nplt.xlim(0, 7)\nplt.ylim(0, 5)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#span",
    "href": "slides/22-linear-algebra-I.html#span",
    "title": "Linear algebra I",
    "section": "Span",
    "text": "Span\n\nThe span of a set of vectors \\(\\mathbf{v1},\\mathbf{v2},...,\\mathbf{vn}\\) is the set of all linear combinations of the vectors.\ni.e., all the vectors \\(\\mathbf{b}\\) for which the equation \\([\\mathbf{v1} \\space \\mathbf{v2} \\space... \\space\\mathbf{vn}]\\mathbf{x=b}\\)"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#linear-independent-vs.-dependent",
    "href": "slides/22-linear-algebra-I.html#linear-independent-vs.-dependent",
    "title": "Linear algebra I",
    "section": "Linear independent vs. dependent",
    "text": "Linear independent vs. dependent\n\nVectors are linearly independent if each vector lies outside the span of the remaining vectors. Otherwise, the vectors are said to be linearly dependent1.\n\n\n\n\n\n\nFor more information, see this nice blog on the topic"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#linear-independent-vs.-dependent-1",
    "href": "slides/22-linear-algebra-I.html#linear-independent-vs.-dependent-1",
    "title": "Linear algebra I",
    "section": "Linear independent vs. dependent",
    "text": "Linear independent vs. dependent\n\nThe vector on th right can be constructed by any two combination of the other vectors."
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#matrices-1",
    "href": "slides/22-linear-algebra-I.html#matrices-1",
    "title": "Linear algebra I",
    "section": "Matrices",
    "text": "Matrices\n\nDefinitionPython\n\n\n\nMatrices are collections of vectors arranged in rows and columns.\nRepresent linear transformations.\nExample Matrix:\n\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & 0 \\\\0 & 2 \\end{bmatrix}\n\\]\n\n\n\nA = np.array([[3,0],[0,2]])\nprint(A)\n\n[[3 0]\n [0 2]]"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#matrix-transposition",
    "href": "slides/22-linear-algebra-I.html#matrix-transposition",
    "title": "Linear algebra I",
    "section": "Matrix transposition",
    "text": "Matrix transposition\n\nMatrix transposition involves swapping the rows and columns.\nNotation: If \\(\\mathbf{A}\\) is a matrix, then its transpose is denoted as \\(\\mathbf{A}^T\\).\nGiven the matrix \\(\\mathbf{A}\\):\n\n\\[\n\\mathbf{A}= \\begin{bmatrix}a_{11} & a_{12} & a_{13}\\\\a_{21} & a_{22} & a_{23}\\\\a_{31} & a_{32} & a_{33}\\end{bmatrix}\n\\]\n\nThe matrix \\(\\mathbf{A}^T\\) is:\n\n\\[\n\\mathbf{A}^T=\\begin{bmatrix}a_{11} & a_{21} & a_{31}\\\\a_{12} & a_{22} & a_{32}\\\\a_{13} & a_{23} & a_{33}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#matrix-transposition-python",
    "href": "slides/22-linear-algebra-I.html#matrix-transposition-python",
    "title": "Linear algebra I",
    "section": "Matrix transposition: Python",
    "text": "Matrix transposition: Python\n\n\nCode\nimport numpy as np\n\n# Original matrix\nA = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\n# Transpose of the matrix\nA_T = A.T\n\nprint(\"Original Matrix:\")\nprint(A)\nprint(\"\\nTransposed Matrix:\")\nprint(A_T)\n\n\nOriginal Matrix:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nTransposed Matrix:\n[[1 4 7]\n [2 5 8]\n [3 6 9]]"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#matrix-vector-multiplication",
    "href": "slides/22-linear-algebra-I.html#matrix-vector-multiplication",
    "title": "Linear algebra I",
    "section": "Matrix-vector multiplication",
    "text": "Matrix-vector multiplication"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#matrix-vector-multiplication-1",
    "href": "slides/22-linear-algebra-I.html#matrix-vector-multiplication-1",
    "title": "Linear algebra I",
    "section": "Matrix-vector multiplication",
    "text": "Matrix-vector multiplication\nGeometrically"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#matrix-vector-multiplication-2",
    "href": "slides/22-linear-algebra-I.html#matrix-vector-multiplication-2",
    "title": "Linear algebra I",
    "section": "Matrix-vector multiplication",
    "text": "Matrix-vector multiplication\n\nExamplePythonVisual\n\n\n\nMatrix-vector multiplication transforms the vector according to the basis vectors of the matrix.\nFormula: \\(\\mathbf{A} \\cdot \\mathbf{v}\\)\n\n\\[\n\\mathbf{A} \\cdot \\mathbf{v} = \\begin{bmatrix}3 & 0 \\\\0 & 2 \\end{bmatrix} \\cdot [3, 2]\n\\]\n\n\n\nA = np.array([[3,0],[0,2]])\nv = np.array([3, 2])\nnew_v = A.dot(v)\nprint(new_v) \n\n[9 4]\n\n\n\n\n\n\nCode\nA = np.array([[3, 0], [0, 2]])\nv = np.array([3, 2])\nnew_v = A.dot(v)\n\nplt.quiver(*origin, *v, color='r', scale=1, scale_units='xy', angles='xy')\nplt.quiver(*origin, *new_v, color='g', scale=1, scale_units='xy', angles='xy')\nplt.xlim(0, 10)\nplt.ylim(0, 5)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#determinants",
    "href": "slides/22-linear-algebra-I.html#determinants",
    "title": "Linear algebra I",
    "section": "Determinants",
    "text": "Determinants\n\nThe determinant is a function that maps square matrices to real numbers: \\(\\text{Det}:ℝ^{m×m}→ℝ\\)\nwhere the absolute value of the determinant describes the volume of the parallelepided formed by the matrix’s columns."
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#determinants-python",
    "href": "slides/22-linear-algebra-I.html#determinants-python",
    "title": "Linear algebra I",
    "section": "Determinants: Python",
    "text": "Determinants: Python\n\nDeterminants measure the scale factor of a transformation.\nDeterminant of 0 indicates linear dependence.\n\n\nfrom numpy.linalg import det\nA = np.array([[3, 0], [0, 2]])\ndeterminant = det(A)\nprint(determinant)\n\n6.0"
  },
  {
    "objectID": "slides/22-linear-algebra-I.html#ae-15-linear-algebra",
    "href": "slides/22-linear-algebra-I.html#ae-15-linear-algebra",
    "title": "Linear algebra I",
    "section": "ae-15-linear-algebra",
    "text": "ae-15-linear-algebra\nPractice matrix operations (you will be tested on this in Exam 2)\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INFO 511: Fundamentals of Data Science",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nPREPARE\nMATERIALS\nDUE\n\n\n\n\n1\nWed, Jan 15\nWelcome + Intro to data science\n📚 DS From Scratch - Chp 1\n🎥 Week 1 Lectures  ⌨️ ae 00  💻 HW 0\n\n\n\n\n\nFri, Jan 17\nIntroducing your Toolkit\n📃 Jupyter + VSCode  OR  🎥 Jupyter + VSCode (up to 4:50)\n⌨️ ae 01\n\n\n\n\n2\nMon, Jan 20\nIntro to Python + Git/GitHub\n📚 DS From Scratch - Chp 2  🎥 Git + VSCode\n🎥 Week 2 Lectures  💻 DS experience\n\n\n\n\n\nFri, Jan 24\nIntro to Numpy + Pandas\n📚 Python Data Analysis - Chp 4  📚 Python Data Analysis - Chp 5\n💻 HW 1\n\n\n\n\n3\nMon, Jan 27\nExploratory Data Analysis\n📚 Prac Stats for DS - Chp 1  📚 Math for DS - Chp 3\n🎥 Week 3 Lectures  ⌨️ ae 02  ✅ ae 02\n\n\n\n\n\nFri, Jan 31\nData visualization\n📚 Python Data Analysis - Chp 9\n⌨️ ae 03  ✅ ae 03\n\n\n\n\n4\nMon, Feb 3\nData preprocessing\n📚 Python Data Analysis - Chp 7  📃 Data Preprocessing in Python\n🎥 Week 4 Lectures  ⌨️ ae 04  ✅ ae 04 💻 HW 2\n\n\n\n\n\nFri, Feb 7\nData wrangling\n📚 Python Data Analysis - Chp 8\n⌨️ ae 05  ✅ ae 05\nHW 1 @ 5pm\n\n\n5\nMon, Feb 10\nWeb scraping\n📚 DS From Scratch - Chp 9  📃 Web Scraping in Python\n🎥 Week 5 Lectures  ⌨️ ae 06  ✅ ae 06\n\n\n\n\n\nFri, Feb 14\nMidterm Review\n\n\n\n\n\n\n\n\n6\nMon, Feb 17\nMidterm Released\n\n\n🎥 Week 6 Lectures  📝 Midterm assignment\n\n\n\n\n\nFri, Feb 21\nData science ethics\n📚 DS From Scratch - Chp 26  🎥 Misrepresentation  🎥 Data privacy  🎥 Algorithmic bias  🎥 Alberto Cairo - How charts lie  🎥 Joy Buolamwini - How I’m fighting bias in algorithms\n\n\n\n\n\n\n7\nMon, Feb 24\nProbability  Final Project Released\n📚 DS From Scratch - Chp 6 (up to conditional probability)\n🎥 Week 7 Lectures  ⌨️ ae 07  ✅ ae 07  📓 final project description\n\n\n\n\n\nFri, Feb 28\nConditional probability\n📚 DS From Scratch - Chp 6 (Conditional probability)\n⌨️ ae 08  ✅ ae 08  💻 HW 3\nHW 2 @ 5pm\n\n\n8\nMon, Mar 3\nHypothesis testing\n📚 DS From Scratch - Chp 5\n🎥 Week 8 Lectures\n\n\n\n\n\nFri, Mar 7\nSampling distributions + inference\n📚 Review DS From Scratch - Chp 6 (Central Limit Theorem)  📚 IMS - Chp 13\n⌨️ ae 09  ✅ ae 09\nMidterm assignment due @ 5pm\n\n\n9\nMon, Mar 10\nSpring recess ☀️🌼\n\n\nNo Week 9 Lectures\n\n\n\n\n\nFri, Mar 14\nSpring recess ☀️🌼\n\n\nNo Week 9 Lectures\n\n\n\n\n10\nMon, Mar 17\nLinear regression\n📚 DS From Scratch - Chp 14 (up to Gradient Descent)\n🎥 Week 10 Lectures  ⌨️ ae 10  ✅ ae 10\n\n\n\n\n\nFri, Mar 21\nMultiple linear regression\n📚 DS From Scratch - Chp 15 (up to Regularization)\n⌨️ ae 11  ✅ ae 11\n\n\n\n\n11\nMon, Mar 24\nLogistic regression\n📚 DS From Scratch - Chp 16 (up to SVMs)\n🎥 Week 11 Lectures\n\n\n\n\n\nWed, Mar 26\n\n\n\n\n💻 HW 4\nHW 3 @ 5pm\n\n\n\nFri, Mar 28\nPrediction + uncertainty\n📚 DS From Scratch - Chp 11  📚 Python Data Analysis - Chp 12.1 - 12.2\n⌨️ ae 12  ✅ ae 12\n\n\n\n\n12\nMon, Mar 31\nModel validation\n📚 Python for DS - Chp 5.3\n🎥 Week 12 Lectures\n\n\n\n\n\nFri, Apr 4\nCalculus I\n📚 Math for DS - Chp 1 (up to Integrals)\n⌨️ ae 13  ✅ ae 13\n\n\n\n\n13\nMon, Apr 7\nCalculus II\n📚 Math for DS - Chp 1 (Integrals)\n🎥 Week 13 Lectures  ⌨️ ae 14  ✅ ae 14\n\n\n\n\n\n\n\nLinear Algebra I\n📚 Math for DS - Chp 4 (up to Eigenvalues)\n⌨️ ae 15  ✅ ae 15\n\n\n\n\n\nFri, Apr 11\nLinear Algebra II\n📚 Math for DS - Chp 4 (from Eigenvalues)  📃 Dr. Bernstein’s Posts on Linear Algebra\n🎥 Week 14 Lectures  ⌨️ ae 16  ✅ ae 16  💻 HW 5\nHW 4 @ 5pm\n\n\n14\nMon, Apr 14\nComparing machine learning models  (See YouTube recordings)\n📚 DS From Scratch - Chp 17\n🎥 Comparing Machine Learning Classifiers  🎥 Hyperparameter Tuning  ⌨️ ae 17  ✅ ae 16\n\n\n\n\n\nFri, Apr 18\n\n\n\n\n📝 Final assignment\nAEs final deadline\n\n\n15\nMon, Apr 21\nCommunicating data science results effectively\n📚 fdv - Chp 29\n\n\n\n\n\n\n\nFri, Apr 25\nFinal Review  Final Released\n\n\n\n\n💻 DS experience @ 11:59pm\n\n\n16\nMon, Apr 28\nWork on final project and assignment\n\n\n\n\n\n\n\n\n\nFri, May 2\n\n\n\n\n\n\n\n\n\n\n17\nMon, May 5\nWork on final project and assignment\n\n\n\n\nHW 5 @ 5pm\n\n\n\nWed, May 7\n\n\n\n\n\n\n\n\n\n\nFinals\nWed, May 14\nFinish final project\n\n\n\n\nFinal project due @ 5pm  Final assignment due @ 5pm",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing-A.html",
    "href": "ae/ae-09-hypothesis-testing-A.html",
    "title": "Quantifying uncertainty",
    "section": "",
    "text": "An article in the The Tucson Citizen-Times published in the summer of 2020 claims that the average price per guest (ppg) for properties in Tucson is $100 on Airbnb. To evaluate their claim we will use a dataset on 50 randomly selected Asheville Airbnb listings in July 2024. These data can be found in data/tucson.csv.\nLet’s load the packages we’ll use first.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import ttest_1samp\nAnd then the data.\ntucson = pd.read_csv(\"data/tucson.csv\")"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing-A.html#generate",
    "href": "ae/ae-09-hypothesis-testing-A.html#generate",
    "title": "Quantifying uncertainty",
    "section": "Generate",
    "text": "Generate\nWe’ll start by generating the null distribution.\n\nGenerate the null distribution and call it null_dist\n\n\nnp.random.seed(4321)\n\n\nnull_dist = np.random.normal(loc=100, scale=tucson['ppg'].std(), size=1000)\n\n\nTake a look at null_dist. What does each element in this distribution represent?\n\n\nnull_dist\n\narray([ 50.03477537, 162.65487326, 194.93106499,  65.46579584,\n        82.74130219,  62.70337221,  51.03913965, 245.24489938,\n       142.84833267,  26.18667883, 121.52265421, 108.54001302,\n       152.24213198,   4.65396831,   7.99945009,  -7.75783147,\n        90.65062465, 133.66708015, 183.44169565,  20.14010862,\n        49.21399216,  91.58607486, 136.20392946, 127.27325708,\n       121.46585262, 174.31174926,  99.47730758, 150.18901071,\n        11.8026206 ,  57.47695089,  -7.57612584, 164.31051874,\n       213.83596689,  80.60786117, 123.79650421,   4.02649307,\n       134.75867521,  -5.89264938,  11.50108587,  49.97103992,\n       118.5506322 ,  12.38859311,  35.64476738,  31.43368303,\n        76.72419991,  16.26035182, 228.14269044, 110.62569413,\n       127.87323838, 111.28257104,  40.76076578, 134.85515917,\n       267.50017303, 184.828656  , 162.05471212,  51.64389619,\n        19.71374339,  97.50338125, 174.09883061,  17.18417508,\n        46.06683466, -26.09261409, 195.74295681, 122.84642852,\n        43.18323498, 172.21573446, 107.78222391,  81.59440132,\n       124.91730229, 189.82708578,  43.80534897,  29.3806034 ,\n       156.47789448, -77.87403112, 169.16026359, 144.36725648,\n       119.5981452 , 133.98092805, 209.06125973,  88.70135948,\n       117.69170449,  73.81173556, 159.74481661, 174.71030438,\n       -11.66276834,  32.85956222,  93.33251141, 197.98806749,\n       105.76650384,  45.21975612,  89.41257315, 139.59734251,\n        63.65595664, 140.84097732, 109.02315651,  38.55153372,\n       179.72332537,  29.21506949, -44.50213291,  82.90718337,\n       141.34780875,  86.9060758 ,  37.02845199,  86.43741944,\n        56.27832012, -13.18165335,  85.35981833,  82.77992955,\n       158.49727934, 131.77775737, 174.47820502,  -2.97219417,\n       164.92103389,  78.71476133, 131.3153663 ,  69.15511409,\n       172.94061275, 102.80175097, 165.76724604, -25.92430498,\n        61.15786494,  79.10043594, 127.42064928, -32.36565577,\n       147.14067022,  87.15616093,  51.79222151, 223.72790513,\n       177.16746666,  37.51361428,  93.42769412,  67.56451882,\n        89.30774363,  87.1367526 , 146.95063271, 132.39549426,\n       236.04539625,  83.87180643, 181.30906025, 129.52649907,\n       149.73703062,  88.5063165 , 202.41890758,  90.71484082,\n       223.20965695, -42.96660578,  66.51272242,  81.76844948,\n       174.59128854,  28.46072012, -34.34408263,  26.50853357,\n        57.47881921, 196.36039416,  89.85212574, 202.39710527,\n        39.44006389,  26.20748416, 152.93419099,  65.73306136,\n        76.15160163, 110.17581183,  85.01689034,   2.11688942,\n        69.91651262, 110.46185883, 121.31333953, -18.29999849,\n        94.85352447,  78.84853585, 149.81572107, 132.61847733,\n       203.14346401,  69.58899544,  79.39913017,  98.03201902,\n       187.66433139, 164.97035709,  -8.90458706,  91.70685827,\n       162.15748114,  20.82123933, 109.19986866, 137.73186335,\n       145.65968199, 105.81663209,  85.70626059,  98.73920164,\n        35.50367189, 179.10643713,  30.85771642,  70.47482547,\n       191.0488096 , -39.57463069, 167.4731793 ,  86.83301089,\n        80.59868979,  36.66864536, 125.18761125,  45.18938202,\n       -43.36100845, 106.57612465, 100.57068298, -52.494337  ,\n        46.7514217 , 175.34713677, 124.97788937, 167.95041601,\n        39.91041093,  53.44331349,  90.25055209,  22.85310816,\n       131.66131385, 199.33591465, 210.41069548, 128.02471404,\n        96.53704018, 141.49976281,  65.24161702, 213.92567555,\n       190.6353054 , 198.3140315 ,  99.41053935,  78.90655367,\n        -3.49133821,  38.37484546,  41.02726262, -12.8268799 ,\n        31.59170686,  80.53614899,   3.87269836, 133.49269003,\n        89.31375895, 117.01179148,   6.58379686, 102.99658827,\n       211.33324252, 101.00076715,  48.20206965, 119.37396474,\n       116.10165816,  59.89927992, 152.07471801,  51.0745799 ,\n        90.66543854,  -8.64560307,  14.93422678, 108.6231274 ,\n        86.90878462, 100.13816433, 134.3666715 ,  83.10956739,\n        86.66882805,  99.57203663, 110.70916352,  98.29057436,\n        43.28270013, 156.33266087,  61.40125244,  74.1455775 ,\n       216.31490751, 187.42522799,  62.51091613, 206.70907811,\n       -17.06567909, 144.60340096,  72.38409907, 136.72934156,\n       124.42333655, 208.87529176, 151.4755531 ,  43.67291462,\n        23.36929675, 123.83349443,  27.23303186,  30.05703205,\n       -11.31270716,  85.62782564,  48.26224309,  39.17121767,\n        99.96222567, 278.11522916, 100.82030179, 168.8927298 ,\n        87.89074577, 275.04897954, 131.48766206,  93.44432843,\n       106.07919479,  64.17541289, 165.64195502,  65.5742781 ,\n       117.29443561, 113.90627944,  31.3969706 ,  97.84244564,\n       101.334517  , 123.68758695, 193.15530499,  22.48339778,\n        88.84196308,  48.78782986,  86.6445352 ,  65.55026344,\n       102.58490892,  58.65826137,  25.32552571, 193.43617362,\n       155.95369616,  32.47891225,   5.71804076,  31.61106213,\n       110.19373216, 128.62369924,  95.56078661,  48.01804111,\n        74.47577163,  80.40757825, 170.68864975,  68.03900094,\n       158.8422413 , 143.23896841,  88.52817827,  77.56306375,\n       117.8467025 , 117.28056555, 130.41957323,  32.86492458,\n       136.42513328,  66.02730555, 200.25293066, 149.08539361,\n        43.72803874, 157.40046199, 123.78666082,  31.96265279,\n       172.72485542,  25.93192901, 150.6308481 ,  98.2712681 ,\n       124.83190047, 127.07962357, -35.52653712, 142.80969401,\n       122.21466583, 105.14016077, 233.44644143, 118.36945528,\n        61.29304433,  83.64753028,  30.9128471 , 198.05904837,\n       137.42548023,  69.4718388 , 143.05174323, 146.73863655,\n       138.72374026, 115.81737411,  90.45291363, 145.71051425,\n        41.20311271,  94.46941847, -20.85079215, -41.47632443,\n         3.10570214, 166.46391219, 101.8542853 , 234.85516864,\n        43.30501759,  52.08126949, 177.75474292, 176.71356109,\n       217.51329628,  38.39256573,  24.41474428, 112.615036  ,\n       177.50573083, 108.81856212,  90.6318923 , 133.2719098 ,\n       -29.929483  , 116.24176422, 152.54242865, 291.45160614,\n       129.42847224, 198.3137503 ,  68.09383314, -20.59555998,\n       270.28893202, 200.33457336, 212.77407594, 116.96194532,\n        30.00139798,  62.67213666,  49.13354278, 119.04787936,\n       -46.97854604,  16.21655963,  96.56573161, 131.93326492,\n        53.34076595, 150.26661473,  19.38815062,  25.90059663,\n        92.66619765, 115.93991423, 196.94295444, 160.13560498,\n       109.03785752,   7.28965331, 186.63549917,  37.90613997,\n       189.2021799 , 157.42513292, 179.95133968,  -5.77006256,\n       260.87012072, 154.51698335,  -8.41322528,   5.26891891,\n       133.59358158, 198.2062956 ,  31.7572955 , 138.09565829,\n        -7.68504696, 122.00149444, 132.99939165, 127.74987029,\n         8.93047564, 178.58029168, 130.71733298, 179.94840842,\n        49.57583168, -22.75865631, 110.67777638, 235.39192231,\n       158.22733683,  89.93216619,  12.43560813,  65.33497757,\n        57.69195717,  39.47019198, 107.64137403, 146.58757115,\n        94.10436073, 164.97795405,  79.61158445, 114.73599938,\n       201.5150552 , 110.36574725, 135.34416344, 121.22243811,\n        69.34390429,  -5.11608558, 188.81754578, 217.03677753,\n       -22.0996553 , 175.22875008,  42.43590492, -32.44512902,\n        17.88189511,  58.46764944,  52.4198987 , 134.20034263,\n        33.47173165,  37.24022915, 157.69505801, -23.73722151,\n        87.1140624 , 183.29636684,  46.1379621 , 159.26386891,\n       161.20613825,  45.14862188,  16.0487506 ,  95.94162518,\n       134.73712373, 169.11140365,  20.48836696, 132.66538363,\n        91.36483052,  10.84498753,  77.60531639, 110.97190111,\n       109.06567206, 247.46999783,  86.09116702, 105.52675235,\n        69.91120231,  36.17594383, 159.68320934,  79.40300067,\n       265.66607899, 125.86884091, 170.45525917, 116.91882114,\n       122.8187129 , 175.56890559,  18.50856925,  73.47955237,\n       139.03476665,  45.56103297,   7.24820456,  91.46922022,\n        36.39206812, 112.58977368,  62.94104308,  26.83956316,\n       103.9416371 ,  95.69756212, 177.85825144, 152.34371351,\n       202.97497898,  32.72002818, 133.36479191, 173.96950517,\n        58.97750925, 153.59813041,  51.95969862, 164.00557661,\n       218.40662049, 149.38959918, 108.35488311, 136.52133245,\n       117.20174876, 105.76114601,  80.92480655, 161.39572507,\n       158.22807951, 142.70416329,  60.45988109,  98.99976515,\n       127.92924561,  93.92355622, 242.38277125, 111.23135704,\n       167.73167973, 132.77100957, 171.60347096, 118.82480072,\n        33.47958101,  78.4365225 , 150.05396124, 151.04940056,\n        86.25553608, 182.73527874, 126.26225316, 122.61339718,\n        74.58896451, 128.0175515 ,  60.88288226,  78.44899522,\n       186.41969118, 119.18307356, 153.18190205,  98.03931489,\n       209.1383879 , 158.04898855, 144.0202827 , 133.50632511,\n       138.30666307, 237.40351383, 121.56707727, 109.66608483,\n        77.8470543 ,  -7.60293646, -12.32960326, 112.0408156 ,\n        65.81992191, 112.99106499,  88.04151642,  77.69303622,\n       119.23033016,  54.5953792 , 141.9323225 ,  80.65855389,\n       210.779073  , 192.66645026,  82.10695781, 177.91073788,\n       120.17950195, 104.85750109,  47.58952969, 145.07614208,\n       160.55601209, 139.04864833, 145.94601569, 130.76173562,\n        91.72467301, -47.86360162, 182.811149  , 223.25454533,\n        80.14854246,  47.14023982,   4.64939869,   3.69944265,\n       123.68580107,  99.96686455,  23.92270615, 141.23723309,\n       125.72868489, 138.61018087,  86.46938655,  -6.09543628,\n       209.63189481,  48.78625572, -19.77081485,  10.0466482 ,\n       154.56405452, 140.15028138,  52.11965768, 110.22257002,\n        99.43793133,  79.33266239,  11.10075585,  39.66061776,\n       101.05790292,  75.67210489,  85.67878568, 117.56131287,\n       -44.92581639, 163.72894567,  30.51432303, 287.05274177,\n        77.18479367,  92.03055246, 131.09192823,  14.36024811,\n        46.24184697, 199.56150758,  22.60876933,  52.85689941,\n        61.40021674, 198.74138913, 129.3538796 , 145.41361522,\n        95.65404806, 121.60533516, 140.08020563, 190.94028775,\n        98.04756976,  97.04919582, 171.66583254, 121.85377048,\n       161.9292013 , 123.85621667, -86.34246407, 168.64028814,\n        39.63822726, 146.40181145,  79.99975457,  91.1729282 ,\n       165.45501495,  -9.31084539, 112.96713139, 188.45579206,\n       141.49563689, 194.60985876,  47.97512043,  56.49394837,\n       211.94771074, 154.01587344, 116.67889898,  10.53061668,\n       119.88697374,  69.07145634, 126.92990266, 219.71016004,\n        92.76915171, 115.99014122, 146.61942439,  -5.41921302,\n       204.3604502 ,  99.29667241,   1.3753202 ,  52.41065343,\n       126.8209805 ,  98.56736021, 153.22987457, 222.29932116,\n       102.87214273, 149.02023916,  21.64149149,  97.5264277 ,\n        67.89017443, -56.98246263,  37.87858447,  87.36334955,\n       193.03197207, 119.50264443, 225.52403809, 163.6907018 ,\n       111.34084682,  82.47209095, 198.2606687 ,  44.21108783,\n         8.83954279,  64.25305435, -13.54524455,  44.02396654,\n       100.6614043 ,  92.54436655,  30.36927832,  89.16700162,\n        21.29793016, 228.05251142, 138.14357519, 183.41468256,\n       170.01971047, 150.77410514, 132.24862745,  52.48777558,\n       107.20575994, 154.8257222 , 115.09865453,  52.87316414,\n       148.6365115 ,  56.40870255, 154.33407766,  47.77764549,\n        84.39443538, 168.62837302, 154.03621847, 123.1307913 ,\n       152.81495759,  29.69343183, 151.55996635, 160.66190487,\n       192.13319232, 117.26433434, 153.57880414, 172.58636579,\n        51.68457862, 162.06681773, 205.64076059,  92.95599937,\n        37.5516246 , 136.13685013,  58.37281338, 115.50333122,\n        46.88615283, -92.88525262, 184.03533564,  80.22977381,\n        89.90543017, 149.09038527, 127.94080305,  84.73919225,\n        33.09411859,  65.7363112 ,  27.54835241, 166.96508045,\n       174.9431656 ,  83.93113337, 132.58893323,  46.26679525,\n        85.4289971 ,  58.91669165,  90.24579269, 161.03287282,\n        79.32345616, 125.90835937,  87.54497916,  10.5447459 ,\n       229.53869337, 131.99298622,  76.71954507, 226.19497054,\n        60.0459713 ,  91.74096382, 231.35454867, -43.78219432,\n       139.95032452,  83.34817373,  77.84157847,  62.75422266,\n        97.63543707,  60.1483067 ,  80.81659112, 146.09645649,\n        89.63486314, 114.77963205, 145.18882543, 157.26272551,\n       213.0383413 , 117.27511248, 194.95828888, 122.56402944,\n        94.15334215,   9.19722467, 165.54627326,  62.26886157,\n        66.16969734, 149.1126258 ,  85.4417487 ,  42.68864231,\n       161.28995141,  31.57898596, 166.23798717, 162.65247464,\n        46.07511404, 111.41667369,  56.68838668,  92.92761897,\n        44.75340845, 218.77920338,  76.64268612,  62.36082462,\n       142.60048317, 139.05504233,  97.02256758, 155.29692527,\n       161.41577911,  79.21820276,  31.13981384, 166.27669283,\n        82.89332738,  96.28340898, 198.62387325, 106.5692619 ,\n       192.58304601, 123.10009631,  79.63411191, 120.48473844,\n        62.08372307, 249.85715311, 136.79427319,  13.29827742,\n       -10.57803885,  49.72492207,  -4.15713336, 297.54973298,\n        86.00102375, 113.11036453,  51.6036414 , 108.25362924,\n       178.01559651, 232.92963646,  64.6619643 , 137.22871104,\n        -2.18550518,  57.99695096, 205.88235894,  62.67314752,\n        79.78822045, 113.99573347,  93.48519209,  88.80112176,\n        86.9241504 , 126.8065952 , 118.94290879, 120.51990244,\n       164.3930736 , 174.23977326,  96.20838572,  60.20928801,\n        16.52112725,  67.06582105, 210.13216131, 104.67267484,\n       -12.03442876,  37.09508665,  70.77199552, 165.11617635,\n       219.07980779, 142.82759531,  51.27202576, 164.78036426,\n       171.07621271, 154.41378671, 111.58150677, 101.87427717,\n       140.13342838, 210.43886018,  44.80694817, 154.21119467,\n       115.49700414, 187.81019085, -12.10242778, 165.00484795,\n       120.77217759, 239.19180246,  -1.20825092, 110.40101739,\n       166.40260651, -99.76291722,  93.71349591, 109.97991264,\n        77.65466238, 165.11858273, 149.88391579,  90.62321645,\n        72.37884081, 139.24090685, 140.28683694, 206.28330594,\n       264.95636459,  98.21565728, 105.73742748, 181.19738883,\n       115.74809147, 204.62703977,  80.13909664,  48.99109267,\n       215.17723324,  55.04057618,  -6.5407207 , 109.73988255,\n       131.22313984, 239.56384429,  81.41347579, 191.00363375,\n        95.46691658, 215.76492035, 116.92735202,  61.9363502 ,\n       102.8914134 ,  75.16653501, 244.31287003, -43.79812565,\n        63.34672358, 145.6404274 ,  67.77330133, 196.01786617,\n       132.55337106, 179.59494808, 100.82465785,  86.8274616 ,\n       106.17454612,  69.70623925, 119.35071763,   4.54387517,\n       153.03310978, 166.62688634,  31.4415788 ,  65.45358125,\n        94.61227476, 159.05595392,  54.2484859 ,  10.0904782 ,\n       -13.51147304,  95.942916  , 136.85399226,  71.49770242,\n       124.35786213, 103.99998955,  45.00331983, 163.16714273,\n       169.40416695, 159.88402451,  88.35847288,  69.96860958,\n        76.22091419, 170.17409284, 197.14733411, -48.17085397,\n        84.04900777,  34.67567656,  78.52271043, 143.4256792 ,\n       130.44794273, 158.88849751, 201.48585198,  68.5977069 ,\n       114.13311534, 123.7190078 ,  69.62342198, 205.57850237,\n        21.32812133, 129.90334648,  67.88402524,  21.23596491,\n        75.22661864,  90.15695264,  56.05598149,   3.05778085,\n       171.60250908, 183.51526687,  16.72896952,  54.11277909,\n        66.58364056, 180.8671317 , 254.68221636,  40.67707914,\n       104.83667629, 111.75032197, -16.26297054, 132.85242288,\n       138.6726092 ,  99.82635096,  87.96752679, 118.06593876,\n       -18.79223914, 133.8235922 ,  95.5207079 ,  91.91877248,\n       187.02998049,  35.02670179,  92.27980414, 144.56018346])"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing-A.html#visualize",
    "href": "ae/ae-09-hypothesis-testing-A.html#visualize",
    "title": "Quantifying uncertainty",
    "section": "Visualize",
    "text": "Visualize\n\nQuestion: Before you visualize the distribution of null_dist – at what value would you expect this distribution to be centered? Why?\n\nAt 100, since we created this distribution assuming \\(\\mu=100\\)\n\nCreate an appropriate visualization for your null distribution. Does the center of the distribution match what you guessed in the previous question?\n\n\nsns.histplot(null_dist, kde=True, palette=\"colorblind\")\nplt.xlabel('Sample Mean')\nplt.ylabel('Frequency')\nplt.title('Null Distribution with Sample Mean')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow, add a vertical red line on your null distribution that represents your sample statistic.\n\n\nsns.histplot(null_dist, kde=True)\nplt.axvline(116.24, color='red', linestyle='solid', linewidth=2)\nplt.xlabel('Sample Mean')\nplt.ylabel('Frequency')\nplt.title('Null Distribution with Sample Mean')\nplt.show()\n\n\n\n\n\n\n\n\n\nQuestion: Based on the position of this line, does your observed sample mean appear to be an unusual observation under the assumption of the null hypothesis?\n\nYes, it’s pretty far from the center."
  },
  {
    "objectID": "ae/ae-03-tucson-housing-A.html",
    "href": "ae/ae-03-tucson-housing-A.html",
    "title": "AE 03: Tucson Housing",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\n\n\n\nPackages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nRead in Data + wrangle\n\ntucson_housing = pd.read_csv(\"data/tucsonHousing.csv\")\n\ngarage_types = [\"Single Family\", \"Townhouse\"]\n\ntucson_housing['garage'] = tucson_housing['type'].apply(lambda x: 'Garage' if x in garage_types else 'No garage')\n\n\n\nExercise 1\nFrom the histograms, it appears that houses with garages (assumed based on house type) generally have a wider range of prices compared to those without garages. This suggests that having a garage might be associated with higher or more variable house prices in Tucson.\n\ng = sns.FacetGrid(tucson_housing, row=\"garage\")\ng.map(sns.histplot, \"price\", kde=False, binwidth=50000)\ng.set_axis_labels(\"Price\", \"Count\")\ng.set_titles(\"{row_name}\")\ng.fig.suptitle(\"Distribution of House Prices in Tucson by Garage Availability\", y=1.05)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\nClaim 1: Larger houses are priced higher. The scatter plot shows a positive correlation between house area and price, indicating that larger houses tend to be priced higher.\nClaim 2: Newer houses are priced higher. From the color gradient, it appears that newer houses (lighter colors) are generally more expensive, supporting the claim that newer houses are priced higher.\nClaim 3: Bigger and more expensive houses tend to be newer ones than smaller and cheaper ones. The plot suggests that bigger and more expensive houses are often newer, as indicated by the clustering of lighter-colored points (newer houses) towards the upper right of the plot.\n\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nplt.figure(figsize=(8, 6))\nscatter = sns.scatterplot(data=tucson_housing, x='area', y='price', hue='year_built', palette='viridis')\n\nlowess_fit = lowess(tucson_housing['price'], tucson_housing['area'], frac=0.3)\nlowess_x = [point[0] for point in lowess_fit]\nlowess_y = [point[1] for point in lowess_fit]\n\nplt.plot(lowess_x, lowess_y, color='red', linewidth=2)\n\nplt.title('Relationship between House Price and Area in Tucson, Colored by Year Built')\nplt.xlabel('Area (square feet)')\nplt.ylabel('Price ($)')\nplt.legend(title='Year Built', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.show()"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html",
    "href": "ae/ae-04-flights-preprocessing-A.html",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\nimport numpy as np\nfrom nycflights13 import flights"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html#exercise-1---load-data",
    "href": "ae/ae-04-flights-preprocessing-A.html#exercise-1---load-data",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 1 - Load data",
    "text": "Exercise 1 - Load data\nFill in the blanks:\n\n# Load the flights data\ndf = flights\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 336776 entries, 0 to 336775\nData columns (total 19 columns):\n #   Column          Non-Null Count   Dtype  \n---  ------          --------------   -----  \n 0   year            336776 non-null  int64  \n 1   month           336776 non-null  int64  \n 2   day             336776 non-null  int64  \n 3   dep_time        328521 non-null  float64\n 4   sched_dep_time  336776 non-null  int64  \n 5   dep_delay       328521 non-null  float64\n 6   arr_time        328063 non-null  float64\n 7   sched_arr_time  336776 non-null  int64  \n 8   arr_delay       327346 non-null  float64\n 9   carrier         336776 non-null  object \n 10  flight          336776 non-null  int64  \n 11  tailnum         334264 non-null  object \n 12  origin          336776 non-null  object \n 13  dest            336776 non-null  object \n 14  air_time        327346 non-null  float64\n 15  distance        336776 non-null  int64  \n 16  hour            336776 non-null  int64  \n 17  minute          336776 non-null  int64  \n 18  time_hour       336776 non-null  object \ndtypes: float64(5), int64(9), object(5)\nmemory usage: 48.8+ MB\n\n\nThe flights data frame has 336776 rows. Each row represents a observation."
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html#exercise-2---data-cleaning",
    "href": "ae/ae-04-flights-preprocessing-A.html#exercise-2---data-cleaning",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 2 - Data cleaning",
    "text": "Exercise 2 - Data cleaning\nRemove rows with missing values in the arr_delay and distance columns.\nWhat are the names of the variables in flights.\n\ndf_clean = df.dropna(subset=['arr_delay', 'distance'])"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html#exercise-3---original-data-distribution",
    "href": "ae/ae-04-flights-preprocessing-A.html#exercise-3---original-data-distribution",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 3 - Original Data Distribution",
    "text": "Exercise 3 - Original Data Distribution\n\nPlot the original distributions of arr_delay and distance.\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.histplot(df_clean['arr_delay'], bins=30, kde=True, ax=axes[0]).set(title='Original Arrival Delay')\nsns.histplot(df_clean['distance'], bins=30, kde=True, ax=axes[1]).set(title='Original Distance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html#exercise-4---check-for-skewness",
    "href": "ae/ae-04-flights-preprocessing-A.html#exercise-4---check-for-skewness",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 4 - Check for Skewness",
    "text": "Exercise 4 - Check for Skewness\n\nCalculate and print the skewness of arr_delay and distance.\n\n\nskew_arr_delay = df_clean['arr_delay'].skew()\nskew_distance = df_clean['distance'].skew()\nprint(f\"Skewness of Arrival Delay: {skew_arr_delay}\")\nprint(f\"Skewness of Distance: {skew_distance}\")\n\nSkewness of Arrival Delay: 3.716817480457187\nSkewness of Distance: 1.1133926208294944"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html#exercise-5---scaling",
    "href": "ae/ae-04-flights-preprocessing-A.html#exercise-5---scaling",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 5 - Scaling",
    "text": "Exercise 5 - Scaling\n\nCheck the summary statistics of arr_delay and distance to see if scaling is necessary.\n\n\ndf_clean['arr_delay'].describe()\n\ncount    327346.000000\nmean          6.895377\nstd          44.633292\nmin         -86.000000\n25%         -17.000000\n50%          -5.000000\n75%          14.000000\nmax        1272.000000\nName: arr_delay, dtype: float64\n\n\n\ndf_clean['distance'].describe()\n\ncount    327346.000000\nmean       1048.371314\nstd         735.908523\nmin          80.000000\n25%         509.000000\n50%         888.000000\n75%        1389.000000\nmax        4983.000000\nName: distance, dtype: float64\n\n\n\nQuestion: Do arr_delay and distance need to be scaled?\n\nYes, the units are completely different.\n\nApply standard scaling, maximum absolute scaling, and Min-Max Scaling to the transformed arr_delay and distance.\n\n\n# Standard Scaling\nscaler = StandardScaler()\ndf_clean.loc[:, ['arr_delay_standard', 'distance_standard']] = scaler.fit_transform(df_clean[['arr_delay', 'distance']])\n\n# Maximum Absolute Scaling\nmax_abs_scaler = MaxAbsScaler()\ndf_clean.loc[:, ['arr_delay_maxabs', 'distance_maxabs']] = max_abs_scaler.fit_transform(df_clean[['arr_delay', 'distance']])\n\n# Min-Max Scaling\nmin_max_scaler = MinMaxScaler()\ndf_clean.loc[:, ['arr_delay_minmax', 'distance_minmax']] = min_max_scaler.fit_transform(df_clean[['arr_delay', 'distance']])\n\n\nQuestion: What are the two pros and two cons of standardizing data?\n\n\nPros\n\nImproved Model Performance:\n\nConsistency: Ensures that features contribute equally to the model, enhancing performance for algorithms like linear regression and neural networks.\nSpeed: Helps optimization algorithms, like gradient descent, converge faster.\n\nEnhanced Interpretability:\n\nStandardization: Makes model coefficients easier to understand, especially in linear models.\nComparison: Simplifies comparison between features during data analysis.\n\n\n\n\nCons\n\nPotential Loss of Interpretability:\n\nRaw Values: Scaled values might lose their original meaning and units.\n\nAssumption of Distribution:\n\nNormality: Some methods assume data is normally distributed, which may not always be true.\nSensitive to Outliers: Outliers can distort scaled values in methods like standard scaling."
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing-A.html#exercise-6---transformation",
    "href": "ae/ae-04-flights-preprocessing-A.html#exercise-6---transformation",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 6 - Transformation",
    "text": "Exercise 6 - Transformation\n\nCheck the summary statistics again with your min-max standardized columns.\n\n\ndf_clean['arr_delay_minmax'].describe()\n\ncount    327346.000000\nmean          0.068406\nstd           0.032867\nmin           0.000000\n25%           0.050810\n50%           0.059647\n75%           0.073638\nmax           1.000000\nName: arr_delay_minmax, dtype: float64\n\n\n\ndf_clean['distance_minmax'].describe()\n\ncount    327346.000000\nmean          0.197506\nstd           0.150094\nmin           0.000000\n25%           0.087497\n50%           0.164797\n75%           0.266979\nmax           1.000000\nName: distance_minmax, dtype: float64\n\n\n\nQuestion: Why should you use the min-max scaled data instead of a different scaling for the transformations (hint: especially log transformation)\n\nThe other transformations had with negative values for arr_delay.\n\nApply a log transformation to arr_delay if it is positively skewed and apply a square root transformation to distance if it is negatively skewed (use if else statements).\nHint: Logical operators in Python:\n\n\n\n\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?\n\n\nx and y\nis x AND y?\n\n\nx or y\nis x OR y?\n\n\npd.isna(x)\nis x NA?\n\n\n~pd.isna(x)\nis x not NA?\n\n\nx in y\nis x in y?\n\n\nx not in y\nis x not in y?\n\n\nnot x\nis not x? (only makes sense if x is True or False)\n\n\n\n\n\nif skew_arr_delay &gt; 0:\n    df_clean.loc[:, 'arr_delay_transformed'] = np.log1p(df_clean['arr_delay_minmax'])\nelse:\n    df_clean.loc[:, 'arr_delay_transformed'] = df_clean['arr_delay_minmax']\n\nif skew_distance &gt; 0:\n    df_clean.loc[:, 'distance_transformed'] = np.sqrt(df_clean['distance_minmax'])\nelse:\n    df_clean.loc[:, 'distance_transformed'] = df_clean['distance_minmax']\n\n\nQuestion: Why do we have to add a constant when we perform a log or square-root transformation (i.e., np.log1p(df['column' + 1]))?\n\nThe logarithmic and square-root functions do not contain negative numbers or 0."
  },
  {
    "objectID": "ae/ae-08-conditional-probability.html",
    "href": "ae/ae-08-conditional-probability.html",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "",
    "text": "Learn to calculate and interpret the probability of having a disease given a positive test result using sensitivity, specificity, and prevalence data."
  },
  {
    "objectID": "ae/ae-08-conditional-probability.html#goal",
    "href": "ae/ae-08-conditional-probability.html#goal",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "",
    "text": "Learn to calculate and interpret the probability of having a disease given a positive test result using sensitivity, specificity, and prevalence data."
  },
  {
    "objectID": "ae/ae-08-conditional-probability.html#scenario",
    "href": "ae/ae-08-conditional-probability.html#scenario",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "Scenario:",
    "text": "Scenario:\nYou are provided with the following data for COVID-19 rapid self-administered tests and population statistics from Pima County, Arizona found in Lecture 12."
  },
  {
    "objectID": "ae/ae-08-conditional-probability.html#understand-the-definitions",
    "href": "ae/ae-08-conditional-probability.html#understand-the-definitions",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "Understand the Definitions:",
    "text": "Understand the Definitions:\n\nSensitivity \\(P(T | D)\\): Probability of a positive test given the person has the disease.\nSpecificity \\(P(T^c | D^c)\\): Probability of a negative test given the person does not have the disease.\nPrevalence \\(P(D)\\): Probability that a randomly selected person has the disease.\n\n\nFormulate Bayes’ Rule:\n\\[\nP(D | T) = \\frac{P(T \\text{ and } D)}{P(T)}\n\\]\nWe know that:\n\\[\nP(T \\text{ and } D) = P(T | D) ⋅ P(D)\n\\]\nAnd:\n\\[\nP(T) = P(T | D) ⋅ P(D) + P(T | D^c) ⋅ P(D^c)\n\\]\nWhere:\n\\[\nP(T | D^c) = 1-P(T^c | D^c)\n\\]"
  },
  {
    "objectID": "ae/ae-08-conditional-probability.html#exercises",
    "href": "ae/ae-08-conditional-probability.html#exercises",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "Exercises",
    "text": "Exercises\nUsing the given data, calculate the probability that an individual has COVID-19 given a positive test result \\(P(T | D)\\).\n\nSubstitute the Given Values:\n\nSensitivity: \\(P(T | D)\\) = ___\nSpecificity: \\(P(T^c | D^c)\\) = ___\nPrevalence: \\(P(D)\\) = ___ among persons aged 10 years and older.\n\nCalculate the Complementary Probabilities\n\n\\(P(T | D^c)\\): Probability of a positive test given no disease.\n\\[P(T | D^c)=1-P(T^c | D^c)=\\]\n\\(P(D^c)\\): Probability of not having the disease.\n\\[P(D^c) = 1-P(D)\\]\n\nCalculate the Probability of a Positive Test \\(P(T)\\):\n\n\\[P(T)=\\]\n\nCalculate the Posterior Probability \\(P(D | T)\\)\n\n\\[\nP(D | T)=\\frac{P(T | D) ⋅ P(D)}{P(T | D) ⋅ P(D) + (1-P(T^c | D^c)) ⋅ (1-P(D))}\n\\]\n\\[\n=\n\\]\n\nDiscussion Questions:\n\nIs this calculation surprising?\n\nConsidering the given sensitivity, specificity, and prevalence, is the high probability of having the disease given a positive test result unexpected? Why or why not?\n\n\nAdd response here.\n\nWhat is the explanation?\n\nExplain why the probability of having the disease given a positive test result is so high. Consider the impact of sensitivity, specificity, and prevalence.\n\n\nAdd response here.\n\nWas this calculation actually reasonable to perform?\n\nDiscuss whether it is reasonable to calculate the probability of having the disease based on the given data. Are there any limitations or assumptions in this calculation?\n\n\nAdd response here.\n\nWhat if we tested in a different population, such as high-risk individuals?\n\nHow might the probability of having the disease given a positive test result change if the test was administered to a population with a higher prevalence of COVID-19?\n\n\nAdd response here.\n\nWhat if we were to test a random individual in a county where the prevalence of COVID-19 is approximately 25%?\n\nRecalculate the probability of having the disease given a positive test result for a population with a 25% prevalence of COVID-19. How does this compare to the original calculation?\n\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-11-spam-filter.html",
    "href": "ae/ae-11-spam-filter.html",
    "title": "AE 11: Building a spam filter",
    "section": "",
    "text": "In this application exercise, we will\nTo illustrate logistic regression, we will build a spam filter from email data.\nThe data come from incoming emails in David Diez’s (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012. All personally identifiable information has been removed.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nnp.random.seed(123)\n\nemail = pd.read_csv('data/email.csv')\nprint(email.info())\nprint(email.describe())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3921 entries, 0 to 3920\nData columns (total 21 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   spam          3921 non-null   int64  \n 1   to_multiple   3921 non-null   int64  \n 2   from          3921 non-null   int64  \n 3   cc            3921 non-null   int64  \n 4   sent_email    3921 non-null   int64  \n 5   time          3921 non-null   object \n 6   image         3921 non-null   int64  \n 7   attach        3921 non-null   int64  \n 8   dollar        3921 non-null   int64  \n 9   winner        3921 non-null   object \n 10  inherit       3921 non-null   int64  \n 11  viagra        3921 non-null   int64  \n 12  password      3921 non-null   int64  \n 13  num_char      3921 non-null   float64\n 14  line_breaks   3921 non-null   int64  \n 15  format        3921 non-null   int64  \n 16  re_subj       3921 non-null   int64  \n 17  exclaim_subj  3921 non-null   int64  \n 18  urgent_subj   3921 non-null   int64  \n 19  exclaim_mess  3921 non-null   int64  \n 20  number        3921 non-null   object \ndtypes: float64(1), int64(17), object(3)\nmemory usage: 643.4+ KB\nNone\n              spam  to_multiple         from           cc   sent_email  \\\ncount  3921.000000  3921.000000  3921.000000  3921.000000  3921.000000   \nmean      0.093599     0.158123     0.999235     0.404489     0.277990   \nstd       0.291307     0.364903     0.027654     2.666424     0.448066   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     1.000000     0.000000     0.000000   \n50%       0.000000     0.000000     1.000000     0.000000     0.000000   \n75%       0.000000     0.000000     1.000000     0.000000     1.000000   \nmax       1.000000     1.000000     1.000000    68.000000     1.000000   \n\n             image       attach       dollar      inherit       viagra  \\\ncount  3921.000000  3921.000000  3921.000000  3921.000000  3921.000000   \nmean      0.048457     0.132874     1.467228     0.038001     0.002040   \nstd       0.450848     0.718518     5.022298     0.267899     0.127759   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n75%       0.000000     0.000000     0.000000     0.000000     0.000000   \nmax      20.000000    21.000000    64.000000     9.000000     8.000000   \n\n          password     num_char  line_breaks       format      re_subj  \\\ncount  3921.000000  3921.000000  3921.000000  3921.000000  3921.000000   \nmean      0.108136    10.706586   230.658505     0.695231     0.261413   \nstd       0.959931    14.645786   319.304959     0.460368     0.439460   \nmin       0.000000     0.001000     1.000000     0.000000     0.000000   \n25%       0.000000     1.459000    34.000000     0.000000     0.000000   \n50%       0.000000     5.856000   119.000000     1.000000     0.000000   \n75%       0.000000    14.084000   298.000000     1.000000     1.000000   \nmax      28.000000   190.087000  4022.000000     1.000000     1.000000   \n\n       exclaim_subj  urgent_subj  exclaim_mess  \ncount   3921.000000  3921.000000   3921.000000  \nmean       0.080337     0.001785      6.584290  \nstd        0.271848     0.042220     51.479871  \nmin        0.000000     0.000000      0.000000  \n25%        0.000000     0.000000      0.000000  \n50%        0.000000     0.000000      1.000000  \n75%        0.000000     0.000000      4.000000  \nmax        1.000000     1.000000   1236.000000\nThe variables we’ll use in this analysis are\nGoal: Use the number of exclamation points in an email to predict whether or not it is spam."
  },
  {
    "objectID": "ae/ae-11-spam-filter.html#exercise-1",
    "href": "ae/ae-11-spam-filter.html#exercise-1",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nFit the logistic regression model using the number of exclamation points to predict the probability an email is spam.\n\n\nX = email[['exclaim_mess']]\ny = email['spam']\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\n# Summary output\nprint(f\"Intercept: {log_reg.intercept_[0]}\")\nprint(f\"Coefficient: {log_reg.coef_[0][0]}\")\n\nIntercept: -2.2723427075496767\nCoefficient: 0.00027240944041455475\n\n\n\nHow does the code above differ from previous code we’ve used to fit regression models? Compare your summary output to the estimated model below.\n\n\\[\\log\\Big(\\frac{p}{1-p}\\Big) = -2.27 - 0.000272 \\times exclaim\\_mess\\]\nAdd response here."
  },
  {
    "objectID": "ae/ae-11-spam-filter.html#exercise-2",
    "href": "ae/ae-11-spam-filter.html#exercise-2",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the probability the email is spam if it contains 10 exclamation points? Answer the question using the log_reg.predict_proba() function.\n\nemail_10_exclaim = np.array([[10]])\npred_prob = log_reg.predict_proba(email_10_exclaim)[0][1]\nprint(f\"Predicted probability of spam for an email with 10 exclamation points: {pred_prob}\")\n\nPredicted probability of spam for an email with 10 exclamation points: 0.0936705855941035"
  },
  {
    "objectID": "ae/ae-11-spam-filter.html#exercise-3",
    "href": "ae/ae-11-spam-filter.html#exercise-3",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have the probability an email is spam, but ultimately we want to use the probability to classify an email as spam or not spam. Therefore, we need to set a decision-making threshold, such that an email is classified as spam if the predicted probability is greater than the threshold and not spam otherwise.\nSuppose you are a data scientist working on a spam filter. You must determine how high the predicted probability must be before you think it would be reasonable to call it spam and put it in the junk folder (which the user is unlikely to check).\nWhat are some trade offs you would consider as you set the decision-making threshold?\nAdd response here.\n\nemail['pred_class'] = (log_reg.predict_proba(X)[:, 1] &gt;= 0.5).astype(int)\n\nplt.figure(figsize=(8, 6))\nsns.stripplot(data=email, x='exclaim_mess', y=email['spam'].astype(str), hue='pred_class', palette='colorblind', jitter=True, alpha=0.5)\nplt.xlabel('Number of Exclamation Marks')\nplt.ylabel('Spam (0 or 1)')\nplt.title('Logistic Regression Model Predictions with Jitter')\nplt.show()"
  },
  {
    "objectID": "ae/ae-11-spam-filter.html#exercise-4",
    "href": "ae/ae-11-spam-filter.html#exercise-4",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model with all variables in the dataset as predictors and recreate the visualization above for this model.\n\n# add code here\n\n\nUse model evaluation metrics from confusion_matrix and classification_report to evaluate our model.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html",
    "href": "ae/ae-12-candy-ranking-A.html",
    "title": "AE 12: Ultimate candy ranking",
    "section": "",
    "text": "Load the Penguins Dataset: Import and explore the dataset to understand its structure and the features available for analysis.\nPreprocess the Data: Clean the data by handling missing values and standardize the numerical features for PCA.\nPerform PCA: Apply Principal Component Analysis to reduce the dimensionality of the data and extract the principal components.\nVisualize the PCA Result: Create a scatter plot of the principal components to visualize the clustering of different penguin species.\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-1",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-1",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 1",
    "text": "Exercise 1\nCreate the full model and show the \\(R^2_{adj}\\):\n\nfull_model = smf.ols('winpercent ~ chocolate * sugarpercent + fruity * pricepercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nprint(f'Adjusted R-squared: {full_model.rsquared_adj}')\n\nAdjusted R-squared: 0.42530696026839643\n\n\nIs the model a good fit of the data?\nThe model moderately fits the data (42.5% variation explained)."
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-2",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-2",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 2",
    "text": "Exercise 2\nProduce all possible models removing 1 term at a time from the full model. Describe what is being removed above each code cell.\n\n# Blank dictionary to store new models\nmodels = {}\n\n\nRemove chocolate and it’s associated interaction\n\n\nmodel1 = smf.ols('winpercent ~ fruity * pricepercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model1'] = model1.rsquared_adj\n\n\nRemove fruity and its associated interaction\n\n\nmodel2 = smf.ols('winpercent ~ chocolate * sugarpercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model2'] = model2.rsquared_adj\n\n\nRemove nougat\n\n\nmodel3 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model3'] = model3.rsquared_adj\n\n\nRemove pricepercent and its associated interactions\n\n\nmodel4 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + nougat + sugarpercent', data=candy_rankings).fit()\nmodels['model4'] = model4.rsquared_adj\n\n\nRemove sugarpercent*chocolate\n\n\nmodel5 = smf.ols('winpercent ~ chocolate + fruity * pricepercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model5'] = model5.rsquared_adj\n\n\nRemove pricepercent*fruity\n\n\nmodel6 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model6'] = model6.rsquared_adj"
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-3",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-3",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompare all models using the framework best_model_step1 = max(models, key=models.get):\n\nbest_model_step1 = max(models, key=models.get)\nprint(f'Best model in Exercise 2: {best_model_step1} with Adjusted R-squared: {models[best_model_step1]}')\n\nBest model in Exercise 2: model3 with Adjusted R-squared: 0.4318254523153029\n\n\n\nWhich model is best:\n\nOf the models from Exercises 1 and 2, the model with the highest adjusted \\(R^2\\) is the one with nougat removed. Therefore, we will go to Exercise 4 eliminating one variable at a time from that model."
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-4",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-4",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate all possible models removing 1 term at a time from the model selected in the previous exercise. Again, describe what is being removed above each code cell.\n\n# Blank dictionary to store new models\nmodels = {}\n\n\nRemove chocolate and its associated interactions\n\n\nmodel7 = smf.ols('winpercent ~ fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model7'] = model7.rsquared_adj\n\n\nRemove fruity and its associated interactions\n\n\nmodel8 = smf.ols('winpercent ~ chocolate * sugarpercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model8'] = model8.rsquared_adj\n\n\nRemove pricepercent and its associated interactions\n\n\nmodel9 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + sugarpercent', data=candy_rankings).fit()\nmodels['model9'] = model9.rsquared_adj\n\n\nRemove sugarpercent*chocolate\n\n\nmodel10 = smf.ols('winpercent ~ chocolate + fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model10'] = model10.rsquared_adj\n\n\nRemove pricepercent*fruity\n\n\nmodel11 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model11'] = model11.rsquared_adj"
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-5",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-5",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 5",
    "text": "Exercise 5\nCompare all models using the framework best_model_step2 = max(models, key=models.get):\n\nbest_model_step2 = max(models, key=models.get)\nprint(f'Best model in Exercise 4: {best_model_step2} with Adjusted R-squared: {models[best_model_step2]}')\n\nBest model in Exercise 4: model10 with Adjusted R-squared: 0.43488989967424574\n\n\n\nWhich model is best:\n\nThe model with sugarpercent*chocolate has the highest \\(R^2\\) of all the models we’ve tested so far, so we will now go to Exercise 6 eliminating one variable at a time from this model."
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-6",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-6",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate all possible models removing 1 term at a time from the model selected in the previous step. Again, describe what is being removed above each code cell.\n\n# Blank dictionary to store new models\nmodels = {}\n\n\nRemove chocolate\n\n\nmodel12 = smf.ols('winpercent ~ fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model12'] = model12.rsquared_adj\n\n\nRemove fruity and its associated interactions\n\n\nmodel13 = smf.ols('winpercent ~ chocolate * sugarpercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model13'] = model13.rsquared_adj\n\n\nRemove sugarpercent\n\n\nmodel14 = smf.ols('winpercent ~ chocolate + fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model14'] = model14.rsquared_adj\n\n\nRemove pricepercent and its associated interactions\n\n\nmodel15 = smf.ols('winpercent ~ chocolate + fruity + sugarpercent', data=candy_rankings).fit()\nmodels['model15'] = model15.rsquared_adj\n\n\nRemove pricepercent*fruity\n\n\nmodel16 = smf.ols('winpercent ~ chocolate + fruity + sugarpercent + pricepercent', data=candy_rankings).fit()\nmodels['model16'] = model16.rsquared_adj"
  },
  {
    "objectID": "ae/ae-12-candy-ranking-A.html#exercise-7",
    "href": "ae/ae-12-candy-ranking-A.html#exercise-7",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 7",
    "text": "Exercise 7\nCompare all models using the framework best_model_step3 = max(models, key=models.get):\n\nbest_model_step3 = max(models, key=models.get)\nprint(f'Best model in Exercise 6: {best_model_step3} with Adjusted R-squared: {models[best_model_step3]}')\n\nBest model in Exercise 6: model14 with Adjusted R-squared: 0.43488989967424574\n\n\n\nWhich model is best:\n\nNone of the models in Exercise 6 resulted in a higher adjusted \\(R^2_{adj}\\). Therefore our final model is the one selected in Exercise 5.\n\nselected_model = smf.ols('winpercent ~ chocolate + fruity + sugarpercent + pricepercent + pricepercent*fruity', data=candy_rankings).fit()\nprint(selected_model.summary())\n\ncoefficients = selected_model.params\nprint(coefficients)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             winpercent   R-squared:                       0.469\nModel:                            OLS   Adj. R-squared:                  0.435\nMethod:                 Least Squares   F-statistic:                     13.93\nDate:                Mon, 19 Aug 2024   Prob (F-statistic):           9.38e-10\nTime:                        13:38:21   Log-Likelihood:                -321.79\nNo. Observations:                  85   AIC:                             655.6\nDf Residuals:                      79   BIC:                             670.2\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                      31.4753      4.271      7.369      0.000      22.974      39.977\nchocolate[T.True]              20.7720      3.923      5.295      0.000      12.964      28.580\nfruity[T.True]                 11.8091      5.130      2.302      0.024       1.598      22.020\nsugarpercent                    8.1016      4.560      1.777      0.079      -0.974      17.177\npricepercent                    6.8970      6.770      1.019      0.311      -6.579      20.373\npricepercent:fruity[T.True]   -17.4218      9.943     -1.752      0.084     -37.213       2.369\n==============================================================================\nOmnibus:                        0.714   Durbin-Watson:                   1.634\nProb(Omnibus):                  0.700   Jarque-Bera (JB):                0.808\nSkew:                           0.201   Prob(JB):                        0.668\nKurtosis:                       2.744   Cond. No.                         13.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nIntercept                      31.475334\nchocolate[T.True]              20.772023\nfruity[T.True]                 11.809087\nsugarpercent                    8.101581\npricepercent                    6.897010\npricepercent:fruity[T.True]   -17.421838\ndtype: float64"
  },
  {
    "objectID": "ae/ae-14-integration.html",
    "href": "ae/ae-14-integration.html",
    "title": "AE 14: Integration",
    "section": "",
    "text": "In this exercise, we will:"
  },
  {
    "objectID": "ae/ae-14-integration.html#exercise-1",
    "href": "ae/ae-14-integration.html#exercise-1",
    "title": "AE 14: Integration",
    "section": "Exercise 1",
    "text": "Exercise 1\nFunction:\n\\[\nf(x) = 5x^3\n\\]\nSolution:\n\nThe power rule for integration states that \\(\\int x^n \\space dx = \\frac{x^{n+1}}{n+1}+C\\)\nApplying the power rule:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-14-integration.html#exercise-2",
    "href": "ae/ae-14-integration.html#exercise-2",
    "title": "AE 14: Integration",
    "section": "Exercise 2",
    "text": "Exercise 2\nFunction:\n\\[\ng(x) = \\sqrt{x}\n\\]\nSolution:\n\nRewrite the function with a fractional exponent: add response here.\nApply the power rule:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-14-integration.html#exercise-3",
    "href": "ae/ae-14-integration.html#exercise-3",
    "title": "AE 14: Integration",
    "section": "Exercise 3",
    "text": "Exercise 3\nFunction:\n\\[\nh(x)=\\ln(x)\n\\]\nSolution:\n\nUse the integral of the natural logarithm function:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-14-integration.html#exercise-4",
    "href": "ae/ae-14-integration.html#exercise-4",
    "title": "AE 14: Integration",
    "section": "Exercise 4",
    "text": "Exercise 4\nFunction:\n\\[\n\\int xe^x \\space dx\n\\]\nSolution\n\nIdentify the parts: Let \\(u=x\\) and \\(dv=e^x \\space dx\\)\nDifferentiate and integrate:\n\nDifferentiate: add response here.\nIntegrate: add response here.\n\nApply the integration by parts formula: \\(\\int u \\space dv = uv - \\int v \\space du\\)\n\nadd response here.\n\nSimplify the integral:\n\nadd response here.\n\nFinal answer:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-14-integration.html#exercise-5",
    "href": "ae/ae-14-integration.html#exercise-5",
    "title": "AE 14: Integration",
    "section": "Exercise 5",
    "text": "Exercise 5\nFunction:\n\\[\n\\int x \\ln(x) \\space dx\n\\]\nSolution:\n\nIdentify the parts: ___ and ___\nDifferentiate and integrate:\n\nDifferentiate: add response here.\nIntegrate: add response here.\n\nApply the integration by parts formula: \\(\\int u \\space dv = uv - \\int v \\space du\\)\nSubstitute the parts:\n\nadd response here.\n\nSimplify the integral:\n\nadd response here.\n\nIntegrate the remaining part:\n\nadd response here.\n\nCombine the results:\n\nadd response here.\n\nFinal answer:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-14-integration.html#exercise-6",
    "href": "ae/ae-14-integration.html#exercise-6",
    "title": "AE 14: Integration",
    "section": "Exercise 6",
    "text": "Exercise 6\nFunction:\n\\[\n\\int x \\sin (x^2) \\space dx\n\\]\nSolution:\n\nIdentify the substitution: Let ___\nDifferentiate and solve for ___:\n\nadd response here.\nadd response here.\n\nSubstitute into the integral\n\nadd response here.\n\nIntegrate:\n\nadd response here.\n\nSubstitute back into the original variable:\n\nadd response here.\n\nFinal answer\n\nadd response here."
  },
  {
    "objectID": "ae/ae-07-probability-A.html",
    "href": "ae/ae-07-probability-A.html",
    "title": "AE 07: Practicing with probabilities",
    "section": "",
    "text": "Understand and calculate basic probabilities using a real-world dataset."
  },
  {
    "objectID": "ae/ae-07-probability-A.html#goal",
    "href": "ae/ae-07-probability-A.html#goal",
    "title": "AE 07: Practicing with probabilities",
    "section": "",
    "text": "Understand and calculate basic probabilities using a real-world dataset."
  },
  {
    "objectID": "ae/ae-07-probability-A.html#data",
    "href": "ae/ae-07-probability-A.html#data",
    "title": "AE 07: Practicing with probabilities",
    "section": "Data",
    "text": "Data\nA cohort study on coffee consumption and mortality from:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid not die\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601\n\n\n\nSource: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5788283/"
  },
  {
    "objectID": "ae/ae-07-probability-A.html#definitions",
    "href": "ae/ae-07-probability-A.html#definitions",
    "title": "AE 07: Practicing with probabilities",
    "section": "Definitions:",
    "text": "Definitions:\n\nEvent A: The person died.\nEvent B: The person is a non-coffee drinker."
  },
  {
    "objectID": "ae/ae-07-probability-A.html#two-important-rules",
    "href": "ae/ae-07-probability-A.html#two-important-rules",
    "title": "AE 07: Practicing with probabilities",
    "section": "Two important rules",
    "text": "Two important rules\nSuppose we have events \\(A\\) and \\(B\\), with probabilities \\(P(A)\\) and \\(P(B)\\) of occurring. Based on the Kolmogorov axioms:\n\nComplement Rule: \\(P(A^c) = 1 - P(A)\\)\nInclusion-Exclusion: \\(P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\\)"
  },
  {
    "objectID": "ae/ae-07-probability-A.html#exercise",
    "href": "ae/ae-07-probability-A.html#exercise",
    "title": "AE 07: Practicing with probabilities",
    "section": "Exercise",
    "text": "Exercise\nCalculate the following probabilities for a randomly selected person in the cohort:\n\nTotal number of people in the cohort.\n\n\\(Total=5438+1039+29712+4440+24934+3601=69604\\)\n\n\\(\\small{P(A)}\\): Probability that the person died.\n\n\\(\\small{P(A)} = \\frac{1039+4440+3601}{69604}=\\frac{9080}{69604}\\approx0.1305\\)\n\n\\(\\small{P(B)}\\): Probability that the person is a non-coffee drinker.\n\n\\(\\small{P(B)} = \\frac{5438+1039}{69604} = \\frac{6477}{69604} \\approx 0.0931\\)\n\n\\(\\small{P(A \\text{ and } B)}\\): Probability that the person died and is a non-coffee drinker.\n\n\\(\\small{P(A \\cap B)}=\\frac{1039}{69604} \\approx 0.0149\\)\n\n\\(\\small{P(A \\text{ or } B)}\\): Probability that the person died or is a non-coffee drinker.\n\n\\(\\small{P(A \\cup B)}=\\small{P(A)}+\\small{P(B)}-\\small{P(A \\cap B)}=0.1305+0.0931-0.014\\)\n\n\\(\\small{P(A \\text{ or } B^c)}\\): Probability that the person died or is not a non-coffee drinker.\n\n\\(\\small{P(A \\cup B)}=\\small{P(A)}+\\small{P(B)}-\\small{P(A \\cap B)}\\)\nFirst, calculate \\(\\small{P(B^c)}\\):\n\\(\\small{P(B^c)}=1-\\small{P(B)}=1-0.0931=0.9069\\)\nThen calculate \\(\\small{P(A \\cap B^c)}\\) (people who dies and drink coffee occasionally or regularly):\n\\(\\small{P(A \\cap B^c)}=\\frac{4440+3601}{69604}=\\frac{8041}{69604}\\approx0.1155\\)\nFinally, calculate \\(\\small{P(A \\text{ or } B^c)}\\):\n\\(\\small{P(A \\cup B^c)}=0.1305+0.9069-0.1155 \\approx 0.9219\\)"
  },
  {
    "objectID": "ae/ae-07-probability-A.html#discussion-questions",
    "href": "ae/ae-07-probability-A.html#discussion-questions",
    "title": "AE 07: Practicing with probabilities",
    "section": "Discussion Questions:",
    "text": "Discussion Questions:\n\nWhat do these probabilities tell us about the relationship between coffee consumption and mortality in this cohort?\n\n\nThe probabilities calculated provide insights into the relationship between coffee consumption and mortality:\n\n\\(\\small{P(A)}\\approx0.1305\\): About 13.05% of the people in the cohort died.\n\\(\\small{P(B)} \\approx 0.0931\\): About 9.31% of the people in the cohort do not drink coffee.\n\\(\\small{P(A \\text{ and } B)} \\approx 0.0149\\): About 1.49% of the people in the cohort both died and do not drink coffee.\n\\(\\small{P(A \\text{ or } B)} \\approx 0.2087\\): About 20.87% of the people in the cohort either died or do not drink coffee.\n\\(\\small{P(A \\text{ or } B^c)} \\approx 0.9219\\): About 92.19% of the people in the cohort either died or drink coffee.\n\nFrom these probabilities, we can observe that the proportion of people who died (P(A)) is higher among those who drink coffee compared to non-coffee drinkers. However, the data does not immediately suggest a direct causative effect of coffee consumption on mortality. Rather, it describes the distribution of mortality across different coffee consumption groups.\n\n\nAre there any limitations to interpreting these probabilities as causal effects?\n\n\nYes, there are several limitations to interpreting these probabilities as causal effects:\n\nConfounding Factors: The relationship between coffee consumption and mortality might be influenced by other variables (e.g., age, health status, lifestyle choices) that were not accounted for in this dataset.\nCorrelation vs. Causation: These probabilities show an association, but they do not prove that coffee consumption causes changes in mortality rates. There might be underlying factors that drive both coffee consumption habits and mortality.\nSampling Bias: If the cohort is not representative of the general population, the results might not be generalizable.\nData Quality: The accuracy and reliability of the data collected on coffee consumption and mortality could affect the outcomes. Misreporting or measurement errors can lead to biased probabilities.\n\n\n\nHow might additional factors or confounders influence these probabilities?\n\n\nAdditional factors or confounders could significantly influence these probabilities:\nAge: Older individuals might have different coffee consumption patterns and mortality risks compared to younger individuals.\nHealth Status: Individuals with pre-existing health conditions might be more likely to avoid coffee and have higher mortality rates.\nLifestyle Choices: Other lifestyle factors such as diet, exercise, smoking, and alcohol consumption could affect both coffee drinking habits and mortality rates.\nSocioeconomic Status: Income, education, and access to healthcare can influence both coffee consumption and overall health outcomes.\n\nTo better understand the relationship between coffee consumption and mortality, it would be necessary to control for these confounders through statistical methods such as multivariate analysis or propensity score matching. This would help isolate the effect of coffee consumption from other influencing factors."
  },
  {
    "objectID": "ae/ae-16-pca-A.html",
    "href": "ae/ae-16-pca-A.html",
    "title": "AE 16: Principal component analysis",
    "section": "",
    "text": "In this application exercise, we will:"
  },
  {
    "objectID": "ae/ae-16-pca-A.html#exercise-1",
    "href": "ae/ae-16-pca-A.html#exercise-1",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nWatch this video on Principal Component Analysis:\n\n\n\nWhat were three takeaways from this video? Include how you think linear algebra contributes to PCA:\n\nAnswer will vary."
  },
  {
    "objectID": "ae/ae-16-pca-A.html#packages",
    "href": "ae/ae-16-pca-A.html#packages",
    "title": "AE 16: Principal component analysis",
    "section": "Packages",
    "text": "Packages\nWe will primarily use the seaborn and sklearn packages.\n\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ae/ae-16-pca-A.html#exercise-2",
    "href": "ae/ae-16-pca-A.html#exercise-2",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nLoad the Penguins Dataset using seaborn\n\nimport seaborn as sns\nimport pandas as pd\n\npenguins = sns.load_dataset('penguins')\n\nprint(penguins.head())\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female"
  },
  {
    "objectID": "ae/ae-16-pca-A.html#exercise-3",
    "href": "ae/ae-16-pca-A.html#exercise-3",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 3",
    "text": "Exercise 3\nPreprocess the data\nWe need to handle missing values and select the numerical features for PCA.\n\npenguins.dropna(inplace=True)\n\nfeatures = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\nX = penguins[features]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"
  },
  {
    "objectID": "ae/ae-16-pca-A.html#exercise-4",
    "href": "ae/ae-16-pca-A.html#exercise-4",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 4",
    "text": "Exercise 4\nPerform PCA\nUse PCA from sklearn to reduce the dimensionality of the data. Hint: use two principal components\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\nprincipal_components = pca.fit_transform(X_scaled)\n\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n\npca_df['species'] = penguins['species']"
  },
  {
    "objectID": "ae/ae-16-pca-A.html#exercise-5",
    "href": "ae/ae-16-pca-A.html#exercise-5",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 5",
    "text": "Exercise 5\nVisualize the PCA Result\nUse seaborn to visualize the principal components.\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='species', palette='colorblind')\nplt.title('PCA of Penguins Dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()"
  },
  {
    "objectID": "ae/ae-15-linear-algebra.html",
    "href": "ae/ae-15-linear-algebra.html",
    "title": "AE 15: Linear algebra",
    "section": "",
    "text": "In this exercise, we will:"
  },
  {
    "objectID": "ae/ae-15-linear-algebra.html#transposition",
    "href": "ae/ae-15-linear-algebra.html#transposition",
    "title": "AE 15: Linear algebra",
    "section": "Transposition",
    "text": "Transposition\n\nExercise 1\nGiven a vector \\(\\mathbf{y}\\):\n\\[\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix}\n\\]\nWrite down its transpose \\(\\mathbf{y}^\\top\\)\nSolution:\n\nThe transpose of the vector \\(\\mathbf{y}\\) is:\n\nadd response here.\n\n\nExercise 2\nGiven the following matrix \\(\\mathbf{N}\\):\n\\[\n\\mathbf{N} = \\begin{bmatrix}n_{11} & n_{12} \\\\n_{21} & n_{22} \\\\n_{31} & n_{32}\\end{bmatrix}\n\\]\nWrite down its transpose, \\(\\mathbf{N}^{\\top}\\)\nSolution:\n\nThe transpose of the matrix \\(\\mathbf{N}\\) is:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-15-linear-algebra.html#matrix-operations",
    "href": "ae/ae-15-linear-algebra.html#matrix-operations",
    "title": "AE 15: Linear algebra",
    "section": "Matrix operations",
    "text": "Matrix operations\n\nExercise 3\nConsider the following matrices \\(\\mathbf{C}\\) and \\(\\mathbf{D}\\):\n\\[\n\\mathbf{C} = \\begin{bmatrix}c_{11} & c_{12} & c_{13} \\\\c_{21} & c_{22} & c_{23}\\end{bmatrix}, \\quad\\mathbf{D} = \\begin{bmatrix}d_{11} & d_{12} \\\\d_{21} & d_{22} \\\\d_{31} & d_{32}\\end{bmatrix}\n\\]\n\nWhat are the dimensions of \\(\\mathbf{C}\\)?\nWhat are the dimensions of \\(\\mathbf{D}\\)?\nFor the matrix product \\(\\mathbf{C} \\mathbf{D}\\):\n\nDetermine if the product is valid, and explain why.\nIf the product is valid, write down the dimensions of the resulting matrix without computing the product.\n\n\nSolution:\nadd response here.\n\n\nExercise 4\nConsider the following matrices \\(\\mathbf{E}\\) and \\(\\mathbf{F}\\):\n\\[\n\\mathbf{E} = \\begin{bmatrix}e_{11} & e_{12} & e_{13} & e_{14} \\\\e_{21} & e_{22} & e_{23} & e_{24}\\end{bmatrix}, \\quad\\mathbf{F} = \\begin{bmatrix}f_{11} & f_{12} \\\\f_{21} & f_{22} \\\\f_{31} & f_{32} \\\\f_{41} & f_{42}\\end{bmatrix}\n\\]\n\nWhat are the dimensions of \\(\\mathbf{E}\\)?\nWhat are the dimensions of \\(\\mathbf{F}\\)?\nFor the matrix product \\(\\mathbf{E} \\mathbf{F}\\):\n\nDetermine if the product is valid, and explain why.\nIf the product is valid, write down the dimensions of the resulting matrix without computing the product.\n\n\nSolution:\nadd response here."
  },
  {
    "objectID": "ae/ae-08-conditional-probability-A.html",
    "href": "ae/ae-08-conditional-probability-A.html",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "",
    "text": "Learn to calculate and interpret the probability of having a disease given a positive test result using sensitivity, specificity, and prevalence data."
  },
  {
    "objectID": "ae/ae-08-conditional-probability-A.html#goal",
    "href": "ae/ae-08-conditional-probability-A.html#goal",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "",
    "text": "Learn to calculate and interpret the probability of having a disease given a positive test result using sensitivity, specificity, and prevalence data."
  },
  {
    "objectID": "ae/ae-08-conditional-probability-A.html#scenario",
    "href": "ae/ae-08-conditional-probability-A.html#scenario",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "Scenario:",
    "text": "Scenario:\nYou are provided with the following data for COVID-19 rapid self-administered tests and population statistics from Pima County, Arizona found in Lecture 12."
  },
  {
    "objectID": "ae/ae-08-conditional-probability-A.html#understand-the-definitions",
    "href": "ae/ae-08-conditional-probability-A.html#understand-the-definitions",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "Understand the Definitions:",
    "text": "Understand the Definitions:\n\nSensitivity \\(P(T | D)\\): Probability of a positive test given the person has the disease.\nSpecificity \\(P(T^c | D^c)\\): Probability of a negative test given the person does not have the disease.\nPrevalence \\(P(D)\\): Probability that a randomly selected person has the disease.\n\n\nFormulate Bayes’ Rule:\n\\[\nP(D | T) = \\frac{P(T \\text{ and } D)}{P(T)}\n\\]\nWe know that:\n\\[\nP(T \\text{ and } D) = P(T | D) ⋅ P(D)\n\\]\nAnd:\n\\[\nP(T) = P(T | D) ⋅ P(D) + P(T | D^c) ⋅ P(D^c)\n\\]\nWhere:\n\\[\nP(T | D^c) = 1-P(T^c | D^c)\n\\]"
  },
  {
    "objectID": "ae/ae-08-conditional-probability-A.html#exercises",
    "href": "ae/ae-08-conditional-probability-A.html#exercises",
    "title": "AE 08: Understanding Probabilities with COVID-19 Rapid Self-Administered Tests",
    "section": "Exercises",
    "text": "Exercises\nUsing the given data, calculate the probability that an individual has COVID-19 given a positive test result \\(P(T | D)\\).\n\nSubstitute the Given Values:\n\nSensitivity: \\(P(T | D)\\) = 0.087\nSpecificity: \\(P(T^c | D^c)\\) = 0.642\nPrevalence: \\(P(D)\\) = 0.998. among persons aged 10 years and older.\n\nCalculate the Complementary Probabilities\n\n\\(P(T | D^c)\\): Probability of a positive test given no disease.\n\\[P(T | D^c)=1-P(T^c | D^c)\\]\n\\(1 - 0.998 = 0.002\\)\n\\(P(D^c)\\): Probability of not having the disease.\n\\[P(D^c) = 1-P(D)\\]\n\\(1 - 0.087 = 0.913\\)\n\nCalculate the Probability of a Positive Test \\(P(T)\\):\n\n\\[P(T)=P(T | D)⋅ P(D) + P(T | D^c) ⋅ P(D^c)\\]\n\\(P(T)=(0.642×0.087)+(0.002×0.913)\\) \\(P(T)=0.055854+0.001826=0.05768\\)\n\nCalculate the Posterior Probability \\(P(D | T)\\)\n\n\\[\nP(D | T)=\\frac{P(T | D) ⋅ P(D)}{P(T | D) ⋅ P(D) + (1-P(T^c | D^c)) ⋅ (1-P(D))}\n\\]\n\\(P(D∣T)=\\frac{0.05768}{0.055854}​≈0.968\\)\n\nDiscussion Questions:\n\nIs this calculation surprising?\n\nConsidering the given sensitivity, specificity, and prevalence, is the high probability of having the disease given a positive test result unexpected? Why or why not?\n\n\n\nNo, given the high specificity, false positives are minimal, so a positive result is likely accurate.\n\n\nWhat is the explanation?\n\nExplain why the probability of having the disease given a positive test result is so high. Consider the impact of sensitivity, specificity, and prevalence.\n\n\n\nThe combination of high specificity and moderate sensitivity ensures that the test reliably rules out non-disease cases, contributing to the high posterior probability.\n\n\nWas this calculation actually reasonable to perform?\n\nDiscuss whether it is reasonable to calculate the probability of having the disease based on the given data. Are there any limitations or assumptions in this calculation?\n\n\n\nYes, but assumptions such as perfect accuracy of prevalence data and no external biases limit real-world applicability.\n\n\nWhat if we tested in a different population, such as high-risk individuals?\n\nHow might the probability of having the disease given a positive test result change if the test was administered to a population with a higher prevalence of COVID-19?\n\n\n\nThe posterior probability would increase with higher prevalence.\n\n\nWhat if we were to test a random individual in a county where the prevalence of COVID-19 is approximately 25%?\n\nRecalculate the probability of having the disease given a positive test result for a population with a 25% prevalence of COVID-19. How does this compare to the original calculation?\n\n\n\nIf prevalence = 25%:\n\n\\(P(T) = (0.642 \\times 0.25) + (0.002 \\times 0.75) = 0.162\\)\n\\(P(D | T) = \\frac{0.1605}{0.162} \\approx 0.991​\\)"
  },
  {
    "objectID": "ae/ae-05-majors-wrangling.html",
    "href": "ae/ae-05-majors-wrangling.html",
    "title": "AE 05: Wrangling College Majors",
    "section": "",
    "text": "Our ultimate goal in this application exercise is to make the following data visualization."
  },
  {
    "objectID": "ae/ae-05-majors-wrangling.html#goal",
    "href": "ae/ae-05-majors-wrangling.html#goal",
    "title": "AE 05: Wrangling College Majors",
    "section": "",
    "text": "Our ultimate goal in this application exercise is to make the following data visualization."
  },
  {
    "objectID": "ae/ae-05-majors-wrangling.html#data",
    "href": "ae/ae-05-majors-wrangling.html#data",
    "title": "AE 05: Wrangling College Majors",
    "section": "Data",
    "text": "Data\nFor this exercise you will work with data on the proportions of Bachelor’s degrees awarded in the US between 2005 and 2015. The dataset you will use is in your data/ folder and it’s called degrees.csv.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\nimport seaborn as sns\n\ndegrees = pd.read_csv(\"data/degrees.csv\")\n\nAnd let’s take a look at the data.\n\ndegrees.head()\n\n\n\n\n\n\n\n\ndegree\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nInformation Science & eSociety (BA)\n63.0\n61.0\n67.0\n71\n38\n\n\n1\nInformation Science (BS)\nNaN\nNaN\nNaN\n16\n57\n\n\n2\nInformation (PhD)\n2.0\n3.0\n1.0\n1\n1\n\n\n3\nLibrary & Information Science (MA)\n47.0\n57.0\n72.0\n42\n58\n\n\n4\nInformation (MS)\n8.0\n10.0\n13.0\n5\n2"
  },
  {
    "objectID": "ae/ae-05-majors-wrangling.html#pivoting",
    "href": "ae/ae-05-majors-wrangling.html#pivoting",
    "title": "AE 05: Wrangling College Majors",
    "section": "Pivoting",
    "text": "Pivoting\n\nPivot the degrees data frame longer such that each row represents a degree type / year combination and year and number of graduates for that year are columns in the data frame.\n\n\n# add your code here\n\n\nQuestion: What is the type of the year variable? Why? What should it be?\n\nAdd your response here.\n\nStart over with pivoting, and this time also make sure year is a numerical variable in the resulting data frame.\n\n\n# add your code here\n\n\nQuestion: What would an NA mean in this context? Hint: The data come from the university registrars, and they have records on every single graduates, there shouldn’t be anything “unknown” to them about who graduated when.\n\nAdd your response here.\n\nAdd on to your pipeline that you started with pivoting and convert NAs in n to 0s.\n\n\n# add your code here\n\n\nIn our plot the degree types are BA, BS, MA, MS, and PhD. This information is in our dataset, in the degree column, but this column also has additional characters we don’t need. Create a new column called degree_type with levels BA, BS, MA, MS, and PhD (in this order) based on degree. Do this by adding on to your pipeline from earlier.\n\n\n# add your code here\n\n\nNow we start making our plot, but let’s not get too fancy right away. Create the following plot, which will serve as the “first draft” on the way to our Goal. Do this by adding on to your pipeline from earlier.\nHint: Make sure to state ci=None when using sns.lineplot()\n\n\n\n\n\n\n\n# add your code here\n\n\nWhat aspects of the plot need to be updated to go from the draft you created above to the Goal plot at the beginning of this application exercise.\n\nAdd your response here.\n\nUpdate x-axis scale such that the years displayed go from 2019 to 2023 in unique years. Do this by adding on to your pipeline from earlier.\n\n\n# add your code here\n\n\nUpdate line colors using the following level / color assignments. Once again, do this by adding on to your pipeline from earlier.\n\nBA: “#53868B”\nBS: “#7AC5CD”\nMA: “#89a285”\nMS: “#8B814C”\nPhD: “#CDBE70”\n\n\n\n# add your code here\n\n\nUpdate the plot labels (title, x, and y) and use sns.set_style(\"white_grid\"). Once again, do this by adding on to your pipeline from earlier.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-14-integration-A.html",
    "href": "ae/ae-14-integration-A.html",
    "title": "AE 14: Integration",
    "section": "",
    "text": "In this exercise, we will:"
  },
  {
    "objectID": "ae/ae-14-integration-A.html#exercise-1",
    "href": "ae/ae-14-integration-A.html#exercise-1",
    "title": "AE 14: Integration",
    "section": "Exercise 1",
    "text": "Exercise 1\nFunction:\n\\[\nf(x) = 5x^3\n\\]\nSolution:\n\nThe power rule for integration states that \\(\\int x^n \\space dx = \\frac{x^{n+1}}{n+1}+C\\)\nApplying the power rule:\n\n\\[\n\\int 5x^3 \\, dx = 5 \\cdot \\frac{x^{3+1}}{3+1} + C = \\frac{5x^4}{4} + C\n\\]"
  },
  {
    "objectID": "ae/ae-14-integration-A.html#exercise-2",
    "href": "ae/ae-14-integration-A.html#exercise-2",
    "title": "AE 14: Integration",
    "section": "Exercise 2",
    "text": "Exercise 2\nFunction:\n\\[\ng(x) = \\sqrt{x}\n\\]\nSolution:\n\nRewrite the function with a fractional exponent: \\(\\sqrt{x}=x^{1/2}\\).\nApply the power rule:\n\n\\[\ng(x)=x^{1/2}\n\\]\n\\[\n\\int x^{1/2} \\, dx = \\int x^{1/2} \\, dx = \\frac{x^{1/2+1}}{1/2+1} + C = \\frac{x^{3/2}}{3/2} + C = \\frac{2}{3} x^{3/2} + C\n\\]"
  },
  {
    "objectID": "ae/ae-14-integration-A.html#exercise-3",
    "href": "ae/ae-14-integration-A.html#exercise-3",
    "title": "AE 14: Integration",
    "section": "Exercise 3",
    "text": "Exercise 3\nFunction:\n\\[\nh(x)=\\ln(x)\n\\]\nSolution:\n\nUse the integral of the natural logarithm function:\n\n\\[\n\\int \\ln(x) \\, dx = x \\ln(x) - x + C\n\\]"
  },
  {
    "objectID": "ae/ae-14-integration-A.html#exercise-4",
    "href": "ae/ae-14-integration-A.html#exercise-4",
    "title": "AE 14: Integration",
    "section": "Exercise 4",
    "text": "Exercise 4\nFunction:\n\\[\n\\int xe^x \\space dx\n\\]\nSolution\n\nIdentify the parts: Let \\(u=x\\) and \\(dv=e^x \\space dx\\)\nDifferentiate and integrate:\n\nDifferentiate: \\(du = dx\\)\nIntegrate: \\(v = e^x\\)\n\nApply the integration by parts formula: \\(\\int u \\space dv = uv - \\int v \\space du\\)\n\n\\[\n\\int x e^x \\, dx = x e^x - \\int e^x \\, dx\n\\]\n\nSimplify the integral:\n\n\\[\n\\int x e^x \\, dx = x e^x - e^x + C\n\\]\n\nFinal answer:\n\n\\[\n\\int x e^x \\, dx = e^x (x - 1) + C\n\\]"
  },
  {
    "objectID": "ae/ae-14-integration-A.html#exercise-5",
    "href": "ae/ae-14-integration-A.html#exercise-5",
    "title": "AE 14: Integration",
    "section": "Exercise 5",
    "text": "Exercise 5\nFunction:\n\\[\n\\int x \\ln(x) \\space dx\n\\]\nSolution:\n\nIdentify the parts: \\(u=\\ln(x)\\) and \\(dv=x \\space dx\\)\nDifferentiate and integrate:\n\nDifferentiate: \\(du=\\frac{1}{x}\\space dx\\)\nIntegrate: \\(v=\\frac{x^{2}}{2}\\)\n\nApply the integration by parts formula: \\(\\int u \\space dv = uv - \\int v \\space du\\)\nSubstitute the parts:\n\n\\[\n\\int x \\space \\ln(x) \\space dx = \\ln(x)  \\cdot \\frac{x^2}{2} - \\int \\frac{x^2}{2} \\cdot \\frac{1}{x} \\space dx\n\\]\n\nSimplify the integral:\n\n\\[\n\\int x \\ln(x) \\space dx = \\frac{x^{2} \\ln(x)}{2} - \\frac{1}{2} \\int x \\space dx\n\\]\n\nIntegrate the remaining part:\n\n\\[\n\\frac{1}{2} \\int x \\space dx = \\frac{1}{2} \\cdot \\frac{x^2}{2} = \\frac{x^2}{4}\n\\]\n\nCombine the results:\n\n\\[\n\\int x \\ln(x) \\space dx = \\frac{x^2 \\ln(x)}{2} - \\frac{x^2}{4} + C\n\\]\n\nFinal answer:\n\n\\[\n\\int x \\ln(x) \\space dx = \\frac{x^2}{2} \\ln(x) - \\frac{x^2}{4} + C\n\\]"
  },
  {
    "objectID": "ae/ae-14-integration-A.html#exercise-6",
    "href": "ae/ae-14-integration-A.html#exercise-6",
    "title": "AE 14: Integration",
    "section": "Exercise 6",
    "text": "Exercise 6\nFunction:\n\\[\n\\int x \\sin (x^2) \\space dx\n\\]\nSolution:\n\nIdentify the substitution: Let \\(u = x^2\\)\nDifferentiate and solve for \\(du\\):\n\n\\(du = 2x \\space dx\\)\n\\(\\frac{1}{2}du = x \\space dx\\)\n\nSubstitute into the integral\n\n\\[\n\\int x sin(x^2) \\space dx = \\int \\sin(u) \\cdot \\frac{1}{2} \\space du\n\\]\n\nIntegrate:\n\n\\[\n\\frac{1}{2} \\int \\sin(u) \\space du = \\frac{1}{2}(-\\cos(u)) + C\n\\]\n\nSubstitute back into the original variable:\n\n\\[\n\\frac{1}{2}(-\\cos(u)) + C = -\\frac{1}{2}\\cos(x^2) + C\n\\]\n\nFinal answer\n\n\\[\n\\int x sin(x^2) \\space dx = -\\frac{1}{2}\\cos(x^2) + C\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html",
    "href": "ae/ae-13-derivation-A.html",
    "title": "AE 13: Derivation",
    "section": "",
    "text": "In this exercise, we will:"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-1",
    "href": "ae/ae-13-derivation-A.html#exercise-1",
    "title": "AE 13: Derivation",
    "section": "Exercise 1",
    "text": "Exercise 1\nFunction:\n\\[\nf(x) = 5x^3\n\\]\nSolution:\n\nThe power rule states that \\(\\frac{d}{dx}(x^n)=nx^{n-1}\\)\nApplying the power rule:\n\n\\[\nf'(x) = \\frac{d}{dx}(5x^3) = 15x^2\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-2",
    "href": "ae/ae-13-derivation-A.html#exercise-2",
    "title": "AE 13: Derivation",
    "section": "Exercise 2",
    "text": "Exercise 2\nFunction:\n\\[\ng(x) = \\sqrt{x}\n\\]\nSolution:\n\nRewrite the function with a fractional exponent: \\(\\sqrt{x}=x^{1/2}\\).\nApply the power rule:\n\n\\[\ng(x)=x^{1/2}\n\\]\n\\[\ng'(x) = \\frac{1}{2}x^{1/2-1} = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-3",
    "href": "ae/ae-13-derivation-A.html#exercise-3",
    "title": "AE 13: Derivation",
    "section": "Exercise 3",
    "text": "Exercise 3\nFunction:\n\\[\nh(x)=\\ln(x)\n\\]\nSolution:\n\nThe derivative of the natural logarithm function is \\(\\frac{1}{x}\\)\n\n\\[\nh'(x) = \\frac{d}{dx}(\\ln(x)) = \\frac{1}{x}\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-4",
    "href": "ae/ae-13-derivation-A.html#exercise-4",
    "title": "AE 13: Derivation",
    "section": "Exercise 4",
    "text": "Exercise 4\nFunction:\n\\[\nf(x)=(2x^3+3x)^4\n\\]\nSolution\n\nIdentify the outer function and inner function:\n\nOuter function: \\(u^4\\)\nInner function: \\(u=2x^3+3x\\)\n\nApply the chain rule:\n\n\\[\n\\frac{d}{dx}(u^4)=4u^3 \\cdot \\frac{d}{dx}(u)\n\\]\n\nDifferentiate the inner function:\n\n\\[\n\\frac{d}{dx}(2x^3+3x)=6x^2+3\n\\]\n\nCombine the results:\n\n\\[\n4(2x^3 + 3x)^3 \\cdot (6x^2 + 3)\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-5",
    "href": "ae/ae-13-derivation-A.html#exercise-5",
    "title": "AE 13: Derivation",
    "section": "Exercise 5",
    "text": "Exercise 5\nFunction:\n\\[\n\\frac{d}{dx}(x^2 e^x)\n\\]\nSolution:\n\nIdentify the product of two functions: \\(u=x^2\\) and \\(v=e^x\\)\nApply the product rule: \\((u \\cdot v)^{'}=u^{'} \\cdot v + u \\cdot v^{'}\\)\nDifferentiate each function:\n\n\\[\nu^{'}=\\frac{d}{dx}(x^2)=2x\n\\]\n\\[\nv^{'}=\\frac{d}{dx}(e^x)=e^x\n\\]\n\nCombine the results:\n\n\\[\ng^{'}=x^2 \\cdot e^x + e^x \\cdot 2x = e^x(x^2+2x)\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-6",
    "href": "ae/ae-13-derivation-A.html#exercise-6",
    "title": "AE 13: Derivation",
    "section": "Exercise 6",
    "text": "Exercise 6\nFunction:\n\\[\nh(x)=\\sin (x^2)\n\\]\nSolution:\n\nIdentify the outer function and inner function:\n\nOuter function: \\(\\sin (u)\\)\nInner function: \\(u = x^2\\)\n\nApply the chain rule: \\(\\frac{d}{dx}(\\sin (u))=\\cos (u) \\cdot \\frac{d}{dx}(u)\\)\nDifferentiate the inner function:\n\n\\[\n\\frac{d}{dx}(x^2)=2x\n\\]\n\nCombine the results:\n\n\\[\nh^{'}(x)=2x\\cos (x^2)\n\\]"
  },
  {
    "objectID": "ae/ae-13-derivation-A.html#exercise-7",
    "href": "ae/ae-13-derivation-A.html#exercise-7",
    "title": "AE 13: Derivation",
    "section": "Exercise 7",
    "text": "Exercise 7\nFunction:\n\\[\nf(x)=(\\ln (x) \\cdot e^{2x})^3\n\\]\nSolution\n\nIdentify the outer function and inner function\n\nOuter function: \\(u^3\\)\nInner function: \\(u=\\ln (x) \\cdot e^{2x}\\)\n\nApply the chain rule: \\(\\frac{d}{dx}(u^3)=3u^2 \\cdot \\frac{d}{dx}(u)\\)\nUse the product rule to differentiate the inner function:\n\n\\[\nu=\\ln (x), v=e^{2x}\n\\]\n\nDifferentiate each function:\n\n\\[\nu^{'}=\\frac{d}{dx}(\\ln (x)) = \\frac{1}{x}\n\\]\n\\[\nv^{'}=\\frac{d}{dx}(e^{2x}) = 2e^{2x}\n\\]\n\nApply the product rule:\n\n\\[\n\\frac{d}{dx}(\\ln(x) \\cdot e^{2x}) = \\left( \\frac{1}{x} \\cdot e^{2x} \\right) + \\left( \\ln(x) \\cdot 2e^{2x} \\right)\n\\]\n\\[\n= e^{2x} \\left( \\frac{1}{x} + 2 \\ln(x) \\right)\n\\]\n\nCombine the outer function derivative:\n\n\\[\nf'(x) = 3 \\left( \\ln(x) \\cdot e^{2x} \\right)^2 \\cdot e^{2x} \\left( \\frac{1}{x} + 2 \\ln(x) \\right)\n\\]\n\nSimplify:\n\n\\[\nf'(x) = 3e^{4x} (\\ln(x))^2 \\left( \\frac{1}{x} + 2 \\ln(x) \\right)\n\\]"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html",
    "href": "ae/ae-04-flights-preprocessing.html",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\nimport numpy as np\nfrom nycflights13 import flights"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html#exercise-1---load-data",
    "href": "ae/ae-04-flights-preprocessing.html#exercise-1---load-data",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 1 - Load data",
    "text": "Exercise 1 - Load data\nFill in the blanks:\n\n# add code here\n\nThe flights data frame has __ rows. Each row represents a __."
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html#exercise-2---data-cleaning",
    "href": "ae/ae-04-flights-preprocessing.html#exercise-2---data-cleaning",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 2 - Data cleaning",
    "text": "Exercise 2 - Data cleaning\nRemove rows with missing values in the arr_delay and distance columns.\nWhat are the names of the variables in flights.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html#exercise-3---original-data-distribution",
    "href": "ae/ae-04-flights-preprocessing.html#exercise-3---original-data-distribution",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 3 - Original Data Distribution",
    "text": "Exercise 3 - Original Data Distribution\n\nPlot the original distributions of arr_delay and distance.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html#exercise-4---check-for-skewness",
    "href": "ae/ae-04-flights-preprocessing.html#exercise-4---check-for-skewness",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 4 - Check for Skewness",
    "text": "Exercise 4 - Check for Skewness\n\nCalculate and print the skewness of arr_delay and distance.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html#exercise-5---scaling",
    "href": "ae/ae-04-flights-preprocessing.html#exercise-5---scaling",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 5 - Scaling",
    "text": "Exercise 5 - Scaling\n\nCheck the summary statistics of arr_delay and distance to see if scaling is necessary.\n\n\n# add code here\n\n\n# add code here\n\n\nQuestion: Do arr_delay and distance need to be scaled? Why?\n\nadd response here.\n\nApply standard scaling, maximum absolute scaling, and Min-Max Scaling to the transformed arr_delay and distance.\nHint: use the framework df_clean.loc[:, ['arr_delay_minmax', 'distance_minmax']] to prevent errors\n\n\n# add code here\n\n\nQuestion: What are the two pros and two cons of standardizing data?\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-04-flights-preprocessing.html#exercise-6---transformation",
    "href": "ae/ae-04-flights-preprocessing.html#exercise-6---transformation",
    "title": "AE 04: NYC flights + data preprocessing",
    "section": "Exercise 6 - Transformation",
    "text": "Exercise 6 - Transformation\n\nCheck the summary statistics again with your min-max standardized columns.\n\n\n# add code here\n\n\n# add code here\n\n\nQuestion: Why should you use the min-max scaled data instead of a different scaling for the transformations (hint: especially log transformation)\n\nAdd response here.\n\nApply a log transformation to arr_delay if it is positively skewed and apply a square root transformation to distance if it is negatively skewed (use if else statements).\nHint: Logical operators in Python:\n\n\n\n\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?\n\n\nx and y\nis x AND y?\n\n\nx or y\nis x OR y?\n\n\npd.isna(x)\nis x NA?\n\n\n~pd.isna(x)\nis x not NA?\n\n\nx in y\nis x in y?\n\n\nx not in y\nis x not in y?\n\n\nnot x\nis not x? (only makes sense if x is True or False)\n\n\n\n\n\n# add code here\n\n\nQuestion: Why do we have to add a constant when we perform a log or square-root transformation (i.e., np.log1p(df['column' + 1]))?\n\nadd response here."
  },
  {
    "objectID": "ae/ae-11-spam-filter-A.html",
    "href": "ae/ae-11-spam-filter-A.html",
    "title": "AE 11: Building a spam filter",
    "section": "",
    "text": "In this application exercise, we will\nTo illustrate logistic regression, we will build a spam filter from email data.\nThe data come from incoming emails in David Diez’s (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012. All personally identifiable information has been removed.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nnp.random.seed(123)\n\nemail = pd.read_csv('data/email.csv')\nprint(email.info())\nprint(email.describe())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3921 entries, 0 to 3920\nData columns (total 21 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   spam          3921 non-null   int64  \n 1   to_multiple   3921 non-null   int64  \n 2   from          3921 non-null   int64  \n 3   cc            3921 non-null   int64  \n 4   sent_email    3921 non-null   int64  \n 5   time          3921 non-null   object \n 6   image         3921 non-null   int64  \n 7   attach        3921 non-null   int64  \n 8   dollar        3921 non-null   int64  \n 9   winner        3921 non-null   object \n 10  inherit       3921 non-null   int64  \n 11  viagra        3921 non-null   int64  \n 12  password      3921 non-null   int64  \n 13  num_char      3921 non-null   float64\n 14  line_breaks   3921 non-null   int64  \n 15  format        3921 non-null   int64  \n 16  re_subj       3921 non-null   int64  \n 17  exclaim_subj  3921 non-null   int64  \n 18  urgent_subj   3921 non-null   int64  \n 19  exclaim_mess  3921 non-null   int64  \n 20  number        3921 non-null   object \ndtypes: float64(1), int64(17), object(3)\nmemory usage: 643.4+ KB\nNone\n              spam  to_multiple         from           cc   sent_email  \\\ncount  3921.000000  3921.000000  3921.000000  3921.000000  3921.000000   \nmean      0.093599     0.158123     0.999235     0.404489     0.277990   \nstd       0.291307     0.364903     0.027654     2.666424     0.448066   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     1.000000     0.000000     0.000000   \n50%       0.000000     0.000000     1.000000     0.000000     0.000000   \n75%       0.000000     0.000000     1.000000     0.000000     1.000000   \nmax       1.000000     1.000000     1.000000    68.000000     1.000000   \n\n             image       attach       dollar      inherit       viagra  \\\ncount  3921.000000  3921.000000  3921.000000  3921.000000  3921.000000   \nmean      0.048457     0.132874     1.467228     0.038001     0.002040   \nstd       0.450848     0.718518     5.022298     0.267899     0.127759   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n75%       0.000000     0.000000     0.000000     0.000000     0.000000   \nmax      20.000000    21.000000    64.000000     9.000000     8.000000   \n\n          password     num_char  line_breaks       format      re_subj  \\\ncount  3921.000000  3921.000000  3921.000000  3921.000000  3921.000000   \nmean      0.108136    10.706586   230.658505     0.695231     0.261413   \nstd       0.959931    14.645786   319.304959     0.460368     0.439460   \nmin       0.000000     0.001000     1.000000     0.000000     0.000000   \n25%       0.000000     1.459000    34.000000     0.000000     0.000000   \n50%       0.000000     5.856000   119.000000     1.000000     0.000000   \n75%       0.000000    14.084000   298.000000     1.000000     1.000000   \nmax      28.000000   190.087000  4022.000000     1.000000     1.000000   \n\n       exclaim_subj  urgent_subj  exclaim_mess  \ncount   3921.000000  3921.000000   3921.000000  \nmean       0.080337     0.001785      6.584290  \nstd        0.271848     0.042220     51.479871  \nmin        0.000000     0.000000      0.000000  \n25%        0.000000     0.000000      0.000000  \n50%        0.000000     0.000000      1.000000  \n75%        0.000000     0.000000      4.000000  \nmax        1.000000     1.000000   1236.000000\nThe variables we’ll use in this analysis are\nGoal: Use the number of exclamation points in an email to predict whether or not it is spam."
  },
  {
    "objectID": "ae/ae-11-spam-filter-A.html#exercise-1",
    "href": "ae/ae-11-spam-filter-A.html#exercise-1",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nFit the logistic regression model using the number of exclamation points to predict the probability an email is spam.\n\n\nX = email[['exclaim_mess']]\ny = email['spam']\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\n# Summary output\nprint(f\"Intercept: {log_reg.intercept_[0]}\")\nprint(f\"Coefficient: {log_reg.coef_[0][0]}\")\n\nIntercept: -2.2723427075496767\nCoefficient: 0.00027240944041455475\n\n\n\nHow does the code above differ from previous code we’ve used to fit regression models? Compare your summary output to the estimated model below.\n\n\\[\\log\\Big(\\frac{p}{1-p}\\Big) = -2.27 - 0.000272 \\times exclaim\\_mess\\]\nWe use logistic instead of linear regression."
  },
  {
    "objectID": "ae/ae-11-spam-filter-A.html#exercise-2",
    "href": "ae/ae-11-spam-filter-A.html#exercise-2",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the probability the email is spam if it contains 10 exclamation points? Answer the question using the log_reg.predict_proba() function.\n\nemail_10_exclaim = np.array([[10]])\npred_prob = log_reg.predict_proba(email_10_exclaim)[0][1]\nprint(f\"Predicted probability of spam for an email with 10 exclamation points: {pred_prob}\")\n\nPredicted probability of spam for an email with 10 exclamation points: 0.0936705855941035"
  },
  {
    "objectID": "ae/ae-11-spam-filter-A.html#exercise-3",
    "href": "ae/ae-11-spam-filter-A.html#exercise-3",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have the probability an email is spam, but ultimately we want to use the probability to classify an email as spam or not spam. Therefore, we need to set a decision-making threshold, such that an email is classified as spam if the predicted probability is greater than the threshold and not spam otherwise.\nSuppose you are a data scientist working on a spam filter. You must determine how high the predicted probability must be before you think it would be reasonable to call it spam and put it in the junk folder (which the user is unlikely to check).\nWhat are some trade offs you would consider as you set the decision-making threshold?\nAnswers will vary.\n\nemail['pred_class'] = (log_reg.predict_proba(X)[:, 1] &gt;= 0.5).astype(int)\n\nplt.figure(figsize=(8, 6))\nsns.stripplot(data=email, x='exclaim_mess', y=email['spam'].astype(str), hue='pred_class', palette='colorblind', jitter=True, alpha=0.5)\nplt.xlabel('Number of Exclamation Marks')\nplt.ylabel('Spam (0 or 1)')\nplt.title('Logistic Regression Model Predictions with Jitter')\nplt.show()"
  },
  {
    "objectID": "ae/ae-11-spam-filter-A.html#exercise-4",
    "href": "ae/ae-11-spam-filter-A.html#exercise-4",
    "title": "AE 11: Building a spam filter",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model with all variables in the dataset as predictors and recreate the visualization above for this model.\nHint: use sns.stripplot() like above.\n\nemail_processed = pd.get_dummies(email, drop_first=True)\nemail_processed = email_processed.drop(columns=['date_column_name'], errors='ignore')\n\nX_all = email_processed.drop(columns=['spam'])\ny_all = email_processed['spam']\n\nlog_reg_all = LogisticRegression(max_iter=100)\nlog_reg_all.fit(X_all, y_all)\n\nemail_processed['pred_class_all'] = (log_reg_all.predict_proba(X_all)[:, 1] &gt;= 0.5).astype(int)\n\nplt.figure(figsize=(8, 6))\nsns.stripplot(data=email_processed, x='exclaim_mess', y=email['spam'].astype(str), hue='pred_class_all', palette='colorblind', jitter=True, alpha=0.5)\nplt.xlabel('Number of Exclamation Marks')\nplt.ylabel('Spam (0 or 1)')\nplt.title('Logistic Regression Model Predictions with All Variables')\nplt.show()\n\n\n\n\n\n\n\n\n\nUse model evaluation metrics from confusion_matrix and classification_report to evaluate our model.\n\n\nconf_matrix = confusion_matrix(y_all, email_processed['pred_class_all'])\nclass_report = classification_report(y_all, email_processed['pred_class_all'])\nprint(conf_matrix)\nprint(class_report)\n\n[[3447  107]\n [ 282   85]]\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.95      3554\n           1       0.44      0.23      0.30       367\n\n    accuracy                           0.90      3921\n   macro avg       0.68      0.60      0.63      3921\nweighted avg       0.88      0.90      0.89      3921"
  },
  {
    "objectID": "summative/midterm.html",
    "href": "summative/midterm.html",
    "title": "Midterm Assignment",
    "section": "",
    "text": "Note\n\n\n\nYou can find the blizzard_salary.csv data here.\nIn 2020, employees of Blizzard Entertainment circulated a spreadsheet to anonymously share salaries and recent pay increases amidst rising tension in the video game industry over wage disparities and executive compensation. (Source: Blizzard Workers Share Salaries in Revolt Over Pay)\nThe name of the data frame used for this analysis is blizzard_salary and the variables are:\nThe top ten rows and .info of blizzard_salary are shown below:\npercent_incr salary_type  annual_salary performance_rating\n0           1.0        year            1.0               High\n1           1.0        year            1.0         Successful\n2           1.0        year            1.0               High\n3           1.0      Hourly        33987.2         Successful\n4           NaN      Hourly        34798.4               High\n5           NaN      Hourly        35360.0                NaN\n6           NaN      Hourly        37440.0                NaN\n7           0.0      Hourly        37814.4                NaN\n8           4.0      Hourly        41100.8                Top\n9           1.2      Hourly        42328.0                NaN\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 409 entries, 0 to 465\nData columns (total 4 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   percent_incr        370 non-null    float64\n 1   salary_type         409 non-null    object \n 2   annual_salary       409 non-null    float64\n 3   performance_rating  298 non-null    object \ndtypes: float64(2), object(2)\nmemory usage: 16.0+ KB\nNone"
  },
  {
    "objectID": "summative/midterm.html#question-1",
    "href": "summative/midterm.html#question-1",
    "title": "Midterm Assignment",
    "section": "Question 1",
    "text": "Question 1\nWhich of the following is correct? Choose all that apply.\n\nThe blizzard_salary dataset has 399 rows.\nThe blizzard_salary dataset has 4 columns.\nEach row represents a Blizzard Entertainment worker who filled out the spreadsheet.\nThe percent_incr variable is numerical and discrete.\nThe salary_type variable is numerical.\nThe annual_salary variable is numerical.\nThe performance_rating variable is categorical and ordinal."
  },
  {
    "objectID": "summative/midterm.html#question-2",
    "href": "summative/midterm.html#question-2",
    "title": "Midterm Assignment",
    "section": "Question 2",
    "text": "Question 2\nFigure 1 (a) and Figure 1 (b) show the distributions of annual salaries of hourly and salaried workers. The two figures show the same data, with the facets organized across rows and across columns. Which of the two figures is better for comparing the median annual salaries of hourly and salaried workers. Explain your reasoning.\n\n\n\n\n\n\n\n\n\n\n\n(a) Option 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Option 2\n\n\n\n\n\n\n\nFigure 1: Distribution of annual salaries of Blizzard employees"
  },
  {
    "objectID": "summative/midterm.html#question-3",
    "href": "summative/midterm.html#question-3",
    "title": "Midterm Assignment",
    "section": "Question 3",
    "text": "Question 3\nSuppose your teammate wrote the following code as part of their analysis of the data.\n\nblizzard_summary = blizzard_salary.groupby('salary_type').agg(\n    mean_annual_salary=('annual_salary', 'mean'),\n    median_annual_salary=('annual_salary', 'median')\n).reset_index()\n\nprint(blizzard_summary)\n\nThey then printed out the results shown below. Unfortunately one of the numbers got erased from the printout. It’s indicated with _____ below.\n  salary_type    mean_annual_salary    median_annual_salary\n  Hourly         63003.                54246.\n  Salaried       90183.                _____\nWhich of the following is the best estimate for that erased value?\n\n30,000\n50,000\n80,000\n100,000"
  },
  {
    "objectID": "summative/midterm.html#question-4",
    "href": "summative/midterm.html#question-4",
    "title": "Midterm Assignment",
    "section": "Question 4",
    "text": "Question 4\nWhich distribution of annual salaries has a higher standard deviation?\n\nHourly workers\nSalaried workers\nRoughly the same"
  },
  {
    "objectID": "summative/midterm.html#question-5",
    "href": "summative/midterm.html#question-5",
    "title": "Midterm Assignment",
    "section": "Question 5",
    "text": "Question 5\nWhich of the following alternate plots would also be useful for visualizing the distributions of annual salaries of hourly and salaried workers? Choose all that apply.\na. Box plots\nb. Density plots\nc. Pie charts\nd. Waffle charts\ne. Histograms\nf. Scatterplots"
  },
  {
    "objectID": "summative/midterm.html#questions-6-and-7",
    "href": "summative/midterm.html#questions-6-and-7",
    "title": "Midterm Assignment",
    "section": "Questions 6 and 7",
    "text": "Questions 6 and 7\nSuppose you made the bar plot shown in Figure 2 (a) to visualize the distribution of performance_rating and your teammate made the bar plot shown in Figure 2 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Option 1\n\n\n\n\n\n\n\n\n\n\n\n(b) Option 2\n\n\n\n\n\n\n\nFigure 2: Distribution of performance rating\n\n\n\nYou made your bar plot without transforming the data in any way, while your friend did first transform the data with code like the following:\n\nblizzard_salary['performance_rating'] = pd._(1)_(blizzard_salary['performance_rating'], categories=[_(2)_], ordered=True)\n\nQuestion 6: What goes in the blank (1)?\n\n.sort_values()\n.Categorical()\n.groupby()\n.fillna()\n\nQuestion 7: What goes in the blank (2)?\n\n\"Poor\", \"Successful\", \"High\", \"Top\"\n\"Successful\", \"High\", \"Top\"\n\"Top\", \"High\", \"Successful\", \"Poor\"\nPoor, Successful, High, Top"
  },
  {
    "objectID": "summative/midterm.html#questions-8---10",
    "href": "summative/midterm.html#questions-8---10",
    "title": "Midterm Assignment",
    "section": "Questions 8 - 10",
    "text": "Questions 8 - 10\nFinally, another teammate creates the following two plots.\n\n\n\n\n\n\n\n\n\n\n\n(a) Option 1\n\n\n\n\n\n\n\n\n\n\n\n(b) Option 2\n\n\n\n\n\n\n\nFigure 3: Distribution of salary type by performance rating\n\n\n\nQuestion 8: Your teammate asks you for help deciding which one to use in the final report for visualizing the relationship between performance rating and salary type. In 1-3 sentences, can you help them make a decision, justify your choice, and write the narrative that should go with the plot?\nQuestion 9: A friend with a keen eye points out that the number of observations in Figure 3 (a) seems lower than the total number of observations in blizzard_salary. What might be going on here? Explain your reasoning.\nQuestion 10: Below are the proportions of performance ratings for hourly and salaried workers. Recreate the plot in Figure 3 (b), then interpret how the results from the table and within the plot relate to each other.\n\n\nperformance_rating  Poor  Successful      High       Top\nsalary_type                                             \nHourly               0.0    0.149068  0.064815  0.166667\nyear                 1.0    0.850932  0.935185  0.833333"
  },
  {
    "objectID": "summative/midterm.html#questions-11-and-12",
    "href": "summative/midterm.html#questions-11-and-12",
    "title": "Midterm Assignment",
    "section": "Questions 11 and 12",
    "text": "Questions 11 and 12\nThe table below shows the distribution of salary_type and performance_rating.\n\n\n     percent_incr salary_type  annual_salary performance_rating\n226           0.0        year        80000.0               Poor\n245           3.0        year        83000.0               Poor\n340           0.0        year       116000.0               Poor\n391           0.0        year       135219.0               Poor\n415           0.0        year       147500.0               Poor\n\n\nThe pipeline below produces a data frame with a fewer number of rows than blizzard_salary.\n\nfiltered_df = blizzard_salary[(blizzard_salary['salary_type'] _(1)_ \"Hourly\") _(2)_ (blizzard_salary['performance_rating'] == \"Poor\")]\nfiltered_df = filtered_df._(3)_(by='annual_salary')\nprint(filtered_df)\n\n\n\n     percent_incr salary_type  annual_salary performance_rating\n226           0.0        year        80000.0               Poor\n245           3.0        year        83000.0               Poor\n340           0.0        year       116000.0               Poor\n391           0.0        year       135219.0               Poor\n415           0.0        year       147500.0               Poor\n\n\nQuestion 11: Which of the following goes in blanks (1) and (2)?\n\n\n\n\n(1)\n(2)\n\n\n\n\na.\n!=\n|\n\n\nb.\n==\n&\n\n\nc.\n!=\n&\n\n\nd.\n==\n|\n\n\n\nQuestion 12: Which function or functions go into blank (3)?\n\n.sort_values()\n.assign()\n.groupby()\n.aggregate()"
  },
  {
    "objectID": "summative/midterm.html#question-13",
    "href": "summative/midterm.html#question-13",
    "title": "Midterm Assignment",
    "section": "Question 13",
    "text": "Question 13\nYou’re reviewing another team’s work and they made the following visualization:\n\n\n\n\n\n\n\n\n\nAnd they wrote the following interpretation for the relationship between annual salary and percent increase for Top performers:\n\nThe relationship is positive, having a higher salary results in a higher percent increase. There is one clear outlier.\n\nWhich of the following is/are the most accurate and helpful) peer review note for this interpretation. Choose all that apply.\n\nThe interpretation is complete and perfect, no changes needed!\nThe interpretation doesn’t mention the direction of the relationship.\nThe interpretation doesn’t mention the form of the relationship, which is linear.\nThe interpretation doesn’t mention the strength of the relationship, which is somewhat strong.\nThere isn’t a clear outlier in the plot. If any points stand out as potential outliers, more guidance should be given to the reader to identify them (e.g., salary and/or percent increase amount).\nThe interpretation is causal – we don’t know if the cause of the high percent increase is higher annual salary based on observational data. The causal direction might be the other way around, or there may be other factors contributing to the apparent relationship."
  },
  {
    "objectID": "summative/midterm.html#question-14",
    "href": "summative/midterm.html#question-14",
    "title": "Midterm Assignment",
    "section": "Question 14",
    "text": "Question 14\nBelow is some code and its output.\n```{python}\n# label=plot blizzard\n\nsns.boxplot(data=blizzard_salary, x='performance_rating', y='percent_incr')\nplt.xlabel('Performance rating')\nplt.ylabel('Percent increase')\nplt.show()\n\n```\n\n\nText(0, 0.5, 'Percent increase')\n\n\n\n\n\n\n\n\n\nPart 1: List at least 3 things that should be fixed or improved in the code.\nPart 2: How could we show missing values in this plot?"
  },
  {
    "objectID": "summative/midterm.html#question-15",
    "href": "summative/midterm.html#question-15",
    "title": "Midterm Assignment",
    "section": "Question 15",
    "text": "Question 15\nYou’re working on a data analysis on salaries of Blizzard employees in a Quarto document in a project version controlled by Git. You create a plot and write up a paragraph describing any patterns in it. Then, your teammate says “render, commit, and push”.\nPart 1: What do they mean by each of these three steps. In 1-2 sentences for each, explain in your own words what they mean.\n\nRender:\n\n\nCommit:\n\n\nPush:\n\nPart 2: Your teammate is getting impatient and they interrupt you after you rendered and committed and say “I still can’t see your changes in our shared GitHub repo when I look at it in my web browser.” Which of the following answers is the most accurate?\n\nI rendered my document, you should be seeing my changes on GitHub when you look at it in your web browser.\nI committed my changes, you should be seeing my changes on GitHub when you look at it in your web browser.\nI didn’t yet push my changes, it’s expected that you are not seeing them on GitHub when you look at it in your web browser. Wait until I push, and check again.\nYou need to pull to see my changes on GitHub in the web browser."
  },
  {
    "objectID": "summative/midterm.html#bonus",
    "href": "summative/midterm.html#bonus",
    "title": "Midterm Assignment",
    "section": "Bonus",
    "text": "Bonus\nPick a concept we introduced in class so far that you’ve been struggling with and explain it in your own words."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Course GitHub organization\n🔗 on GitHub\n\n\n\n\n\n\nSlack\n🔗 info511spring2025\n\n\nClass recordings\n🔗 on Panopto\n\n\nGradebook\n🔗 on D2L\n\n\nTextbooks\n🔗 Data Science from Scratch (must sign-in via U of A Libraries)\n🔗 Python for Data Analysis\n🔗 An Introduction to Statistical Learning\n🔗 Practical Statistics for Data Science\n🔗 Essential Math for Data Science (must sign-in via U of A Libraries)\n\n\nPackage documentation\n🔗 NumPy: numpy.org\n🔗 pandas: pandas.pydata.org/\n🔗 scikit-learn: scikit-learn.org\n🔗 matplotlib: matplotlib.org\n🔗 seaborn: seaborn.pydata.org",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6 - Modeling I",
    "section": "",
    "text": "In this lab you’ll start your practice of statistical modeling. You’ll fit models, interpret model output, and make decisions about your data and research question based on the model results.\n\n\nIn this lab we will work with the seaborn and statsmodels modules.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.eval_measures import mse\n\n\n\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment.\n\n\nAdditionally, if you’re using functions that are not introduced in the course materials, you must cite your sources.\n\n\n\n\n\n\nImportant\n\n\n\nFailure to cite outside resources used, including Large Language Models like Chat GPT, is a violation of the University of Arizona Code of Academic Integrity and will be treated as such."
  },
  {
    "objectID": "labs/lab-6.html#packages",
    "href": "labs/lab-6.html#packages",
    "title": "Lab 6 - Modeling I",
    "section": "",
    "text": "In this lab we will work with the seaborn and statsmodels modules.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.eval_measures import mse"
  },
  {
    "objectID": "labs/lab-6.html#guidelines",
    "href": "labs/lab-6.html#guidelines",
    "title": "Lab 6 - Modeling I",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment.\n\n\nAdditionally, if you’re using functions that are not introduced in the course materials, you must cite your sources.\n\n\n\n\n\n\nImportant\n\n\n\nFailure to cite outside resources used, including Large Language Models like Chat GPT, is a violation of the University of Arizona Code of Academic Integrity and will be treated as such."
  },
  {
    "objectID": "labs/lab-6.html#question-1",
    "href": "labs/lab-6.html#question-1",
    "title": "Lab 6 - Modeling I",
    "section": "Question 1",
    "text": "Question 1\nWe are interested in the impact of smoking during pregnancy. Since it is not possible to run a randomized controlled experiment to investigate this impact, we will instead use a data set has been of interest to medical researchers who are studying the relation between habits and practices of expectant mothers and the birth of their children. This is a random sample of 1,000 cases from a data set released in 2014 by the state of North Carolina. The data set is called births14 and it is available in your data folder.\n\nCreate a version of the births14 data set dropping observations where there are NAs for habit. You can call this version births14_habitgiven.\nPlotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Create an appropriate plot displaying the relationship between weight and habit. In 2-3 sentences, discuss the relationships observed.\nNow, fit a linear model that predicts weight from habit. Provide the tidy summary output below.\nWrite the estimated least squares regression line below using proper notation.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you need to type an equation using proper notation, type your answers in-between $$ and $$. You may use \\hat{example} to put a hat on a character."
  },
  {
    "objectID": "labs/lab-6.html#question-2",
    "href": "labs/lab-6.html#question-2",
    "title": "Lab 6 - Modeling I",
    "section": "Question 2",
    "text": "Question 2\n\nAnother researcher is interested in assessing the relationship between babies’ weights and mothers’ ages. Fit another linear model to investigate this relationship. Provide the summary output below.\nIn 2-3 sentences, explain how the regression line to model these data is fit, i.e., based on what criteria Python determines the regression line.\nInterpret the intercept in the context of the data and the research question. Is the intercept meaningful in this context? Why or why not?\nInterpret the slope in the context of the data and the research question."
  },
  {
    "objectID": "labs/lab-6.html#question-3",
    "href": "labs/lab-6.html#question-3",
    "title": "Lab 6 - Modeling I",
    "section": "Question 3",
    "text": "Question 3\nLet’s start by reading in the parasites data and examining the relationship between divergence_time and parsim.\n\nLoad the data and save the data frame as parasites.\nBased on the goals of the analysis, what is the response variable?\nVisualize the relationship between the two variables.\nUse the visualization to describe the relationship between the two variables."
  },
  {
    "objectID": "labs/lab-6.html#question-4",
    "href": "labs/lab-6.html#question-4",
    "title": "Lab 6 - Modeling I",
    "section": "Question 4",
    "text": "Question 4\nNext, model this relationship.\n\nFit the model and write the estimated regression equation.\nInterpret the slope and the intercept in the context of the data.\nRecreate the visualization from Question 3, this time adding a regression line to the visualization.\nWhat do you notice about the prediction (regression) line that may be strange, particularly for very large divergence times?"
  },
  {
    "objectID": "labs/lab-6.html#question-5",
    "href": "labs/lab-6.html#question-5",
    "title": "Lab 6 - Modeling I",
    "section": "Question 5",
    "text": "Question 5\nSince parsim takes values between 0 and 1, we want to transform this variable so that it can range between (−∞,+∞). This will be better suited for fitting a regression model (and interpreting predicted values!)\n\nCreate a new variable transformed_parsim that is calculated as math.log(parsim/(1-parsim)). Add this variable to your data frame.\n\n\n\n\n\n\n\nNote\n\n\n\nmath.log() in Python represents the natural log.\n\n\n\nThen, visualize the relationship between divergence_time and transformed_parsim. Add a regression line to your visualization.\nWrite a 1-2 sentence description of what you observe in the visualization."
  },
  {
    "objectID": "labs/lab-6.html#question-6",
    "href": "labs/lab-6.html#question-6",
    "title": "Lab 6 - Modeling I",
    "section": "Question 6",
    "text": "Question 6\nWhich variable is the strongest individual predictor of parasite similarity between species?\nTo answer this question, begin by fitting a linear regression model to each pair of variables. Save each model output as dt_model, dist_model, BM_model, and prec_model, respectively.\n\ndivergence_time and transformed_parsim\ndistance and transformed_parsim\nBMdiff and transformed_parsim\nprecdiff and transformed_parsim\n\n\nReport the slopes for each of these models. Use proper notation.\nTo answer the question of interest, would it be useful to compare the slopes in each model to choose the variable that is the strongest predictor of parasite similarity? Why or why not?"
  },
  {
    "objectID": "labs/lab-6.html#question-7",
    "href": "labs/lab-6.html#question-7",
    "title": "Lab 6 - Modeling I",
    "section": "Question 7",
    "text": "Question 7\nNow, what if we calculated \\(R^2\\) to help answer our question? To compare the explanatory power of each individual predictor, we will look at \\(R^2\\) between the models. \\(R^2\\) is a measure of how much of the variability in the response variable is explained by the model.\nAs you may have guessed from the name \\(R^2\\) can be calculated by squaring the correlation when we have a simple linear regression model. The correlation \\(r\\) takes values \\(-1\\) to \\(1\\), therefore, \\(R^2\\) takes values 0 to 1. Intuitively, if \\(r=1\\) or \\(−1\\), then \\(R^2=1\\), indicating the model is a perfect fit for the data. If \\(r\\approx0\\) then \\(R^2\\approx0\\), indicating the model is a very bad fit for the data.\nYou can calculate \\(R^2\\) using the .rsquared function. For example, you can calculate \\(R^2\\) for dt_model using the code dt_model.rsquared.\n\nCalculate and report \\(R^2\\) for each model fit in the previous exercise.\nTo answer our question of interest, would it be useful to compare the \\(R^2\\) in each model to choose the variable that is the strongest predictor of parasite similarity? Why or why not?"
  },
  {
    "objectID": "labs/lab-6.html#question-8",
    "href": "labs/lab-6.html#question-8",
    "title": "Lab 6 - Modeling I",
    "section": "Question 8",
    "text": "Question 8\n\nData: For our analysis, we will only be working with data from 2007. Below, filter the data set so only values from the year 2007 are shown. Save this data set as gapminder_07 and use it for the remainder of this exercise and the following.\nVisualization: We are interested in learning more about GDP, and we’ll start with exploring the relationship between life expectancy and GDP. Create two visualizations:\n\n\nScatter plot of gdpPercap vs. lifeExp.\nScatter plot of gdpPercap_log vs. lifeExp, where gdpPercap_log is a new variable you add to the data set by taking the natural log of gdpPercap.\n\nFirst describe the relationship between each pair of the variables. Then, comment on which relationship would be better modeled using a linear model, and explain your reasoning.\n\nModel fitting:\n\n\nFit a linear model predicting log gross domestic product from life expectancy. Display the tidy summary.\n\n\nModel evaluation:\n\n\nCalculate the R-squared of the model using .rsquared from a model.\nInterpret R-squared in the context of the data and the research question."
  },
  {
    "objectID": "labs/lab-6.html#question-9",
    "href": "labs/lab-6.html#question-9",
    "title": "Lab 6 - Modeling I",
    "section": "Question 9",
    "text": "Question 9\nNext, we want to examine if the relationship between GDP and life expectancy that we observed in the previous exercise holds across all continents in our data. We’ll continue to work with logged GDP (gdpPercap_log) and data from 2007.\n\nJustification: Create a scatter plot of gdpPercap_log vs. lifeExp, where the points are colored by continent. Do you think the trend between gdpPercap_log and lifeExp is different for different continents? Justify your answer with specific features of the plot.\nModel fitting and interpretation:\n\n\nRegardless of your answer in part (a), fit an additive model (main effects) that predicts gdpPercap_log from life expectancy and continent (with Africas as the baseline level). Display a tidy summary of the model output.\nInterpret the intercept of the model, making sure that your interpretation is in the units of the original data (not on log scale).\nInterpret the slope of the model, making sure that your interpretation is in the units of the original data (not on log scale).\n\n\nPrediction: Predict the GDP of a country in Asia where the average life expectancy is 70 years old."
  },
  {
    "objectID": "labs/lab-6.html#question-10",
    "href": "labs/lab-6.html#question-10",
    "title": "Lab 6 - Modeling I",
    "section": "Question 10",
    "text": "Question 10\nCommunication is a critical yet often overlooked part of data science. When we engage with our audience and capture their interest, we can ultimately better communicate what we are trying to share.\nPlease watch the following video: Hans Rosling: 200 years in 4 minutes.\nThen, write a paragraph (4-5 sentences) addressing the following:\n\nWhat did you enjoy about the presentation of data? What did you find interesting\nWere there any aspects of the presentation that were hard to follow? If so, what?\nWhat are your general take-aways from this presentation?\nWhat are your general take-aways from how this presentation was given?"
  },
  {
    "objectID": "labs/lab-6.html#submission",
    "href": "labs/lab-6.html#submission",
    "title": "Lab 6 - Modeling I",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in the .ipynb file by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrun all code in your Jupyter notebook\ncommitted and pushed everything to your GitHub repository such that the Git pane in VS Code is empty"
  },
  {
    "objectID": "labs/lab-6.html#grading",
    "href": "labs/lab-6.html#grading",
    "title": "Lab 6 - Modeling I",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/ds-experience.html",
    "href": "labs/ds-experience.html",
    "title": "Data Science Experience",
    "section": "",
    "text": "Important\n\n\n\nCompletion of the DS Experience counts as a replacement for one homework from hw-01 to hw-05.\nThe world of data science is vast and continually growing! The goal of the data science experience assignments is to help you engage with the data science communities outside of the classroom.\nThere will be one optional data science experience assignment that will replace up to one homework grade (among homeworks 01-05). The submission for the statistics experience is due on Friday, April 25 at 11:59pm AZ time on GitHub (you will have an assigned repo). No late work will be accepted for the data science experience. You may submit the data science experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a data science experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit.",
    "crumbs": [
      "Homework",
      "Data Science Experience"
    ]
  },
  {
    "objectID": "labs/ds-experience.html#part-1-experience-data-science-outside-of-the-classroom",
    "href": "labs/ds-experience.html#part-1-experience-data-science-outside-of-the-classroom",
    "title": "Data Science Experience",
    "section": "Part 1: Experience data science outside of the classroom",
    "text": "Part 1: Experience data science outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to data science and they fit in one of the six categories. You can email Professor Chism if there is an activity you’d like to do but you’re not sure if it qualifies for the data science experience.\n\nCategory 1: Attend a talk, conference, or workshop\nAttend an talk, panel, or conference related to data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a data scientist\nTalk with someone who uses data science in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the data science experience. A few suggestions are below:\n\nData Science Imposters Podcast\nData Skeptic Podcast\nFiveThirtyEight Model Talk\nposit::conf 2023 talks\nArizona Data Lab workshops\n\nThis list is not exhaustive. You may listen to other podcasts or watch other data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the data science experience.\n\nFor reference, here are some other podcasts.\n\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. Information for an upcoming data challenge is linked below.\n\nKaggle Competitions\n\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through University of Arizona library.\n\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nList of books about data science ethics",
    "crumbs": [
      "Homework",
      "Data Science Experience"
    ]
  },
  {
    "objectID": "labs/ds-experience.html#part-2-summarize-your-experience",
    "href": "labs/ds-experience.html#part-2-summarize-your-experience",
    "title": "Data Science Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on GitHub.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!",
    "crumbs": [
      "Homework",
      "Data Science Experience"
    ]
  },
  {
    "objectID": "labs/ds-experience.html#submission",
    "href": "labs/ds-experience.html#submission",
    "title": "Data Science Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Data Science Experience assignment on GitHub by Friday, April 25 at 11:59pm AZ time. It must be submitted by the deadline on GitHub to be considered for grading.",
    "crumbs": [
      "Homework",
      "Data Science Experience"
    ]
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "",
    "text": "In this lab you’ll review and get practice with a variety of concepts, methods, and tools you’ve encountered thus far, with a focus on misrepresentation and ethics.\n\n\nBy the end of the lab, you will…\n\nCalculate single event, conditional, and “and” probabilities.\nInterpret probabilities in the context of the problem.\nDisplay a fundamental understanding of Simpson’s Paradox.\nPractice teamwork and collaboration on GitHub.\n\n\n\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-5.html#learning-objectives",
    "href": "labs/lab-5.html#learning-objectives",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "",
    "text": "By the end of the lab, you will…\n\nCalculate single event, conditional, and “and” probabilities.\nInterpret probabilities in the context of the problem.\nDisplay a fundamental understanding of Simpson’s Paradox.\nPractice teamwork and collaboration on GitHub."
  },
  {
    "objectID": "labs/lab-5.html#guidelines",
    "href": "labs/lab-5.html#guidelines",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-5.html#part-1---probability-and-you",
    "href": "labs/lab-5.html#part-1---probability-and-you",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "Part 1 - Probability and you",
    "text": "Part 1 - Probability and you\n\nQuestion 1\nWe use probabilities all the time when making decisions. As a group, provide two real world examples of when you’ve used probability to make decisions in your every day life. Think critically. Be creative."
  },
  {
    "objectID": "labs/lab-5.html#part-2---risk-of-coronary-heart-disease",
    "href": "labs/lab-5.html#part-2---risk-of-coronary-heart-disease",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "Part 2 - Risk of coronary heart disease",
    "text": "Part 2 - Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nQuestion 2\nLoad in the data set called education-disease and answer the following questions below.\n\n\nQuestion 3\nHow many levels of education are there in these data? How many levels of disease are there? Hint: The unique() function might be helpful.\n\n\nQuestion 4\nConvert the data to a two-way table where each cell is the number of people falling into each combination of Disease and Education. Hint: Use groupby and pivot_table. Your answer should be a 4x3 data frame with counts in each cell.\n\n# add code here.\n\nUsing the summary table you created above, answer the remaining questions. You do not have to use Python functions for your calculations, you can use Python as a calculator using the values from the summary table. Make sure to show your work, i.e., instead of reporting just the final answer, use Python to calculate that in a way we can see the counts you’ve used along the way.\n\n\nQuestion 5\nWhat is the probability of a random individual having high school or GED education and not being high risk for cardiovascular disease?\n\n\nQuestion 6\nWhat is the probability that a random individual who is already high risk for cardiovascular disease has a college education?"
  },
  {
    "objectID": "labs/lab-5.html#part-3---bike-rentals",
    "href": "labs/lab-5.html#part-3---bike-rentals",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "Part 3 - Bike rentals",
    "text": "Part 3 - Bike rentals\nBike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. You are tasked to investigate the relationship between the temperature outside and the number of bikes rented in the Washington DC area between the years 2011 and 2022. You will be investigating data for the months June, July, September, and November.\nBelow is a list of variables and their definitions:\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\nseason\nNumerical representation of Spring (2), Summer (3), and Fall (4)\n\n\nyear\nNumerical representation of 2011 (0) or 2012 (1)\n\n\nmonth\nMonth in which data were collected\n\n\nholiday\nIndicator variable for whether data were collected on a holiday (1) or not (0)\n\n\nweekday\nNumerical representation of day of week\n\n\ntemp\nTemperature in Celsius\n\n\ncount\nNumber of bike rentals for that day\n\n\n\n\nQuestion 7\n\nRead in the bike data. Then, create a scatter plot that investigates the relationship between the number of bikes rented and the temperature outside. Include a straight line of best fit to help discuss the discovered relationship. Summarize your findings in 2-3 sentences.\nAnother researcher suggests to look at the relationship between bikes rented and temperature by each of the four months of interest. Recreate your plot in part a, and color the points by month. Include a straight line for each of the four months to help discuss each month’s relationship between bikes rented and temperature. In 3-4 sentences, summarize your findings.\n\nPlease watch the following video on Simpson’s Paradox here. After you do, please answer the following questions.\n\n\nQuestion 8\nIn your own words, summarize Simpson’s Paradox in 2-3 sentences.\n\n\nQuestion 9\nCompare and contrast your findings from part (a) and part (b). What’s different?\n\n\nQuestion 10\nThink critically about your answer to part d. What other context from this study could be creating this paradox? That is, identify a potential confounding"
  },
  {
    "objectID": "labs/lab-5.html#submission",
    "href": "labs/lab-5.html#submission",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in the .ipynb file by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrun all code in your Jupyter notebook\ncommitted and pushed everything to your GitHub repository such that the Git pane in VS Code is empty"
  },
  {
    "objectID": "labs/lab-5.html#grading",
    "href": "labs/lab-5.html#grading",
    "title": "Lab 5 - Probability and Simpson’s Paradox",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/lab-0.html",
    "href": "labs/lab-0.html",
    "title": "Lab 0 - Hello, World and INFO 511!",
    "section": "",
    "text": "This lab will set you up for the computing workflow and give you an opportunity to introduce yourselves to each other and the teaching team.",
    "crumbs": [
      "Homework",
      "Homework 0"
    ]
  },
  {
    "objectID": "labs/lab-0.html#access-python-and-vscode",
    "href": "labs/lab-0.html#access-python-and-vscode",
    "title": "Lab 0 - Hello, World and INFO 511!",
    "section": "Access Python and VSCode",
    "text": "Access Python and VSCode\n\nPython: Follow the directions in https://datasciaz.netlify.app/computing/computing-python.html.",
    "crumbs": [
      "Homework",
      "Homework 0"
    ]
  },
  {
    "objectID": "labs/lab-0.html#create-a-github-account",
    "href": "labs/lab-0.html#create-a-github-account",
    "title": "Lab 0 - Hello, World and INFO 511!",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nGo to https://github.com/ and walk through the steps for creating an account. You do not have to use your U of A email address, but I recommend doing so.1\n\n\n\n\n\n\nNote\n\n\n\nYou’ll need to choose a user name. I recommend reviewing the user name advice at https://happygitwithr.com/github-acct#username-advice before choosing a username.\n\n\n\n\n\n\n\n\nWhat if I already have a GitHub account?\n\n\n\n\n\nIf you already have a GitHub account, you do not need to create a new one for this course. Just log in to that account to make sure you still remember your username and password.",
    "crumbs": [
      "Homework",
      "Homework 0"
    ]
  },
  {
    "objectID": "labs/lab-0.html#set-up-git",
    "href": "labs/lab-0.html#set-up-git",
    "title": "Lab 0 - Hello, World and INFO 511!",
    "section": "Set up Git",
    "text": "Set up Git\nFollow the Setting up Git directions on the course website.",
    "crumbs": [
      "Homework",
      "Homework 0"
    ]
  },
  {
    "objectID": "labs/lab-0.html#footnotes",
    "href": "labs/lab-0.html#footnotes",
    "title": "Lab 0 - Hello, World and INFO 511!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGitHub has some perks for students you can take advantage of later in the course or in your future work, and it helps to have a .edu address to get verified as a student.↩︎",
    "crumbs": [
      "Homework",
      "Homework 0"
    ]
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2 - Data visualization",
    "section": "",
    "text": "This lab will begin your journey honing your data science workflow.\n\n\n\n\n\n\nNote\n\n\n\nThis lab assumes you’ve completed Lab 0 and Lab 1 and doesn’t repeat setup and overview content from those labs. If you have not yet done those, you should go back and review them before starting with this one.\n\n\n\n\nBy the end of the lab, you will…\n\nGain practice writing a reproducible report using Quarto\nContinue practice with version control using Git and GitHub\nBe able to create data visualizations using seaborn\n\n\n\n\n\n\n\nGo to the course organization at github.com/INFO-511-S25 organization on GitHub. Click on the repo with the prefix lab-2. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use HTTPS (this might already be selected by default). Click on the clipboard icon to copy the repo URL.\nIn VS Code, go to File ➛ New Window ➛Clone Git Repository (under Start).\nCopy and paste the URL of your assignment repo into the dialog box Provide repository URL or pick a repository source.\nClick lab-2.ipynb to open the template Jupyter notebook file. This is where you will write up your code and narrative for the lab.\nAlso see similar steps within the Setting up Python page on the course website.\n\n\n\n\n\nIn this lab, we will work with the seaborn library, which is a powerful Python library for creating data visualizations.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nRun All in the document which loads these packages with the import function.\nThe seaborn library is built on top of matplotlib and closely integrated with pandas. It provides a high-level interface for drawing attractive and informative statistical graphics.\n\n\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-2.html#learning-objectives",
    "href": "labs/lab-2.html#learning-objectives",
    "title": "Lab 2 - Data visualization",
    "section": "",
    "text": "By the end of the lab, you will…\n\nGain practice writing a reproducible report using Quarto\nContinue practice with version control using Git and GitHub\nBe able to create data visualizations using seaborn"
  },
  {
    "objectID": "labs/lab-2.html#getting-started",
    "href": "labs/lab-2.html#getting-started",
    "title": "Lab 2 - Data visualization",
    "section": "",
    "text": "Go to the course organization at github.com/INFO-511-S25 organization on GitHub. Click on the repo with the prefix lab-2. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use HTTPS (this might already be selected by default). Click on the clipboard icon to copy the repo URL.\nIn VS Code, go to File ➛ New Window ➛Clone Git Repository (under Start).\nCopy and paste the URL of your assignment repo into the dialog box Provide repository URL or pick a repository source.\nClick lab-2.ipynb to open the template Jupyter notebook file. This is where you will write up your code and narrative for the lab.\nAlso see similar steps within the Setting up Python page on the course website."
  },
  {
    "objectID": "labs/lab-2.html#packages",
    "href": "labs/lab-2.html#packages",
    "title": "Lab 2 - Data visualization",
    "section": "",
    "text": "In this lab, we will work with the seaborn library, which is a powerful Python library for creating data visualizations.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nRun All in the document which loads these packages with the import function.\nThe seaborn library is built on top of matplotlib and closely integrated with pandas. It provides a high-level interface for drawing attractive and informative statistical graphics."
  },
  {
    "objectID": "labs/lab-2.html#guidelines",
    "href": "labs/lab-2.html#guidelines",
    "title": "Lab 2 - Data visualization",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-2.html#part-1",
    "href": "labs/lab-2.html#part-1",
    "title": "Lab 2 - Data visualization",
    "section": "Part 1",
    "text": "Part 1\nLet’s take a trip to the Midwest!\nWe will use the midwest data frame for this lab.\n\nmidwest = pd.read_csv('data/midwest.csv')\n\nThe data contains demographic characteristics of counties in the Midwest region of the United States.\n\n\n\n\n\n\nNote\n\n\n\nIn the future, you will be expected to load the data.\n\n\n\nQuestion 1\nVisualize the distribution of population density of counties using a histogram with sns.histplot() with four separate binwidths: a binwidth of 100, a binwidth of 1,000, a binwidth of 10,000, and a binwidth of 100,000.\nFor example, you can create the first plot with:\n\n# Create the histogram with binwidth = 100\nsns.histplot(midwest['popdensity'], binwidth=100)\nplt.xlabel('Population Density')\nplt.ylabel('Count')\nplt.title('Population Density of Midwestern Counties\\nBinwidth = 100')\nplt.show()\n\nYou will need to make four different histograms. Make sure to set informative titles and axis labels for each of your plots .Then, comment on which binwidth is most appropriate for these data and why.\n\nRun all, commit, and sync your changes to GitHub with the commit message “Added answer for Question 1”.\nMake sure to commit and push all changed files so that your Git pane is empty afterward.\n\n\n\nQuestion 2\nVisualize the distribution of population density of counties again, this time using a boxplot with sns.boxplot(). Make sure to set informative titles and axis labels for your plot. Then, using information as needed from the box plot as well as the histogram from Question 1, describe the distribution of population density of counties and comment on any potential outliers, making sure to identify at least one county that is a clear outlier by name in your narrative and commenting on whether it makes sense to you that this county is an outlier. You can use the data viewer to identify the outliers interactively, you do not have to write code to identify them.\n\nRun all, commit, and sync your changes to GitHub with the commit message “Added answer for Question 2”.\nMake sure to commit and push all changed files so that your Git pane is empty afterward.\n\n\n\nQuestion 3\nCreate a scatterplot of the percentage below poverty (percbelowpoverty on the y-axis) versus percentage of people with a college degree (percollege on the x-axis), where the color and shape of points are determined by state. Make sure to set informative titles, axis, and legend labels for your plot. First, describe the overall relationship between percentage of people with a college degree and percentage below poverty in Midwestern states, making sure to identify at least one county that is a clear outlier by name in your narrative. You can use the data viewer to identify the outliers interactively, you do not have to write code to identify them. Then, comment on whether you can identify how this relationship varies across states.\n\nRun all, commit, and sync your changes to GitHub with the commit message “Added answer for Question 3”.\nMake sure to commit and push all changed files so that your Git pane is empty afterward.\n\n\n\nQuestion 4\nNow, let’s examine the relationship between the same two variables, once again using different colors and shapes to represent each state, and using a separate plot for each state, i.e., with faceting using sns.FacetGrid. In addition to points (sns.scatterplot()). Make sure to set informative titles, axis, and legend labels for your plot. Which plot do you prefer - this plot or the plot in Question 3? Briefly explain your choice.\n\nRun all, commit, and sync your changes to GitHub with the commit message “Added answer for Question 4”.\nMake sure to commit and push all changed files so that your Git pane is empty afterward.\n\n\n\nQuestion 5\nDo some states have counties that tend to be geographically larger than others?\nTo explore this question, create side-by-side boxplots of area (area) of a county based on state (state). How do typical county area sizes compare across states? How do variabilities of county sizes compare across states? Which state has the single largest county? Identify the name of this county. You can use the data viewer to identify it interactively, you do not have to write code.\n\nNow is another good time to Run all, commit, and sync your changes to GitHub with a meaningful commit message.\nOnce again, make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\n\nQuestion 6\nDo some states have a higher percentage of their counties located in a metropolitan area?\nCreate a segmented bar chart with one bar per state and the bar filled with colors according to the value of metro – one color indicating Yes and the other color indicating No for whether a county is considered to be a metro area. The y-axis of the segmented barplot should range from 0 to 1, indicating proportions. Compare the percentage of counties in metro areas across the states based on this plot. Make sure to supplement your narrative with rough estimates of these percentages.\n\n\n\n\n\n\nHint\n\n\n\nFor this question, you should begin with the data wrangling pipeline below. We will learn more about data wrangling in the coming weeks, so this is a mini-preview. This pipeline creates a new variable called metro based on the value of the existing variable called inmetro. If the value of inmetro is equal to 1 ('Yes' if x == 1), it sets the value of metro to “Yes”, and if not, it sets the value of metro to “No”. The resulting data frame is assigned back to midwest, overwriting the existing midwest data frame with a version that includes the new metro variable.\n\nmidwest['metro'] = midwest['inmetro'].apply(lambda x: 'Yes' if x == 1 else 'No')\n\n\n\n\nNow is another good time to Run all, commit, and sync your changes to GitHub with a meaningful commit message.\nAnd once again, make sure to commit and push all changed files so that your Git pane is empty afterward. We keep repeating this because it’s important and because we see students forget to do this. So take a moment to make sure you’re following along with the version control instructions.\n\n\n\nQuestion 7\nRecreate the plot below, and then give it a title. Then, identify at least one county that is a clear outlier in Wisconsin (WI) by name. You can use the data viewer to identify them interactively, you do not have to write code. Comment on the population composition of this county by investigating the percentage of other races living there.\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe seaborn reference for themes in the Python Graph Gallery will be helpful in determining the theme.\nThe size of the points is 50.\nThe transparency (alpha) of the points is 0.5.\nYou can put line breaks in labels with \\n.\n\n\n\n\nRun all, commit, and sync your final changes to GitHub with a meaningful commit message.\n\nMake sure to commit and push all changed files so that your Git pane is empty afterwards."
  },
  {
    "objectID": "labs/lab-2.html#part-2",
    "href": "labs/lab-2.html#part-2",
    "title": "Lab 2 - Data visualization",
    "section": "Part 2",
    "text": "Part 2\nEnough about the Midwest!\nIn this part we will use a new, more recent, and potentially more relevant dataset on counties in Arizona.\nThis dataset is stored in a file called az-county.csv in the data folder of your project/repository.\nYou can read this file into Python with the following code:\n\naz_county = pd.read_csv('data/az-county.csv')\n\nThis will read the CSV (comma separated values) file from the data folder and store the dataset as a data frame called az_county in Python.\nThe variables in the dataset and their descriptions are as follows:\n\ncounty: Name of county.\nstate_abb: State abbreviation (AZ).\nstate_name: State name (Arizona).\nland_area_m2: Land area of county in meters-squared, based on the 2020 census.\nland_area_mi2: Land area of county in miles-squared, based on the 2020 census.\npopulation: Population of county, based on the 2020 census.\ndensity: Population density calculated as population divided by land area in miles-squared.\n\nIn addition to being more recent and more relevant, this dataset is also more complete in the sense that we know the units of population density: people per mile-squared!\n\nQuestion 8\nFirst, guess what the relationship between population density and land area might be – positive? negative? no relationship?\nThen, make a scatter plot of population density (density on the y-axis) vs. land area in miles-squared (land_area_mi2 on the x-axis). Make sure to set an informative title and axis labels for your plot.Describe the relationship. Was your guess correct?\n\n\nQuestion 9\nNow make a scatter plot of population density (density on the y-axis) vs. land area in meters-squared (land_area_m2 on the x-axis). Make sure to set an informative title and axis labels for your plot. Comment on how this scatterplot compares to the one in Exercise 8 — is the relationship displayed same or different. Explain why.\n\n\nQuestion 10\nWhat are some insights that you found from the az_county data from Questions 8-9?"
  },
  {
    "objectID": "labs/lab-2.html#submission",
    "href": "labs/lab-2.html#submission",
    "title": "Lab 2 - Data visualization",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in the .ipynb file by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrun all code in your Jupyter notebook\ncommitted and pushed everything to your GitHub repository such that the Git pane in VS Code is empty"
  },
  {
    "objectID": "labs/lab-2.html#grading",
    "href": "labs/lab-2.html#grading",
    "title": "Lab 2 - Data visualization",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "In this lab you’ll build the data wrangling and visualization skills you’ve developed so far and data tidying and joining to your repertoire.\n\n\n\n\n\n\nNote\n\n\n\nThis lab assumes you’ve completed the labs so far and doesn’t repeat setup and overview content from those labs. If you have not yet done those, you should go back and review the previous labs before starting on this one.\n\n\n\n\nBy the end of the lab, you will…\n\nBe able to pivot/reshape data using pandas\nContinue developing your data wrangling skills using pandas\nBuild on your mastery of data visualizations using matplotlib and seaborn\nGet more experience with data science workflow using Python, Jupyter, Git, and GitHub\nFurther your reproducible authoring skills with Jupyter Notebooks\nImprove your familiarity with version control using Git and GitHub\n\n\n\n\n\n\n\nGo to the course organization at github.com/INFO-511-S25 organization on GitHub. Click on the repo with the prefix lab-3. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use HTTPS (this might already be selected by default). Click on the clipboard icon to copy the repo URL.\nIn VS Code, go to File ➛ New Window ➛Clone Git Repository (under Start).\nCopy and paste the URL of your assignment repo into the dialog box Provide repository URL or pick a repository source.\nClick lab-3.ipynb to open the template Jupyter notebook file. This is where you will write up your code and narrative for the lab.\nAlso see similar steps within the Setting up Python page on the course website.\n\n\n\n\n\nIn this lab we will work with the pandas, matplotlib, and seaborn packages for data analysis and visualization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nRun All in the document which loads these packages with the import function.\n\n\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-3.html#learning-objectives",
    "href": "labs/lab-3.html#learning-objectives",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "By the end of the lab, you will…\n\nBe able to pivot/reshape data using pandas\nContinue developing your data wrangling skills using pandas\nBuild on your mastery of data visualizations using matplotlib and seaborn\nGet more experience with data science workflow using Python, Jupyter, Git, and GitHub\nFurther your reproducible authoring skills with Jupyter Notebooks\nImprove your familiarity with version control using Git and GitHub"
  },
  {
    "objectID": "labs/lab-3.html#getting-started",
    "href": "labs/lab-3.html#getting-started",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "Go to the course organization at github.com/INFO-511-S25 organization on GitHub. Click on the repo with the prefix lab-3. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use HTTPS (this might already be selected by default). Click on the clipboard icon to copy the repo URL.\nIn VS Code, go to File ➛ New Window ➛Clone Git Repository (under Start).\nCopy and paste the URL of your assignment repo into the dialog box Provide repository URL or pick a repository source.\nClick lab-3.ipynb to open the template Jupyter notebook file. This is where you will write up your code and narrative for the lab.\nAlso see similar steps within the Setting up Python page on the course website."
  },
  {
    "objectID": "labs/lab-3.html#packages",
    "href": "labs/lab-3.html#packages",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "In this lab we will work with the pandas, matplotlib, and seaborn packages for data analysis and visualization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nRun All in the document which loads these packages with the import function."
  },
  {
    "objectID": "labs/lab-3.html#guidelines",
    "href": "labs/lab-3.html#guidelines",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-3.html#part-1",
    "href": "labs/lab-3.html#part-1",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Part 1",
    "text": "Part 1\nInflation across the world\nFor this part of the analysis you will work with inflation data from various countries in the world over the last 30 years.\n\ncountry_inflation = pd.read_csv(\"data/country-inflation.csv\")\n\n\nQuestion 1\nGet to know the data.\n\ninfo() at the country_inflation data frame and answer the following questions based on the output. How many rows does country_inflation have and what does each row represent? How many columns does country_inflation have and what does each column represent?\nDisplay a list of the countries included in the dataset.\n\n\n\n\n\n\n\nTip\n\n\n\nA function that can be useful for part (b) is unique(). Check out its documentation for examples of usage.\n\n\n\n\nQuestion 2\nWhich countries had the top three highest inflation rates in 2021? Your output should be a data frame with two columns, country and 2021, with inflation rates in descending order, and three rows for the top three countries. Briefly comment on how the inflation rates for these countries compare to the inflation rate for United States in that year.\n\n\nQuestion 3\nCopy the inf_ratio data set and perform the following:\n\ncalculate the ratio of the inflation in 2021 and inflation in 1993 for each country and store this information in a new column called inf_ratio,\narrange the data frame in decreasing order of inf_ratio, and\nselect the variables country and inf_ratio to display as the result.\n\nDo not save this new variable in inf_ratio, but instead keep it within the copy.\nWhich country’s inflation change is the largest over this time period? Did inflation increase of decrease between 1993 and 2021 in this country?\n\n\nQuestion 4\nReshape (pivot) country_inflation such that each row represents a country/year combination, with columns country, year, and annual_inflation. Then, display the resulting data frame and state how many rows and columns it has.\nRequirements:\n\nYour code must use one of melt() or pivot_table(). There are other ways you can do this reshaping move in Python, but this question requires solving this problem by pivoting.\nThe resulting DataFrame must be saved as something other than country_inflation so you (1) can refer to this DataFrame later in your analysis and (2) do not overwrite country_inflation. Use a short but informative name.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe remaining questions in Part 1 require the use of the pivoted data frame from Question 4.\n\n\n\n\nQuestion 5\nUse a separate copy of the data to answer each of the following questions.\nRequirement: Your code must use the query() function for each part, not sort_values().\n\nWhat is the highest inflation rate observed between 1993 and 2021? The output of this copy should be a data frame with one row and three columns. In addition to code and output, your response should include a single sentence stating the country and year.\nWhat is the lowest inflation rate observed between 1993 and 2021? The output of this copy should be a data frame with one row and three columns. In addition to code and output, your response should include a single sentence stating the country and year.\nPutting (a) and (b) together: What are the highest and the lowest inflation rates observed between 1993 and 2021? The output of the copy should be a data frame with two rows and three columns.\n\n\n\nQuestion 6\na. Create a vector called countries_of_interest which contains the names of up tp five countries you want to visualize the inflation rates for over the years. For example, if these countries are India and United States, you can express this as follows:\n\ncountries_of_interest = [\"India\", \"United States\"]\n\nIf they are India, United States, and China, you can express this as follows:\n\ncountries_of_interest = [\"India\", \"United States\", \"China (People's Republic of)\"]\n\nSo on and so forth… Then, in 1-2 sentences, state why you chose these countries.\n\n\n\n\n\n\nNote\n\n\n\nYour countries_of_interest should consist of no more than five countries. Make sure that the spelling of your countries matches how they appear in the dataset.\n\n\nb. In a separate data copy, filter your reshaped dataset to include only the countries_of_interest from part (a), and save the resulting data frame with a new name so you (1) can refer to this data frame later in your analysis and (2) do not overwrite the data frame you’re starting with. Use a short but informative name. Then, in a new copy, find the unique() countries in the data frame you created.\n\n\n\n\n\n\nTip\n\n\n\nThe number of distinct countries in the filtered data frame you created in part (b) should equal the number of countries you chose in part (a). If it doesn’t, you might have misspelled a country name or made a mistake in how to filter for these countries. Go back and check your code.\n\n\n\n\nQuestion 7\nUsing your data frame from the previous question, create a plot of annual inflation vs. year for these countries. Then, in a few sentences, describe the patterns you observe in the plot, particularly focusing on anything you find surprising or not surprising, based on your knowledge (or lack thereof) of these countries economies.\nRequirements for the plot:\n\nData should be represented with points as well as lines connecting the points for each country.\nEach country should be represented by a different color line and different color and shape points.\nAxes and legend should be properly labeled.\nThe plot should have an appropriate title (and optionally a subtitle).\nPlot should be customized in at least one way – you could use a different than default color scale, or different than default theme, or some other customization.\n\n\nIf you haven’t yet done so, now is a good time to run all, commit, and sync.\n\nMake sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-3.html#part-2",
    "href": "labs/lab-3.html#part-2",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Part 2",
    "text": "Part 2\nInflation in the US\nThe OECD defines inflation as follows:\n\nInflation is a rise in the general level of prices of goods and services that households acquire for the purpose of consumption in an economy over a period of time.\nThe main measure of inflation is the annual inflation rate which is the movement of the Consumer Price Index (CPI) from one month/period to the same month/period of the previous year expressed as percentage over time.\nSource: OECD CPI FAQ\n\nCPI is broken down into 12 divisions such as food, housing, health, etc. Your goal in this part is to create another time series plot of annual inflation, this time for US only.\nThe data you will need to create this visualization is spread across two files:\n\nus-inflation.csv: Annual inflation rate for the US for 12 CPI divisions. Each division is identified by an ID number.\ncpi-divisions.csv: A “lookup table” of CPI division ID numbers and their descriptions.\n\nLet’s load both of these files.\n\nus_inflation = pd.read_csv(\"data/us-inflation.csv\")\ncpi_divisions = pd.read_csv(\"data/cpi-divisions.csv\")\n\n\nQuestion 8\na. How many columns and how many rows does the us_inflation dataset have? What are the variables in it? Add a brief (1-2 sentences) narrative summarizing this information.\nb. How many columns and how many rows does the cpi_divisions dataset have? What are the variables in it? Add a brief (1-2 sentences) narrative summarizing this information.\nc. Create a new dataset by joining the us_inflation dataset with the cpi_division_id dataset.\n\nDetermine which type of join is the most appropriate one and use that.\nNote that the two datasets don’t have a common variable. Review the help for the join functions to determine how to use the on argument when the names of the variables that the datasets should be joined by are different.\nUse a short but informative name for the joined dataset, and do not overwrite either of the datasets that go into creating it.\n\nThen, find the number of rows and columns of the resulting dataset and report the names of its columns. Add a brief (1-2 sentences) narrative summarizing this information.\n\n\nQuestion 9\na. Create a vector called divisions_of_interest which contains the descriptions or IDs of CPI divisions you want to visualize. Your divisions_of_interest should consist of no more than five divisions. If you’re using descriptions, make sure that the spelling of your divisions matches how they appear in the dataset. Then, in 1-2 sentences, state why you chose these divisions.\n\n\n\n\n\n\nTip\n\n\n\nRefer back to the guidance provided in Question 6 if you’re not sure how to create this vector.\n\n\nb. In a copy of the data, filter your reshaped dataset to include only the divisions_of_interest from part (a), and save the resulting data frame with a new name so you (1) can refer to this data frame later in your analysis and (2) do not overwrite the data frame you’re starting with. Use a short but informative name. Then, in a new copy of the data, find the unique() divisions in the data frame you created.\n\n\nQuestion 10\nUsing your data frame from the previous question, create a plot of annual inflation vs. year for these divisions. Then, in a few sentences, describe the patterns you observe in the plot, particularly focusing on anything you find surprising or not surprising, based on your knowledge (or lack thereof) of inflation rates in the US over the last decade.\n\nData should be represented with points as well as lines connecting the points for each division.\nEach division should be represented by a different color line and different color and shape points.\nAxes and legend should be properly labeled.\nThe plot should have an appropriate title (and optionally a subtitle).\nPlot should be customized in at least one way – you could use a different than default color scale, or different than default theme, or some other customization.\nIf your legend has labels that are too long, you can try moving the legend to the bottom and stack the labels vertically. Hint: The legend argument of the ax.legend() method will be useful.\n\n\nplt.figure(figsize=(10, 6))\nsns.___(...)\nplt.___(...)\nplt.___(\"Year\")\nplt.ylabel(...)\nplt.___(...)\nplt.show()\n\n\nIf you haven’t yet done so since Part 1, now is a good time to run all, commit, and sync.\n\nMake sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-3.html#submission",
    "href": "labs/lab-3.html#submission",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in the .ipynb file by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrun all code in your Jupyter notebook\ncommitted and pushed everything to your GitHub repository such that the Git pane in VS Code is empty"
  },
  {
    "objectID": "labs/lab-3.html#grading",
    "href": "labs/lab-3.html#grading",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4 - Ethics",
    "section": "",
    "text": "In this lab you’ll review and get practice with a variety of concepts, methods, and tools you’ve encountered thus far, with a focus on misrepresentation and ethics.\n\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-4.html#guidelines",
    "href": "labs/lab-4.html#guidelines",
    "title": "Lab 4 - Ethics",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-4.html#question-1",
    "href": "labs/lab-4.html#question-1",
    "title": "Lab 4 - Ethics",
    "section": "Question 1",
    "text": "Question 1\nThe following chart was shared by @GraphCrimes on X/Twitter on September 3, 2022.\n\n\n\n\n\n\nWhat is misleading about this graph?\nSuppose you wanted to recreate this plot, with improvements to avoid its misleading pitfalls from part (a). You would obviously need the data from the survey in order to be able to do that. How many observations would this data have? How many variables (at least) should it have, and what should those variables be?\nLoad the data for this survey from data/survation.csv. Confirm that the data match the percentages from the visualization. That is, calculate the percentages of public sector, private sector, don’t know for each of the services and check that they match the percentages from the plot."
  },
  {
    "objectID": "labs/lab-4.html#question-2",
    "href": "labs/lab-4.html#question-2",
    "title": "Lab 4 - Ethics",
    "section": "Question 2",
    "text": "Question 2\nCreate an improved version of the visualization. Your improved visualization:\n\nshould also be a stacked bar chart with services on the y-axis, presented in the same order as the original plot, and services to create the segments of the plot, and presented in the same order as the original plot\nshould have the same legend location\nshould have the same title and caption\ndoes not need to have a bolded title or a gray background\n\nHow does the improved visualization look different than the original? Does it send a different message at a first glance?\n\n\n\n\n\n\nTip\n\n\n\nUse \\n to add a line break to your title. And note that since the title is very long, it might run off the page in your code. That’s ok!\nAdditionally, the colors used in the plot are #808080, #FF3205, and #006697.\n\n\n\nRun all, commit, and sync.\n\nMake sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-4.html#question-3",
    "href": "labs/lab-4.html#question-3",
    "title": "Lab 4 - Ethics",
    "section": "Question 3",
    "text": "Question 3\nCalculate the mean of x, mean of y, standard deviation of x, standard deviation of y, and the correlation between x and y for each level of the dataset variable. Then, in 1-2 sentences, comment on how these summary statistics compare across groups (datasets).\n\n\n\n\n\n\nTip\n\n\n\nThere are 13 groups but DataFrames only print out 10 rows by default. Add print(n = 13) as the last step to display all rows."
  },
  {
    "objectID": "labs/lab-4.html#question-4",
    "href": "labs/lab-4.html#question-4",
    "title": "Lab 4 - Ethics",
    "section": "Question 4",
    "text": "Question 4\nCreate a scatterplot of y versus x and color and facet it by dataset. Then, in 1-2 sentences, how these plots compare across groups (datasets). How does your response in this question compare to your response to the previous question and what does this say about using visualizations and summary statistics when getting to know a dataset?\n\nRun all, commit, and sync.\n\nMake sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-4.html#question-5",
    "href": "labs/lab-4.html#question-5",
    "title": "Lab 4 - Ethics",
    "section": "Question 5",
    "text": "Question 5\n\nFill in the code below to create a two-way table that summarizes these data.\n\n\nsurvey_counts = pd.DataFrame({\n  'age': [],\n  'vote': [],\n  'n': []\n  })\n\nsurvey_pivot = survey_counts.pivot(index='___', columns='___', values='___')\nsurvey_pivot\n\nFor parts b-d below, use your response starting with survey_counts, calculate the desired proportions, and make sure the result is an ungrouped data frame with a column for relevant counts, a column for relevant proportions, and a column for the groups you’re interested in.\n\nCalculate the proportions of 18-49 year olds and 50+ year olds in this sample.\nCalculate the proportions of those who want to vote for Donald Trump, Joe Biden, and those who are undecided in this sample.\nCalculate the proportions of individuals in this sample who are planning to vote for each of the candidates or are undecided among those who are 18-49 years old as well as among those who are 50+ years old."
  },
  {
    "objectID": "labs/lab-4.html#question-6",
    "href": "labs/lab-4.html#question-6",
    "title": "Lab 4 - Ethics",
    "section": "Question 6",
    "text": "Question 6\n\nRe-create the following visualization that displays relationship between age and vote.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe colors used in the plot are \"#E81B23\", \"#0015BC\", and \"#808080\". The theme is sns.set_theme(style=\"whitegrid\")\n\n\n\nBased on your calculations so far, as well as your visualization, write 1-3 sentences that describe the relationship, in this sample, between age and plans for presidential vote.\n\n\nRun all, commit, and sync.\n\nMake sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-4.html#question-7",
    "href": "labs/lab-4.html#question-7",
    "title": "Lab 4 - Ethics",
    "section": "Question 7",
    "text": "Question 7\nFor each of the following websites, first determine whether you’re allowed to scrape data from them using tools we’ve learned in this course.\nThen, read (the relevant portions of their) Terms of Use/Service.\n\nESPN: https://www.espn.com / https://disneytermsofuse.com/english/#License-Grant-and-Restrictions\nX/Twitter: https://twitter.com / https://twitter.com/en/tos\nRotten Tomatoes: https://www.rottentomatoes.com / https://www.rottentomatoes.com/policies/terms-of-use\n\nFinally, summarize your findings about whether you can or cannot scrape data from these websites in 1 sentence for each website. Additionally, quote the relevant sentence(s) from the Terms of Use/Service.\n\n\n\n\n\n\nHint\n\n\n\nIn the Terms of Use/Service documents, it might be productive to search for keywords like “scrape” or “scraping” to find the relevant portions."
  },
  {
    "objectID": "labs/lab-4.html#question-8",
    "href": "labs/lab-4.html#question-8",
    "title": "Lab 4 - Ethics",
    "section": "Question 8",
    "text": "Question 8\nOne current ethical discussion in data science involves the training of “Large Language Models” such as ChatGPT. These models are trained using massive corpora (document sets) that include large amounts of work that is covered under copyright law. Read the following two articles:\n\nDo Large Language Models Violate Copyright Law?\nReexamining “Fair Use” in the Age of AI\n\nWrite a short paragraph (maximum 8 sentences) discussing the arguments on both sides of the discussion over copyright in training large language models."
  },
  {
    "objectID": "labs/lab-4.html#question-9",
    "href": "labs/lab-4.html#question-9",
    "title": "Lab 4 - Ethics",
    "section": "Question 9",
    "text": "Question 9\nAnother major ethical discussion in data science resolves around discriminatory biases in machine learning models. These biases can have real-world impacts in lending, criminal justice, hiring, and more. Many of these algorithms are so-called “black boxes”, meaning the exact process they take from input to output is unclear. Read the following articles:\n\nAmazon scraps secret AI recruiting tool that showed bias against\nThe Atlantic: The False Promise of Risk Assessment\n\nWrite a short paragraph (maximum 8 sentences) discussing the nature of biases in machine learning and in datasets, and any possible solutions that could help limit those biases."
  },
  {
    "objectID": "labs/lab-4.html#question-10",
    "href": "labs/lab-4.html#question-10",
    "title": "Lab 4 - Ethics",
    "section": "Question 10",
    "text": "Question 10\nTo complete this exercise you will first need to watch the documentary Coded Bias. To do so, you either need to be on the Duke network or connected to the Duke VPN. Then go to U of A Libraries Permalink and click on “View Online”. Once you watch the video, write a reflection in 2-5 bullet points highlighting at least one thing that you already knew about (from the course prep materials) and at least one thing you learned from the movie as well as any other aspects of the documentary that you found interesting / enlightening.\n\n\n\n\n\n\nImportant\n\n\n\nThis question requires no code, only narrative. Remember that, based on the syllabus, you may not use generative AI tools (e.g., Chat GPT) to write narrative on assignments.\n\n\n\nRun all, commit, and sync one last time.\n\nMake sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-4.html#submission",
    "href": "labs/lab-4.html#submission",
    "title": "Lab 4 - Ethics",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in the .ipynb file by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrun all code in your Jupyter notebook\ncommitted and pushed everything to your GitHub repository such that the Git pane in VS Code is empty"
  },
  {
    "objectID": "labs/lab-4.html#grading",
    "href": "labs/lab-4.html#grading",
    "title": "Lab 4 - Ethics",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/lab-4.html#footnotes",
    "href": "labs/lab-4.html#footnotes",
    "title": "Lab 4 - Ethics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFull survey results can be found at https://www.surveyusa.com/client/PollReport.aspx?g=300d50f5-303b-4652-b59e-6fbf1b87e24a.↩︎"
  },
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7 - Modeling II",
    "section": "",
    "text": "In this lab you’ll practice machine learning modeling and then venture on to model validation and quantifying uncertainty.\n\n\nIn this lab we will work with the pandas and sklearn modules.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment.\n\n\nAdditionally, if you’re using functions that are not introduced in the course materials, you must cite your sources.\n\n\n\n\n\n\nImportant\n\n\n\nFailure to cite outside resources used, including Large Language Models like Chat GPT, is a violation of the University of Arizona Code of Academic Integrity and will be treated as such."
  },
  {
    "objectID": "labs/lab-7.html#packages",
    "href": "labs/lab-7.html#packages",
    "title": "Lab 7 - Modeling II",
    "section": "",
    "text": "In this lab we will work with the pandas and sklearn modules.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "labs/lab-7.html#guidelines",
    "href": "labs/lab-7.html#guidelines",
    "title": "Lab 7 - Modeling II",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to Run all, commit, and sync your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment.\n\n\nAdditionally, if you’re using functions that are not introduced in the course materials, you must cite your sources.\n\n\n\n\n\n\nImportant\n\n\n\nFailure to cite outside resources used, including Large Language Models like Chat GPT, is a violation of the University of Arizona Code of Academic Integrity and will be treated as such."
  },
  {
    "objectID": "labs/lab-7.html#question-1",
    "href": "labs/lab-7.html#question-1",
    "title": "Lab 7 - Modeling II",
    "section": "Question 1",
    "text": "Question 1\na. Fit a logistic regression model predicting spam from exclaim_mess (the number of exclamation points in the email message). Then, display the tidy output of the model.\nb. Using this model and the Python function .predict_proba(), predict the probability the email is spam if it contains 10 exclamation points."
  },
  {
    "objectID": "labs/lab-7.html#question-2",
    "href": "labs/lab-7.html#question-2",
    "title": "Lab 7 - Modeling II",
    "section": "Question 2",
    "text": "Question 2\na. Fit another logistic regression model predicting spam from exclaim_mess, winner (indicating whether “winner” appeared in the email), and urgent_subj (whether the word “urgent” is in the subject of the email). Then, display the output of the model intercept and coefficients.\nb. Using this model, predict spam / not spam for all emails in the email dataset.\nc. Using your data frame from the previous part, determine, in a single pipeline, the numbers of emails:\n\nthat are labelled as spam that are actually spam\nthat are not labelled as spam that are actually spam\nthat are labelled as spam that are actually not spam\nthat are not labelled as spam that are actually not spam\n\nd. In a single pipeline, calculate the false positive and false negative rates.\nIn addition to these numbers showing in your Python output, you must write a sentence that explicitly states and identifies the two rates."
  },
  {
    "objectID": "labs/lab-7.html#question-3",
    "href": "labs/lab-7.html#question-3",
    "title": "Lab 7 - Modeling II",
    "section": "Question 3",
    "text": "Question 3\na. Fit another logistic regression model predicting spam from exclaim_mess and another variable you think would be a good predictor. Provide a 1-sentence justification for why you chose this variable. Display the tidy output of the model.\nb. Using this model, predict spam / not spam for all emails in the email dataset.\nc. Using your data frame from the previous part, determine, in a single pipeline, the numbers of emails:\n\nthat are labelled as spam that are actually spam\nthat are not labelled as spam that are actually spam\nthat are labelled as spam that are actually not spam\nthat are not labelled as spam that are actually not spam\n\nd. In a single pipeline, calculate the false positive and false negative rates.\nIn addition to these numbers showing in your Python output, you must write a sentence that explicitly states and identifies the two rates.\ne. Based on the false positive and false negatives rates of this model, comment, in 1-2 sentences, on which model is preferable and why."
  },
  {
    "objectID": "labs/lab-7.html#question-4",
    "href": "labs/lab-7.html#question-4",
    "title": "Lab 7 - Modeling II",
    "section": "Question 4",
    "text": "Question 4\n\nExplain in your own words what k-fold cross validation is and why it is useful.\nImplement 5-fold cross validation on the logistic regression model from Question 1a predicting spam from exclaim_mess. Use the cross_val_score from sklearn to assist with this.\nSummarize the results of your cross validation. What is the average accuracy and standard deviation across the folds?"
  },
  {
    "objectID": "labs/lab-7.html#question-5",
    "href": "labs/lab-7.html#question-5",
    "title": "Lab 7 - Modeling II",
    "section": "Question 5",
    "text": "Question 5\n\nImplement 10-fold cross validation on the logistic regression model from Question 2a predicting spam from exclaim_mess, winner, and urgent_subj.\nSummarize the results of your cross validation. Compare the average accuracy and standard deviation across the folds to the results from Question 4. What do you observe?\nBased on the cross-validation results, which model do you think performs better in terms of generalizability? Justify your answer."
  },
  {
    "objectID": "labs/lab-7.html#question-6",
    "href": "labs/lab-7.html#question-6",
    "title": "Lab 7 - Modeling II",
    "section": "Question 6",
    "text": "Question 6\n\nPlot the accuracy scores of each fold for both the 5-fold and 10-fold cross-validation.\nDescribe any patterns you observe in the plot."
  },
  {
    "objectID": "labs/lab-7.html#question-7",
    "href": "labs/lab-7.html#question-7",
    "title": "Lab 7 - Modeling II",
    "section": "Question 7",
    "text": "Question 7\nRead the data and then explore attributes of bookings and summarize your findings in 5 bullet points. You must provide a visualization or summary supporting each finding.\n\n\n\n\n\n\nNote\n\n\n\nThis is not meant to be an exhaustive exploration. We anticipate a wide variety of answers to this question."
  },
  {
    "objectID": "labs/lab-7.html#question-8",
    "href": "labs/lab-7.html#question-8",
    "title": "Lab 7 - Modeling II",
    "section": "Question 8",
    "text": "Question 8\nUsing these data, we will try to answer the following question:\n\nDo we expect reservations earlier in the month or later in the month to be cancelled?\n\n\nExploration: In a single pipeline, calculate the mean arrival date (arrival_date_day_of_month) for both booking that were cancelled and that were not cancelled.\nJustification: In your own words, explain why we can not fit a linear model to model the relationship between if a hotel reservation was cancelled and the day of month for the booking.\nModel fitting and interpretation:\n\nFit the appropriate model and display a tidy summary of the model output.\nInterpret the slope coefficient in context of the data and the research question.\n\nPredicted: Calculate the probability that the hotel reservation is cancelled if it the arrival date date is on the 26th of the month. Based on this probability, would you predict this booking would be cancelled or not cancelled. Explain your reasoning for your classification."
  },
  {
    "objectID": "labs/lab-7.html#question-9",
    "href": "labs/lab-7.html#question-9",
    "title": "Lab 7 - Modeling II",
    "section": "Question 9",
    "text": "Question 9\nYour task is to make the following plot as ugly and as ineffective as possible. Change colors, axes, fonts, theme, or anything else you can think of in the code chunk below. You can also search online for other themes, fonts, etc. that you want to tweak. Try to make it as ugly as possible, the sky is the limit!\nIn 2-3 sentences, explain why the plot you created is ugly (to you, at least) and ineffective.\n\npenguins = pd.read_csv(\"data/penguins.csv\")\n\nsns.set(style=\"darkgrid\")\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\nplt.show()"
  },
  {
    "objectID": "labs/lab-7.html#question-10",
    "href": "labs/lab-7.html#question-10",
    "title": "Lab 7 - Modeling II",
    "section": "Question 10",
    "text": "Question 10\nIn 2016, the GSS added a new question on harassment at work. The question is phrased as the following.\nOver the past five years, have you been harassed by your superiors or co-workers at your job, for example, have you experienced any bullying, physical or psychological abuse?\nAnswers to this question are stored in the harass5 variable in our data set.\n\nCreate a subset of the data that only contains Yes and No answers for the harassment question. How many respondents chose each of these answers?\nDescribe how bootstrapping can be used to estimate the proportion of all Americans who have been harassed by their superiors or co-workers at their job.\nCalculate a 95% bootstrap confidence interval for the proportion of Americans who have been harassed by their superiors or co-workers at their job. Use 1000 iterations when creating your bootstrap distribution. Interpret this interval in context of the data."
  },
  {
    "objectID": "labs/lab-7.html#submission",
    "href": "labs/lab-7.html#submission",
    "title": "Lab 7 - Modeling II",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in the .ipynb file by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrun all code in your Jupyter notebook\ncommitted and pushed everything to your GitHub repository such that the Git pane in VS Code is empty"
  },
  {
    "objectID": "labs/lab-7.html#grading",
    "href": "labs/lab-7.html#grading",
    "title": "Lab 7 - Modeling II",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/workflow.html",
    "href": "labs/workflow.html",
    "title": "Computing workflow",
    "section": "",
    "text": "This webpage will introduce you to the course computing workflow. The main goal is to reinforce our demo of Python and VS Code, which we will be using throughout the course both to learn the data science concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nPython is the name of the programming language itself and VS Code is a convenient interface, commonly referred to as an integrated development environment or an IDE, for short.\n\n\nAn additional goal is to reinforce Git and GitHub, the version control, web hosting, and collaboration systems that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nAs the assignments progress, you are encouraged to explore beyond what the assignments dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in Python. Today we begin with the fundamental building blocks of Python and VS Code: the interface, reading in data, and basic commands.\n\n\n\n\n\n\nWarning\n\n\n\nThis page assumes that you have already completed Homework 0. If you have not, please go back and do that first before proceeding.\n\n\n\n\n\n\n\nGo to the course GitHub classroom. Click on the desired repo. It contains the starter documents you need to complete the assignment.\nClick on the green CODE button, select Use HTTPS (this might already be selected by default). Click on the clipboard icon to copy the repo URL.\nIn VS Code, go to File ➛ New Window ➛Clone Git Repository (under Start).\nCopy and paste the URL of your assignment repo into the dialog box Provide repository URL or pick a repository source.\nClick ds.py to open the template Python script. This is where you will write up your code for the assignment.\nAlso see similar steps within the Setting up Python page on the course website.\n\n\n\n\nBelow are the components of the VS Code IDE.\n\n\n\n\n\n\n\n\nNow, go to the Git pane in your VS Code window (third icon on the left - above there is a blue (2) next to it).\nIf you have made changes to your Python script (.py) file, you should see it listed here. Click on it to see the differences. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, write a meaningful commit message (for instance, “update author name”) in the Message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\n\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you normally must have staged your commit to be pushed. click on Push. On VS Code however, the staging step has been done for you.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete an assignment and other assignments in this course.",
    "crumbs": [
      "Homework",
      "Computing workflow"
    ]
  },
  {
    "objectID": "labs/workflow.html#getting-started",
    "href": "labs/workflow.html#getting-started",
    "title": "Computing workflow",
    "section": "",
    "text": "Go to the course GitHub classroom. Click on the desired repo. It contains the starter documents you need to complete the assignment.\nClick on the green CODE button, select Use HTTPS (this might already be selected by default). Click on the clipboard icon to copy the repo URL.\nIn VS Code, go to File ➛ New Window ➛Clone Git Repository (under Start).\nCopy and paste the URL of your assignment repo into the dialog box Provide repository URL or pick a repository source.\nClick ds.py to open the template Python script. This is where you will write up your code for the assignment.\nAlso see similar steps within the Setting up Python page on the course website.\n\n\n\n\nBelow are the components of the VS Code IDE.\n\n\n\n\n\n\n\n\nNow, go to the Git pane in your VS Code window (third icon on the left - above there is a blue (2) next to it).\nIf you have made changes to your Python script (.py) file, you should see it listed here. Click on it to see the differences. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, write a meaningful commit message (for instance, “update author name”) in the Message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\n\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you normally must have staged your commit to be pushed. click on Push. On VS Code however, the staging step has been done for you.",
    "crumbs": [
      "Homework",
      "Computing workflow"
    ]
  },
  {
    "objectID": "labs/workflow.html#guidelines",
    "href": "labs/workflow.html#guidelines",
    "title": "Computing workflow",
    "section": "",
    "text": "Note\n\n\n\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete an assignment and other assignments in this course.",
    "crumbs": [
      "Homework",
      "Computing workflow"
    ]
  },
  {
    "objectID": "labs/codespaces.html",
    "href": "labs/codespaces.html",
    "title": "Working with Codespaces",
    "section": "",
    "text": "Navigate to the GitHub repository.\nClick on the green Code button, and select Codespaces.\n\nIf a Codespace environment isn’t set up yet, click Create codespace on main.\nThis will initialize a cloud-based development environment pre-configured for this repository.\n\n\n\n\n\nOnce the Codespace is launched:\n\nVerify Python Version:\n\n\nOpen the terminal in Codespaces and run:\n\npython3 --version\n\nEnsure it outputs Python 3.x. If not, install Python 3 using:\n\nsudo apt-get install python3\n\nInstall Dependencies:\n\n\nInstall the required Python packages listed in requirements.txt:\n\npip install -r requirements.txt\n\nVerify the Installation:\n\n\nRun pip freeze to confirm that the dependencies are installed.\n\n\n\n\n\nRunning the Application:\n\n\nThe main script seems to be ds.py. You can run it with:\n\npython3 ds.py\n\nRunning Tests:\n\n\nThe test file test_ds.py is located in the root directory.\nRun tests using unittest:\n\npython3 -m unittest test_ds.py\n\nAlternatively, if pytest is installed:\n\npytest test_ds.py\n\nLinting Code:\n\n\nUse pylint to check the quality of ds.py:\n\npylint ds.py\n\n\n\n\nEditing the Code:\n\n\nOpen files (like ds.py or test_ds.py) using the built-in VS Code editor in Codespaces.\nMake your edits and save.\n\n\nAdding New Files:\n\n\nTo add new scripts or test files, right-click in the file explorer panel, and select New File.\n\n\n\n\n\nSet up breakpoints by clicking in the gutter (left margin) of your code editor.\nUse the Debug panel to start a debugging session.\n\n\n\n\n\nStage and Commit Changes:\n\n\nStage your changes:\n\ngit add .\n\nCommit the changes:\n\ngit commit -m \"Your commit message\"\n\nPush Changes to GitHub:\n\n\nPush your changes to the repository:\n\ngit push origin main\n\n\n\n\nWhen finished, click Stop Codespace to save resources.\nTo reopen, return to the Codespaces tab in GitHub and select your environment.\n\n\n\n\n\n\n\nTask\nCommand\n\n\n\n\nRun linting\npylint ds.py\n\n\nInstall dependencies\npip install -r requirements.txt\n\n\nStage changes\ngit add .\n\n\nCommit changes\ngit commit -m \"Your commit message\"\n\n\nPush changes\ngit push origin main\n\n\n\nThis guide ensures you’re set up to write, test, and manage Python code effectively in GitHub Codespaces!",
    "crumbs": [
      "Homework",
      "Codespaces"
    ]
  },
  {
    "objectID": "labs/codespaces.html#getting-started-with-python-on-codespaces",
    "href": "labs/codespaces.html#getting-started-with-python-on-codespaces",
    "title": "Working with Codespaces",
    "section": "",
    "text": "Navigate to the GitHub repository.\nClick on the green Code button, and select Codespaces.\n\nIf a Codespace environment isn’t set up yet, click Create codespace on main.\nThis will initialize a cloud-based development environment pre-configured for this repository.\n\n\n\n\n\nOnce the Codespace is launched:\n\nVerify Python Version:\n\n\nOpen the terminal in Codespaces and run:\n\npython3 --version\n\nEnsure it outputs Python 3.x. If not, install Python 3 using:\n\nsudo apt-get install python3\n\nInstall Dependencies:\n\n\nInstall the required Python packages listed in requirements.txt:\n\npip install -r requirements.txt\n\nVerify the Installation:\n\n\nRun pip freeze to confirm that the dependencies are installed.\n\n\n\n\n\nRunning the Application:\n\n\nThe main script seems to be ds.py. You can run it with:\n\npython3 ds.py\n\nRunning Tests:\n\n\nThe test file test_ds.py is located in the root directory.\nRun tests using unittest:\n\npython3 -m unittest test_ds.py\n\nAlternatively, if pytest is installed:\n\npytest test_ds.py\n\nLinting Code:\n\n\nUse pylint to check the quality of ds.py:\n\npylint ds.py\n\n\n\n\nEditing the Code:\n\n\nOpen files (like ds.py or test_ds.py) using the built-in VS Code editor in Codespaces.\nMake your edits and save.\n\n\nAdding New Files:\n\n\nTo add new scripts or test files, right-click in the file explorer panel, and select New File.\n\n\n\n\n\nSet up breakpoints by clicking in the gutter (left margin) of your code editor.\nUse the Debug panel to start a debugging session.\n\n\n\n\n\nStage and Commit Changes:\n\n\nStage your changes:\n\ngit add .\n\nCommit the changes:\n\ngit commit -m \"Your commit message\"\n\nPush Changes to GitHub:\n\n\nPush your changes to the repository:\n\ngit push origin main\n\n\n\n\nWhen finished, click Stop Codespace to save resources.\nTo reopen, return to the Codespaces tab in GitHub and select your environment.\n\n\n\n\n\n\n\nTask\nCommand\n\n\n\n\nRun linting\npylint ds.py\n\n\nInstall dependencies\npip install -r requirements.txt\n\n\nStage changes\ngit add .\n\n\nCommit changes\ngit commit -m \"Your commit message\"\n\n\nPush changes\ngit push origin main\n\n\n\nThis guide ensures you’re set up to write, test, and manage Python code effectively in GitHub Codespaces!",
    "crumbs": [
      "Homework",
      "Codespaces"
    ]
  },
  {
    "objectID": "summative/final.html",
    "href": "summative/final.html",
    "title": "Final Assignment",
    "section": "",
    "text": "A large university knows that about 70% of the full-time students are employed at least 5 hours per week. The members of the Statistics Department wonder if the same proportion of their students work at least 5 hours per week. They randomly sample 25 majors and find that 15 of the students (60%) work 5 or more hours each week.\n\n\nDescribe how you can set up a simulation to estimate the proportion of statistics majors who work 5 or more hours each week based on this sample.\n\n\n\nA bootstrap distribution with 1000 simulations is shown below. Approximate the bounds of the 95% confidence interval based on this distribution.\n\n\n\n\nSuppose the lower bound of the confidence interval from the previous question is L and the upper bound is U. Which of the following is correct?\na. Between L to U of statistics majors work at least 5 hours per week.\nb. 95% of the time the true proportion of statistics majors who work at least 5 hours per week is between L and U.\nc. Between L and U of random samples of 25 statistics majors are expected to yield confidence intervals that contain the true proportion of statistics majors who work at least 5 hours per week.\nd. 95% of random samples of 25 statistics majors will yield confidence intervals between L and U.\ne. None of the above."
  },
  {
    "objectID": "summative/final.html#question-1",
    "href": "summative/final.html#question-1",
    "title": "Final Assignment",
    "section": "",
    "text": "Describe how you can set up a simulation to estimate the proportion of statistics majors who work 5 or more hours each week based on this sample."
  },
  {
    "objectID": "summative/final.html#question-2",
    "href": "summative/final.html#question-2",
    "title": "Final Assignment",
    "section": "",
    "text": "A bootstrap distribution with 1000 simulations is shown below. Approximate the bounds of the 95% confidence interval based on this distribution."
  },
  {
    "objectID": "summative/final.html#question-3",
    "href": "summative/final.html#question-3",
    "title": "Final Assignment",
    "section": "",
    "text": "Suppose the lower bound of the confidence interval from the previous question is L and the upper bound is U. Which of the following is correct?\na. Between L to U of statistics majors work at least 5 hours per week.\nb. 95% of the time the true proportion of statistics majors who work at least 5 hours per week is between L and U.\nc. Between L and U of random samples of 25 statistics majors are expected to yield confidence intervals that contain the true proportion of statistics majors who work at least 5 hours per week.\nd. 95% of random samples of 25 statistics majors will yield confidence intervals between L and U.\ne. None of the above."
  },
  {
    "objectID": "summative/final.html#question-4",
    "href": "summative/final.html#question-4",
    "title": "Final Assignment",
    "section": "Question 4",
    "text": "Question 4\nNext, you fit a model for predicting raises (percent_incr) from salaries (annual_salary). We’ll call this model raise_1_fit. An output of the model is shown below.\n\n\n                  Coef.  Std.Err.         t     P&gt;|t|    [0.025    0.975]\nIntercept      1.869965  0.432035  4.328268  0.000019  1.020397  2.719532\nannual_salary  0.000016  0.000005  3.431459  0.000669  0.000007  0.000024\n\n\nWhich of the following is the best interpretation of the slope coefficient?\n\nFor every additional $1,000 of annual salary, the model predicts the raise to be higher, on average, by 1.6%.\nFor every additional $1,000 of annual salary, the raise goes up by 0.016%.\nFor every additional $1,000 of annual salary, the model predicts the raise to be higher, on average, by 0.016%.\nFor every additional $1,000 of annual salary, the model predicts the raise to be higher, on average, by 1.87%."
  },
  {
    "objectID": "summative/final.html#question-5",
    "href": "summative/final.html#question-5",
    "title": "Final Assignment",
    "section": "Question 5",
    "text": "Question 5\nYou then fit a model for predicting raises (percent_incr) from salaries (annual_salary) and performance ratings (performance_rating). We’ll call this model raise_2_fit. Which of the following is definitely true based on the information you have so far?\n\nIntercept of raise_2_fit is higher than intercept of raise_1_fit.\nSlope of raise_2_fit is higher than RMSE of raise_1_fit.\nAdjusted \\(R^2\\) of raise_2_fit is higher than adjusted \\(R^2\\) of raise_1_fit.\n\\(R^2\\) of raise_2_fit is higher \\(R^2\\) of raise_1_fit."
  },
  {
    "objectID": "summative/final.html#question-6",
    "href": "summative/final.html#question-6",
    "title": "Final Assignment",
    "section": "Question 6",
    "text": "Question 6\nThe tidy model output for the raise_2_fit model you fit is shown below.\n\n\n                                          Coef.  Std.Err.         t  \\\nIntercept                              2.617865  0.452366  5.787046   \nperformance_rating_Poor[T.True]       -3.499070  1.500230 -2.332356   \nperformance_rating_Successful[T.True] -1.730554  0.361704 -4.784449   \nperformance_rating_Top[T.True]         3.628454  0.730187  4.969215   \nannual_salary                          0.000013  0.000004  3.119855   \n\n                                              P&gt;|t|    [0.025    0.975]  \nIntercept                              1.543384e-08  1.728293  3.507437  \nperformance_rating_Poor[T.True]        2.022505e-02 -6.449249 -0.548892  \nperformance_rating_Successful[T.True]  2.493372e-06 -2.441838 -1.019269  \nperformance_rating_Top[T.True]         1.035401e-06  2.192554  5.064355  \nannual_salary                          1.953363e-03  0.000005  0.000021  \n\n\nWhen your teammate sees this model output, they remark “The coefficient for performance_ratingSuccessful is negative, that’s weird. I guess it means that people who get successful performance ratings get lower raises.” How would you respond to your teammate?"
  },
  {
    "objectID": "summative/final.html#question-7",
    "href": "summative/final.html#question-7",
    "title": "Final Assignment",
    "section": "Question 7",
    "text": "Question 7\nUltimately, your teammate decides they don’t like the negative slope coefficients in the model output you created (not that there’s anything wrong with negative slope coefficients!), does something else, and comes up with the following model output. Note however that the coefficient is still negative, but this satisfies your friend…\n\n\n                                          Coef.  Std.Err.         t  \\\nIntercept                              1.785333  0.509233  3.505927   \nperformance_rating_Successful[T.True] -0.800356  0.439216 -1.822238   \nperformance_rating_High[T.True]        1.574196  0.475552  3.310248   \nperformance_rating_Top[T.True]         4.569528  0.768132  5.948886   \nannual_salary                          0.000012  0.000004  2.854297   \n\n                                              P&gt;|t|    [0.025    0.975]  \nIntercept                              5.116857e-04  0.783935  2.786732  \nperformance_rating_Successful[T.True]  6.923704e-02 -1.664068  0.063355  \nperformance_rating_High[T.True]        1.025064e-03  0.639030  2.509362  \nperformance_rating_Top[T.True]         6.327904e-09  3.059009  6.080047  \nannual_salary                          4.559386e-03  0.000004  0.000020  \n\n\nUnfortunately they didn’t write their code in a Quarto document, instead just wrote some code in the Console and then lost track of their work. They remember using the fct_relevel() function and doing something like the following:\n\nblizzard_salary['performance_rating'] = pd.Categorical(\n    blizzard_salary['performance_rating'],\n    categories=[____],\n    ordered=True\n)\n\nWhat should they put in the blanks to get the same model output as above?\n\n“Poor”, “Successful”, “High”, “Top”\n“Successful”, “High”, “Top”\n“Top”, “High”, “Successful”, “Poor”\nPoor, Successful, High, Top"
  },
  {
    "objectID": "summative/final.html#question-8",
    "href": "summative/final.html#question-8",
    "title": "Final Assignment",
    "section": "Question 8",
    "text": "Question 8\nSuppose we fit a model to predict percent_incr from annual_salary and salary_type. A tidy output of the model is shown below.\n\n\n                             Coef.  Std.Err.         t     P&gt;|t|    [0.025  \\\nIntercept                 1.242597  0.570278  2.178932  0.029972  0.121174   \nsalary_type_year[T.True]  0.913343  0.543715  1.679819  0.093844 -0.155845   \nannual_salary             0.000014  0.000005  2.958979  0.003287  0.000005   \n\n                            0.975]  \nIntercept                 2.364020  \nsalary_type_year[T.True]  1.982532  \nannual_salary             0.000023  \n\n\nWhich of the following visualizations represent this model? Explain your reasoning.\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\n\nVisualizations of the relationship between percent increase, annual salary, and salary type\n\nFigure 1\nFigure 2\nFigure 3\nFigure 4"
  },
  {
    "objectID": "summative/final.html#question-9",
    "href": "summative/final.html#question-9",
    "title": "Final Assignment",
    "section": "Question 9",
    "text": "Question 9\nDefine the term parsimonious model."
  },
  {
    "objectID": "summative/final.html#question-10",
    "href": "summative/final.html#question-10",
    "title": "Final Assignment",
    "section": "Question 10",
    "text": "Question 10\nSuppose you now fit a model to predict the natural log of percent increase, log(percent_incr), from performance rating. The model is called raise_4_fit.\nYou’re provided the following:\n\nraise_4_fit_coefs['exp_estimate'] = np.exp(raise_4_fit_coefs['estimate'])\nprint(raise_4_fit_coefs)\n\n                            term  estimate  exp_estimate\n0                      Intercept -2.088594      0.123861\n1  performance_rating_Successful  1.872176      6.502427\n2        performance_rating_High  3.110229     22.426176\n3         performance_rating_Top  3.854360     47.198390\n\n\nBased on this, which of the following is true?\na. The model predicts that the percentage increase employees with Successful performance get, on average, is higher by 10.25% compared to the employees with Poor performance rating.\nb. The model predicts that the percentage increase employees with Successful performance get, on average, is higher by 6.93% compared to the employees with Poor performance rating.\nc. The model predicts that the percentage increase employees with Successful performance get, on average, is higher by a factor of 6.502427 compared to the employees with Poor performance rating.\nd. The model predicts that the percentage increase employees with Successful performance get, on average, is higher by a factor of 1.872176 compared to the employees with Poor performance rating."
  },
  {
    "objectID": "summative/final.html#question-11",
    "href": "summative/final.html#question-11",
    "title": "Final Assignment",
    "section": "Question 11",
    "text": "Question 11\nWhich of the following is the definiton of a regression model? Select all that apply.\na. \\(\\hat{y} = \\beta_0 + \\beta_1 X_1\\)\nb. \\(y = \\beta_0 + \\beta_1 X_1\\)\nc. \\(\\hat{y} = \\beta_0 + \\beta_1 X_1 + \\epsilon\\)\nd. \\(y = \\beta_0 + \\beta_1 X_1 + \\epsilon\\)"
  },
  {
    "objectID": "summative/final.html#question-12",
    "href": "summative/final.html#question-12",
    "title": "Final Assignment",
    "section": "Question 12",
    "text": "Question 12\nCompute the derivative \\(( \\frac{d}{dx} )\\) of the following function:\n\\[\ng(x) = \\left( \\sin(x^2) + \\cos(ax) \\right)^k\n\\]"
  },
  {
    "objectID": "summative/final.html#question-13",
    "href": "summative/final.html#question-13",
    "title": "Final Assignment",
    "section": "Question 13",
    "text": "Question 13\nCompute the following integral:\n\\[\n\\int_{a}^{b} \\left( e^{cx} + \\frac{1}{x^n} \\right) dx\n\\]"
  },
  {
    "objectID": "summative/final.html#question-14",
    "href": "summative/final.html#question-14",
    "title": "Final Assignment",
    "section": "Question 14",
    "text": "Question 14\nGiven a vector \\(x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}\\), write down its transpose \\(x^\\top\\)."
  },
  {
    "objectID": "summative/final.html#question-15",
    "href": "summative/final.html#question-15",
    "title": "Final Assignment",
    "section": "Question 15",
    "text": "Question 15\nGiven the following matrix \\(N\\):\n\\[\nN = \\begin{bmatrix}\nn_{11} & n_{12} \\\\\nn_{21} & n_{22} \\\\\nn_{31} & n_{32} \\\\\nn_{41} & n_{42}\n\\end{bmatrix}\n\\]\nWrite down its transpose, \\(N^\\top\\)."
  },
  {
    "objectID": "summative/final.html#question-16",
    "href": "summative/final.html#question-16",
    "title": "Final Assignment",
    "section": "Question 16",
    "text": "Question 16\nConsider the following matrices \\(C\\) and \\(D\\):\n\\[\nC =\n\\begin{bmatrix}c_{11} & c_{12} \\\\c_{21} & c_{22} \\\\c_{31} & c_{32}\\end{bmatrix}, \\quad\nD =\n\\begin{bmatrix}d_{11} & d_{12} & d_{13} \\\\d_{21} & d_{22} & d_{23}\\end{bmatrix}\n\\]\n\nWhat are the dimensions of \\(C\\)?\nWhat are the dimensions of \\(D\\)?\nFor the matrix product \\(CD\\):\n\nDetermine if the product is valid, and explain why.\nIf the product is valid, write down the dimensions of the resulting matrix without computing the product."
  },
  {
    "objectID": "summative/final.html#question-17",
    "href": "summative/final.html#question-17",
    "title": "Final Assignment",
    "section": "Question 17",
    "text": "Question 17\nGiven the matrices \\(E\\) and \\(F\\):\n\\[\nE = \\begin{bmatrix}\ne_{11} & e_{12} \\\\\ne_{21} & e_{22} \\\\\ne_{31} & e_{32}\n\\end{bmatrix}, \\quad\nF = \\begin{bmatrix}\nf_{11} \\\\\nf_{21}\n\\end{bmatrix}\n\\]\n\nWhat are the dimensions of \\(E\\)?\nWhat are the dimensions of \\(F\\)?\nFor the matrix product \\(EF\\):\n\nDetermine if the product is valid, and explain why.\nIf the product is valid, compute the resulting matrix."
  },
  {
    "objectID": "summative/final.html#bonus",
    "href": "summative/final.html#bonus",
    "title": "Final Assignment",
    "section": "Bonus",
    "text": "Bonus\nPick a concept we introduced in class so far that you’ve been struggling with and explain it in your own words."
  },
  {
    "objectID": "ae/ae-10-modeling-fish-A.html",
    "href": "ae/ae-10-modeling-fish-A.html",
    "title": "AE 10: Modelling fish",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\n\n\nFor this application exercise, we will work with data on fish. The dataset we will use, called fish, is on two common fish species in fish market sales.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nfish = pd.read_csv(\"data/fish.csv\")\nprint(fish.head())\n\n  species  weight  length_vertical  length_diagonal  length_cross   height  \\\n0   Bream     242             23.2             25.4          30.0  11.5200   \n1   Bream     290             24.0             26.3          31.2  12.4800   \n2   Bream     340             23.9             26.5          31.1  12.3778   \n3   Bream     363             26.3             29.0          33.5  12.7300   \n4   Bream     430             26.5             29.0          34.0  12.4440   \n\n    width  \n0  4.0200  \n1  4.3056  \n2  4.6961  \n3  4.4555  \n4  5.1340  \n\n\nThe data dictionary is below:\n\n\n\nvariable\ndescription\n\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\n\nVisualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish.\n\nCreate an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='height', y='weight', data=fish, alpha=0.5)\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (g)\")\nplt.title(\"Relationship between Heights and Weights of Fish\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\nStart from the bottom and go up Identify the first and last point and draw a line through most the others.\n\nNow, let Python draw the line for you. Refer to the documentation at https://seaborn.pydata.org/generated/seaborn.lmplot.html. Specifically, refer to the method section.\n\n\n# Fit a linear model\nX = fish[['height']]\ny = fish['weight']\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predict values\npredictions = model.predict(X)\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='height', y='weight', data=fish, alpha=0.5)\nplt.plot(fish['height'], predictions, color=\"#325b74\", linewidth=2)\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (g)\")\nplt.title(\"Relationship between Heights and Weights of Fish with Linear Regression Line\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat types of questions can this plot help answer?\nIs there a relationship between fish heights and weights of fish?\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\n\nAt 10 cm, we estimate a weight of 375 grams. At 15 cm, we estimate a weight of 600 grams At 20 cm, we estimate a weight of 975 grams. 20 cm would be considered extrapolation.\nWhat is a residual?\nDifference between predicted and observed.\n\n\n\n\nModel fitting\n\nFit a model to predict fish weights from their heights.\n\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nfish['predicted_weight'] = model.predict(X)\nfish['residual'] = fish['weight'] - fish['predicted_weight']\n\n\nPredict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\nheights = np.array([[10], [15], [20]])\npredicted_weights = model.predict(heights)\nfor height, weight in zip(heights, predicted_weights):\n    print(f\"Predicted weight for height {height[0]} cm: {weight:.2f} g\")\n\nPredicted weight for height 10 cm: 320.74 g\nPredicted weight for height 15 cm: 625.32 g\nPredicted weight for height 20 cm: 929.90 g\n\n\n\nCalculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\nfish['predicted_weight'] = model.predict(X)\nfish['residual'] = fish['weight'] - fish['predicted_weight']\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='height', y='residual', data=fish, alpha=0.5)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Residual (g)\")\nplt.title(\"Residuals of the Linear Regression Model\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel summary\n\nDisplay the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\nslope = model.coef_[0]\nintercept = model.intercept_\nprint(f\"Slope: {slope:.2f}\")\nprint(f\"Intercept: {intercept:.2f}\")\n\nSlope: 60.92\nIntercept: -288.42\n\n\n\nWrite out your model using mathematical notation.\n\n\\(\\widehat{weight} = -288 + 60.9 \\times height\\)\n\n\nCorrelation\nWe can also assess correlation between two quantitative variables.\n\nWhat is correlation? What are values correlation can take?\n\nStrength and direction of a linear relationship. It’s bounded by -1 and 1.\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\n\nWhat is the correlation between heights and weights of fish?\n\n\ncorrelation = fish['height'].corr(fish['weight'])\nprint(f\"Correlation between height and weight: {correlation:.2f}\")\n\nCorrelation between height and weight: 0.95\n\n\n\n\nAdding a third variable\n\nDoes the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\nplt.figure(figsize=(8, 6))\nsns.lmplot(x='height', y='weight', hue='species', data=fish, markers=[\"o\", \"s\"], ci=None, palette=\"muted\", height=6, aspect=1.5)\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (g)\")\nplt.title(\"Relationship between Heights and Weights of Fish by Species\")\nplt.show()\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nFitting other models\n\nWe can fit more models than just a straight line. Change the sns.regplot() code you previously used to include lowess=True. What is different from the plot created before?\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='height', y='weight', data=fish, alpha=0.5)\nsns.regplot(x='height', y='weight', data=fish, lowess=True, scatter_kws={'s':10}, line_kws={'color':'red'})\nplt.xlabel(\"Height (cm)\")\nplt.ylabel(\"Weight (g)\")\nplt.title(\"Relationship between Heights and Weights of Fish with LOESS Smoothing\")\nplt.show()"
  },
  {
    "objectID": "ae/ae-02-diwali-sales.html",
    "href": "ae/ae-02-diwali-sales.html",
    "title": "AE 02: Diwali sales + EDA",
    "section": "",
    "text": "Read in the data:\n\nRead the Diwali sales data into a Pandas DataFrame and display the first few rows.\nHint: use , encoding = 'iso-8859-1' within the pd.___() function.\n\n\n\nimport pandas as pd\n\n# add code here\n\n\nExamine the Data:\n\nDisplay basic information about the dataset using the .info() method.\nDisplay summary statistics for the numerical columns using the .describe() method.\n\n\n\n# add code here\n\nAdd narrative here.\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-02-diwali-sales.html#exercise-1",
    "href": "ae/ae-02-diwali-sales.html#exercise-1",
    "title": "AE 02: Diwali sales + EDA",
    "section": "",
    "text": "Read in the data:\n\nRead the Diwali sales data into a Pandas DataFrame and display the first few rows.\nHint: use , encoding = 'iso-8859-1' within the pd.___() function.\n\n\n\nimport pandas as pd\n\n# add code here\n\n\nExamine the Data:\n\nDisplay basic information about the dataset using the .info() method.\nDisplay summary statistics for the numerical columns using the .describe() method.\n\n\n\n# add code here\n\nAdd narrative here.\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-02-diwali-sales.html#exercise-2",
    "href": "ae/ae-02-diwali-sales.html#exercise-2",
    "title": "AE 02: Diwali sales + EDA",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nExploring unique levels, outliers, and missing values\n\nExploring Unique Levels:\n\nIdentify and display the unique values in each categorical column.\n\n\n\n# add code here\n\n\nIdentifying and Visualizing Outliers:\n\nCreate a box plot to visualize outliers in the Amount column.\nIdentify outliers using the IQR method and count the number of outliers for each numerical column.\n\n\n\n# add code here\n\n\nHandling Missing Values:\n\nCheck for missing values in the dataset.\n\n\n\n# add code here\n\nAdd narrative here.\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-05-majors-wrangling-A.html",
    "href": "ae/ae-05-majors-wrangling-A.html",
    "title": "AE 05: Wrangling College Majors",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-05-majors-wrangling-A.html#goal",
    "href": "ae/ae-05-majors-wrangling-A.html#goal",
    "title": "AE 05: Wrangling College Majors",
    "section": "Goal",
    "text": "Goal\nOur ultimate goal in this application exercise is to make the following data visualization."
  },
  {
    "objectID": "ae/ae-05-majors-wrangling-A.html#data",
    "href": "ae/ae-05-majors-wrangling-A.html#data",
    "title": "AE 05: Wrangling College Majors",
    "section": "Data",
    "text": "Data\nFor this exercise you will work with data on the proportions of Bachelor’s degrees awarded in the US between 2005 and 2015. The dataset you will use is in your data/ folder and it’s called degrees.csv.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ninfosci = pd.read_csv(\"data/degrees.csv\")\n\nAnd let’s take a look at the data.\n\ninfosci.head()\n\n\n\n\n\n\n\n\ndegree\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nInformation Science & eSociety (BA)\n63.0\n61.0\n67.0\n71\n38\n\n\n1\nInformation Science (BS)\nNaN\nNaN\nNaN\n16\n57\n\n\n2\nInformation (PhD)\n2.0\n3.0\n1.0\n1\n1\n\n\n3\nLibrary & Information Science (MA)\n47.0\n57.0\n72.0\n42\n58\n\n\n4\nInformation (MS)\n8.0\n10.0\n13.0\n5\n2"
  },
  {
    "objectID": "ae/ae-05-majors-wrangling-A.html#pivoting",
    "href": "ae/ae-05-majors-wrangling-A.html#pivoting",
    "title": "AE 05: Wrangling College Majors",
    "section": "Pivoting",
    "text": "Pivoting\n\nPivot the degrees data frame longer such that each row represents a degree type / year combination and year and number of graduates for that year are columns in the data frame.\n\n\ninfosci_long = infosci.melt(id_vars='degree', var_name='year', value_name='n')\n\ninfosci_long.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30 entries, 0 to 29\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   degree  30 non-null     object \n 1   year    30 non-null     object \n 2   n       24 non-null     float64\ndtypes: float64(1), object(2)\nmemory usage: 852.0+ bytes\n\n\n\nQuestion: What is the type of the year variable? Why? What should it be?\n\nIt’s an object variable since the information came from the columns of the original data frame and Python cannot know that these character strings represent years. The variable type should be numeric (int64).\n\nStart over with pivoting, and this time also make sure year is a numerical variable in the resulting data frame.\n\n\ninfosci_long = infosci.melt(id_vars='degree', var_name='year', value_name='n')\ninfosci_long['year'] = pd.to_numeric(infosci_long['year'])\n\ninfosci_long.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30 entries, 0 to 29\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   degree  30 non-null     object \n 1   year    30 non-null     int64  \n 2   n       24 non-null     float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 852.0+ bytes\n\n\n\nQuestion: What does an NaN mean in this context? Hint: The data come from the university registrar, and they have records on every single graduates, there shouldn’t be anything “unknown” to them about who graduated when.\n\nNAs should actually be 0s.\n\nAdd on to your pipeline that you started with pivoting and convert NaNs in n to 0s.\n\n\ninfosci_long = infosci.melt(id_vars='degree', var_name='year', value_name='n')\ninfosci_long['year'] = pd.to_numeric(infosci_long['year'])\ninfosci_long['n'] = infosci_long['n'].fillna(0)\n\ninfosci_long.isna().sum()\n\ndegree    0\nyear      0\nn         0\ndtype: int64\n\n\n\nIn our plot the degree types are BA, BS, MA, MS, and PhD. This information is in our dataset, in the degree column, but this column also has additional characters we don’t need. Create a new column called degree_type with levels BA, BS, MA, MS, and PhD (in this order) based on degree. Do this by adding on to your pipeline from earlier.\n\n\ninfosci_long = infosci.melt(id_vars='degree', var_name='year', value_name='n')\ninfosci_long['year'] = pd.to_numeric(infosci_long['year'])\ninfosci_long['n'] = infosci_long['n'].fillna(0)\n\ninfosci_long[['major', 'degree_type']] = infosci_long['degree'].str.split(' \\(', expand=True)\ninfosci_long['degree_type'] = infosci_long['degree_type'].str.replace('\\)', '', regex=True)\n\ndegree_order = pd.CategoricalDtype(categories=[\"BA\", \"BS\", \"MA\", \"MS\", \"PhD\"], ordered=True)\ninfosci_long['degree_type'] = infosci_long['degree_type'].astype(degree_order)\n\ninfosci_long.head()\n\n\n\n\n\n\n\n\ndegree\nyear\nn\nmajor\ndegree_type\n\n\n\n\n0\nInformation Science & eSociety (BA)\n2019\n63.0\nInformation Science & eSociety\nBA\n\n\n1\nInformation Science (BS)\n2019\n0.0\nInformation Science\nBS\n\n\n2\nInformation (PhD)\n2019\n2.0\nInformation\nPhD\n\n\n3\nLibrary & Information Science (MA)\n2019\n47.0\nLibrary & Information Science\nMA\n\n\n4\nInformation (MS)\n2019\n8.0\nInformation\nMS\n\n\n\n\n\n\n\n\nNow we start making our plot, but let’s not get too fancy right away. Create the following plot, which will serve as the “first draft” on the way to our Goal. Do this by adding on to your pipeline from earlier.\n\n\nsns.set_style(\"darkgrid\")\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=infosci_long, x='year', y='n', hue='degree_type', ci=None, marker='o')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat aspects of the plot need to be updated to go from the draft you created above to the Goal plot at the beginning of this application exercise.\n\nx-axis scale: need to go from 2019 to 2023 in unique year values\nline colors\naxis labels: title, x, y\ntheme\n\nUpdate x-axis scale such that the years displayed go from 2019 to 2023 in unique years. Do this by adding on to your pipeline from earlier.\n\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=infosci_long, x='year', y='n', hue='degree_type', ci=None, marker='o')\nplt.xticks(ticks=infosci_long['year'].unique(), labels=infosci_long['year'].unique())\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUpdate line colors using the following level / color assignments. Once again, do this by adding on to your pipeline from earlier.\n\nBA: “#53868B”\nBS: “#7AC5CD”\nMA: “#89a285”\nMS: “#8B814C”\nPhD: “#CDBE70”\n\n\n\ncustom_palette = {\n    \"BA\": \"#53868B\",\n    \"BS\": \"#7AC5CD\",\n    \"MA\": \"#89a285\",\n    \"MS\": \"#8B814C\",\n    \"PhD\": \"#CDBE70\"\n}\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=infosci_long, x='year', y='n', hue='degree_type', ci=None, marker='o')\nplt.xticks(ticks=infosci_long['year'].unique(), labels=infosci_long['year'].unique())\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUpdate the plot labels (title, x, and y) and use sns.set_style(\"white_grid\"). Once again, do this by adding on to your pipeline from earlier.\n\n\nsns.set_style(\"whitegrid\")\n\ncustom_palette = {\n    \"BA\": \"#53868B\",\n    \"BS\": \"#7AC5CD\",\n    \"MA\": \"#89a285\",\n    \"MS\": \"#8B814C\",\n    \"PhD\": \"#CDBE70\"\n}\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=infosci_long, x='year', y='n', hue='degree_type', ci=None, marker='o', palette=custom_palette)\nplt.title('College of Information Science degrees over the years\\nAcademic years 2019 - 2023')\nplt.xticks(ticks=infosci_long['year'].unique(), labels=infosci_long['year'].unique())\nplt.xlabel('Graduation year')\nplt.ylabel('Number of students graduating')\nplt.legend(title='Degree type')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "ae/ae-06-wildcat-scrape-A.html",
    "href": "ae/ae-06-wildcat-scrape-A.html",
    "title": "AE 06: Opinion articles in The Arizona Daily Wildcat",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-06-wildcat-scrape-A.html#part-1---data-scraping",
    "href": "ae/ae-06-wildcat-scrape-A.html#part-1---data-scraping",
    "title": "AE 06: Opinion articles in The Arizona Daily Wildcat",
    "section": "Part 1 - Data scraping",
    "text": "Part 1 - Data scraping\nSee wildcat-scrape.py for suggested scraping code."
  },
  {
    "objectID": "ae/ae-06-wildcat-scrape-A.html#part-2---data-analysis",
    "href": "ae/ae-06-wildcat-scrape-A.html#part-2---data-analysis",
    "title": "AE 06: Opinion articles in The Arizona Daily Wildcat",
    "section": "Part 2 - Data analysis",
    "text": "Part 2 - Data analysis\nLet’s start by loading the packages we will need:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n\nLoad the data you saved into the data folder and name it wildcat.\n\n\nwildcat = pd.read_csv(\"data/wildcat.csv\")\n\n\nWho are the most prolific authors of the 100 most recent opinion articles in The Arizona Daily Wildcat?\n\n\nauthor_counts = wildcat['author'].value_counts().reset_index()\nauthor_counts.columns = ['author', 'count']\nprint(author_counts)\n\n               author  count\n0         Greg Castro     35\n1       Toni Marcheva     31\n2    Apoorva Bhaskara     28\n3          Alec Scott     26\n4          Sean Fagan     24\n..                ...    ...\n183       Jack Cooper      1\n184         Amit Syal      1\n185     Quinn McVeigh      1\n186         Eric Wise      1\n187  Gabriel Schivone      1\n\n[188 rows x 2 columns]\n\n\n\nDraw a line plot of the number of opinion articles published per day in The Arizona Daily Wildcat.\n\n\nwildcat['date'] = pd.to_datetime(wildcat['date'])\n\narticles_per_day = wildcat['date'].value_counts().sort_index().reset_index()\narticles_per_day.columns = ['date', 'count']\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=articles_per_day, x='date', y='count', marker='o')\nplt.title('Number of Opinion Articles Published Per Day')\nplt.xlabel('Date')\nplt.ylabel('Number of Articles')\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat percent of the most recent 100 opinion articles in The Arizona Daily Wildcat mention “climate” in their title?\n\n\nmost_recent_100 = wildcat.head(100)\n\nmost_recent_100['title_lower'] = most_recent_100['title'].str.lower()\nmost_recent_100['climate_mentioned'] = most_recent_100['title_lower'].apply(lambda x: 'mentioned' if 'climate' in x else 'not mentioned')\n\nclimate_mentions = most_recent_100['climate_mentioned'].value_counts(normalize=True).reset_index()\nclimate_mentions.columns = ['climate_mentioned', 'percentage']\nprint(climate_mentions)\n\n  climate_mentioned  percentage\n0     not mentioned        0.99\n1         mentioned        0.01\n\n\n\nWhat percent of the most recent 100 opinion articles in The Arizona Daily Wildcat mention “election” in their title or abstract?\n\n\nmost_recent_100 = wildcat.head(100)\n\nmost_recent_100['title_lower'] = most_recent_100['title'].str.lower()\nmost_recent_100['election_mentioned'] = most_recent_100['title_lower'].apply(lambda x: 'mentioned' if 'election' in x else 'not mentioned')\n\nclimate_mentions = most_recent_100['election_mentioned'].value_counts(normalize=True).reset_index()\nclimate_mentions.columns = ['election_mentioned', 'percentage']\nprint(climate_mentions)\n\n  election_mentioned  percentage\n0      not mentioned         1.0\n\n\n\nWhat are the most common words in the titles of the 100 most recent articles?\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport nltk\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nstop_words = set(stopwords.words('english'))\n\nmost_recent_100['tokens'] = most_recent_100['title_lower'].apply(lambda x: [word for word in word_tokenize(x) if word.isalpha() and word not in stop_words])\n\n# Count the frequency of each word\nword_freq = Counter([word for tokens in most_recent_100['tokens'] for word in tokens])\n\n# Convert to DataFrame and plot\nword_freq_df = pd.DataFrame(word_freq.most_common(20), columns=['word', 'count'])\n\nplt.figure(figsize=(8, 6))\nsns.barplot(data=word_freq_df, x='count', y='word', palette='viridis')\nplt.title('Most Common Words in Titles of 100 Most Recent Articles')\nplt.xlabel('Count')\nplt.ylabel('Word')\nplt.show()\n\n\n\n\n\n\n\n\n\nTime permitting:"
  },
  {
    "objectID": "ae/ae-15-linear-algebra-A.html",
    "href": "ae/ae-15-linear-algebra-A.html",
    "title": "AE 15: Linear algebra",
    "section": "",
    "text": "In this exercise, we will:"
  },
  {
    "objectID": "ae/ae-15-linear-algebra-A.html#transposition",
    "href": "ae/ae-15-linear-algebra-A.html#transposition",
    "title": "AE 15: Linear algebra",
    "section": "Transposition",
    "text": "Transposition\n\nExercise 1\nGiven a vector\\(\\mathbf{y}\\):\n\\[\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix}\n\\]\nWrite down its transpose \\(\\mathbf{y}^\\top\\)\nSolution:\n\nThe transpose of the vector \\(\\mathbf{y}\\) is:\n\n\\[\n\\mathbf{y}^\\top = \\begin{bmatrix} y_1 & y_2 & y_3 \\end{bmatrix}\n\\]\n\n\nExercise 2\nGiven the following matrix \\(\\mathbf{N}\\):\n\\[\n\\mathbf{N} = \\begin{bmatrix}n_{11} & n_{12} \\\\n_{21} & n_{22} \\\\n_{31} & n_{32}\\end{bmatrix}\n\\]\nWrite down its transpose, \\(\\mathbf{N}^{\\top}\\)\nSolution:\n\nThe transpose of the matrix \\(\\mathbf{N}\\) is:\n\n\\[\n\\mathbf{N}^\\top = \\begin{bmatrix}n_{11} & n_{21} & n_{31} \\\\n_{12} & n_{22} & n_{32}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ae/ae-15-linear-algebra-A.html#matrix-operations",
    "href": "ae/ae-15-linear-algebra-A.html#matrix-operations",
    "title": "AE 15: Linear algebra",
    "section": "Matrix operations",
    "text": "Matrix operations\n\nExercise 3\nConsider the following matrices \\(\\mathbf{C}\\) and \\(\\mathbf{D}\\):\n\\[\n\\mathbf{C} = \\begin{bmatrix}c_{11} & c_{12} & c_{13} \\\\c_{21} & c_{22} & c_{23}\\end{bmatrix}, \\quad\\mathbf{D} = \\begin{bmatrix}d_{11} & d_{12} \\\\d_{21} & d_{22} \\\\d_{31} & d_{32}\\end{bmatrix}\n\\]\n\nWhat are the dimensions of \\(\\mathbf{C}\\)?\nWhat are the dimensions of \\(\\mathbf{D}\\)?\nFor the matrix product \\(\\mathbf{C} \\mathbf{D}\\):\n\nDetermine if the product is valid, and explain why.\nIf the product is valid, write down the dimensions of the resulting matrix without computing the product.\n\n\nSolution:\n\nThe dimensions of \\(\\mathbf{C}\\) are \\(2 \\times 3\\)\nThe dimensions of \\(\\mathbf{D}\\) are \\(3 \\times 2\\).\nFor the matrix product \\(\\mathbf{C} \\mathbf{D}\\):\n\nThe product is valid because the number of columns in \\(\\mathbf{C}\\) (which is 3) matches the number of rows in \\(\\mathbf{D}\\) (which is 3).\nThe dimensions of the resulting matrix will be \\(2 \\times 2\\).\n\n\n\n\nExercise 4\nConsider the following matrices \\(\\mathbf{E}\\) and \\(\\mathbf{F}\\):\n\\[\n\\mathbf{E} = \\begin{bmatrix}e_{11} & e_{12} & e_{13} & e_{14} \\\\e_{21} & e_{22} & e_{23} & e_{24}\\end{bmatrix}, \\quad\\mathbf{F} = \\begin{bmatrix}f_{11} & f_{12} \\\\f_{21} & f_{22} \\\\f_{31} & f_{32} \\\\f_{41} & f_{42}\\end{bmatrix}\n\\]\n\nWhat are the dimensions of \\(\\mathbf{E}\\)?\nWhat are the dimensions of \\(\\mathbf{F}\\)?\nFor the matrix product \\(\\mathbf{E} \\mathbf{F}\\):\n\nDetermine if the product is valid, and explain why.\nIf the product is valid, write down the dimensions of the resulting matrix without computing the product.\n\n\nSolution:\n\nThe dimensions of \\(\\mathbf{F}\\) are \\(2 \\times 4\\)\nThe dimensions of \\(\\mathbf{F}\\) are \\(4 \\times 2\\).\nFor the matrix product \\(\\mathbf{E} \\mathbf{F}\\):\n\nThe product is valid because the number of columns in \\(\\mathbf{E}\\) (which is 4) matches the number of rows in \\(\\mathbf{F}\\) (which is 4).\nThe resulting matrix is:\n\n\n\\[\n\\mathbf{E} \\mathbf{F} = \\begin{bmatrix}e_{11} & e_{12} & e_{13} & e_{14} \\\\e_{21} & e_{22} & e_{23} & e_{24}\\end{bmatrix}\\begin{bmatrix}f_{11} & f_{12} \\\\f_{21} & f_{22} \\\\f_{31} & f_{32} \\\\f_{41} & f_{42}\\end{bmatrix}= \\\\\\begin{bmatrix}e_{11}f_{11} + e_{12}f_{21} + e_{13}f_{31} + e_{14}f_{41} & e_{11}f_{12} + e_{12}f_{22} + e_{13}f_{32} + e_{14}f_{42} \\\\e_{21}f_{11} + e_{22}f_{21} + e_{23}f_{31} + e_{24}f_{41} & e_{21}f_{12} + e_{22}f_{22} + e_{23}f_{32} + e_{24}f_{42}\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "ae/ae-03-tucson-housing.html",
    "href": "ae/ae-03-tucson-housing.html",
    "title": "AE 03: Tucson Housing + data visualization",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "ae/ae-03-tucson-housing.html#exercise-1",
    "href": "ae/ae-03-tucson-housing.html#exercise-1",
    "title": "AE 03: Tucson Housing + data visualization",
    "section": "Exercise 1",
    "text": "Exercise 1\nSuppose you’re helping some family friends who are looking to buy a house in Tucson. As they browse Zillow listings, they realize some houses have garages and others don’t, and they wonder: Does having a garage make a difference?\nLuckily, you can help them answer this question with data visualization!\n\nMake histograms of the prices of houses in Tucson based on whether they have a garage.\n\nIn order to do this, you will first need to create a new variable called garage (with levels \"Garage\" and \"No garage\").\nBelow is the code for creating this new variable. Here, we transform the tucson_housing data frame to add a new variable called garage which takes the value \"Garage\" if the house type is “Single Family” or “Townhouse” and takes the value \"No garage\" otherwise.\n\n\n\ntucson_housing = pd.read_csv(\"data/tucson_housing.csv\")\n\ngarage_types = [\"Single Family\", \"Townhouse\"]\n\ntucson_housing['garage'] = tucson_housing['type'].apply(lambda x: 'Garage' if x in garage_types else 'No garage')\n\n\nThen, facet by garage and use different colors for the two facets.\nChoose an appropriate binwidth and decide whether a legend is needed, and turn it off if not.\nInclude informative title and axis labels.\nFinally, include a brief (2-3 sentence) narrative comparing the distributions of prices of Tucson houses that do and don’t have garages. Your narrative should touch on whether having a garage “makes a difference” in terms of the price of the house.\n\n\n# add code here\n\nAdd narrative here.\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-03-tucson-housing.html#exercise-2",
    "href": "ae/ae-03-tucson-housing.html#exercise-2",
    "title": "AE 03: Tucson Housing + data visualization",
    "section": "Exercise 2",
    "text": "Exercise 2\nIt’s expected that within any given market, larger houses will be priced higher. It’s also expected that the age of the house will have an effect on the price. However, in some markets new houses might be more expensive while in others new construction might mean “no character” and hence be less expensive. So your family friends ask: “In Tucson, do houses that are bigger and more expensive tend to be newer ones than those that are smaller and cheaper?”\nOnce again, data visualization skills to the rescue!\n\nCreate a scatter plot to explore the relationship between price and area, conditioning for year_built.\nUse sns.scatterplot() to create the scatter plot and lowess from statsmodels to add a LOWESS smooth curve fit to the data.\nInclude informative title, axis, and legend labels.\nDiscuss each of the following claims (1-2 sentences per claim). Your discussion should touch on specific things you observe in your plot as evidence for or against the claims.\n\nClaim 1: Larger houses are priced higher.\nClaim 2: Newer houses are priced higher.\nClaim 3: Bigger and more expensive houses tend to be newer ones than smaller and cheaper ones.\n\n\n\n# add code here\n\nAdd narrative here.\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-02-diwali-sales-A.html",
    "href": "ae/ae-02-diwali-sales-A.html",
    "title": "AE 02: Diwali sales + EDA",
    "section": "",
    "text": "Read in the data:\n\nRead the Diwali sales data into a Pandas DataFrame and display the first few rows.\nHint: use , encoding = 'iso-8859-1' within the pd.___() function.\n\n\n\nimport pandas as pd\n\n# Read in the data\ndiwali = pd.read_csv('data/diwali_sales_data.csv', encoding='iso-8859-1')\ndiwali.head()\n\n\n\n\n\n\n\n\nUser_ID\nCust_name\nProduct_ID\nGender\nAge Group\nAge\nMarital_Status\nState\nZone\nOccupation\nProduct_Category\nOrders\nAmount\n\n\n\n\n0\n1002903\nSanskriti\nP00125942\nF\n26-35\n28\n0\nMaharashtra\nWestern\nHealthcare\nAuto\n1\n23952.0\n\n\n1\n1000732\nKartik\nP00110942\nF\n26-35\n35\n1\nAndhra Pradesh\nSouthern\nGovt\nAuto\n3\n23934.0\n\n\n2\n1001990\nBindu\nP00118542\nF\n26-35\n35\n1\nUttar Pradesh\nCentral\nAutomobile\nAuto\n3\n23924.0\n\n\n3\n1001425\nSudevi\nP00237842\nM\n0-17\n16\n0\nKarnataka\nSouthern\nConstruction\nAuto\n2\n23912.0\n\n\n4\n1000588\nJoni\nP00057942\nM\n26-35\n28\n1\nGujarat\nWestern\nFood Processing\nAuto\n2\n23877.0\n\n\n\n\n\n\n\n\nExamine the Data:\n\nDisplay basic information about the dataset using the .info() method.\nDisplay summary statistics for the numerical columns using the .describe() method.\n\n\n\n# Examine the data\ndiwali.info()\n\n# Describe numerical columns\ndiwali.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11251 entries, 0 to 11250\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   User_ID           11251 non-null  int64  \n 1   Cust_name         11251 non-null  object \n 2   Product_ID        11251 non-null  object \n 3   Gender            11251 non-null  object \n 4   Age Group         11251 non-null  object \n 5   Age               11251 non-null  int64  \n 6   Marital_Status    11251 non-null  int64  \n 7   State             11251 non-null  object \n 8   Zone              11251 non-null  object \n 9   Occupation        11251 non-null  object \n 10  Product_Category  11251 non-null  object \n 11  Orders            11251 non-null  int64  \n 12  Amount            11239 non-null  float64\ndtypes: float64(1), int64(4), object(8)\nmemory usage: 1.1+ MB\n\n\n\n\n\n\n\n\n\nUser_ID\nAge\nMarital_Status\nOrders\nAmount\n\n\n\n\ncount\n1.125100e+04\n11251.000000\n11251.000000\n11251.000000\n11239.000000\n\n\nmean\n1.003004e+06\n35.421207\n0.420318\n2.489290\n9453.610858\n\n\nstd\n1.716125e+03\n12.754122\n0.493632\n1.115047\n5222.355869\n\n\nmin\n1.000001e+06\n12.000000\n0.000000\n1.000000\n188.000000\n\n\n25%\n1.001492e+06\n27.000000\n0.000000\n1.500000\n5443.000000\n\n\n50%\n1.003065e+06\n33.000000\n0.000000\n2.000000\n8109.000000\n\n\n75%\n1.004430e+06\n43.000000\n1.000000\n3.000000\n12675.000000\n\n\nmax\n1.006040e+06\n92.000000\n1.000000\n4.000000\n23952.000000\n\n\n\n\n\n\n\nAdd narrative here…\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-02-diwali-sales-A.html#exercise-1",
    "href": "ae/ae-02-diwali-sales-A.html#exercise-1",
    "title": "AE 02: Diwali sales + EDA",
    "section": "",
    "text": "Read in the data:\n\nRead the Diwali sales data into a Pandas DataFrame and display the first few rows.\nHint: use , encoding = 'iso-8859-1' within the pd.___() function.\n\n\n\nimport pandas as pd\n\n# Read in the data\ndiwali = pd.read_csv('data/diwali_sales_data.csv', encoding='iso-8859-1')\ndiwali.head()\n\n\n\n\n\n\n\n\nUser_ID\nCust_name\nProduct_ID\nGender\nAge Group\nAge\nMarital_Status\nState\nZone\nOccupation\nProduct_Category\nOrders\nAmount\n\n\n\n\n0\n1002903\nSanskriti\nP00125942\nF\n26-35\n28\n0\nMaharashtra\nWestern\nHealthcare\nAuto\n1\n23952.0\n\n\n1\n1000732\nKartik\nP00110942\nF\n26-35\n35\n1\nAndhra Pradesh\nSouthern\nGovt\nAuto\n3\n23934.0\n\n\n2\n1001990\nBindu\nP00118542\nF\n26-35\n35\n1\nUttar Pradesh\nCentral\nAutomobile\nAuto\n3\n23924.0\n\n\n3\n1001425\nSudevi\nP00237842\nM\n0-17\n16\n0\nKarnataka\nSouthern\nConstruction\nAuto\n2\n23912.0\n\n\n4\n1000588\nJoni\nP00057942\nM\n26-35\n28\n1\nGujarat\nWestern\nFood Processing\nAuto\n2\n23877.0\n\n\n\n\n\n\n\n\nExamine the Data:\n\nDisplay basic information about the dataset using the .info() method.\nDisplay summary statistics for the numerical columns using the .describe() method.\n\n\n\n# Examine the data\ndiwali.info()\n\n# Describe numerical columns\ndiwali.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11251 entries, 0 to 11250\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   User_ID           11251 non-null  int64  \n 1   Cust_name         11251 non-null  object \n 2   Product_ID        11251 non-null  object \n 3   Gender            11251 non-null  object \n 4   Age Group         11251 non-null  object \n 5   Age               11251 non-null  int64  \n 6   Marital_Status    11251 non-null  int64  \n 7   State             11251 non-null  object \n 8   Zone              11251 non-null  object \n 9   Occupation        11251 non-null  object \n 10  Product_Category  11251 non-null  object \n 11  Orders            11251 non-null  int64  \n 12  Amount            11239 non-null  float64\ndtypes: float64(1), int64(4), object(8)\nmemory usage: 1.1+ MB\n\n\n\n\n\n\n\n\n\nUser_ID\nAge\nMarital_Status\nOrders\nAmount\n\n\n\n\ncount\n1.125100e+04\n11251.000000\n11251.000000\n11251.000000\n11239.000000\n\n\nmean\n1.003004e+06\n35.421207\n0.420318\n2.489290\n9453.610858\n\n\nstd\n1.716125e+03\n12.754122\n0.493632\n1.115047\n5222.355869\n\n\nmin\n1.000001e+06\n12.000000\n0.000000\n1.000000\n188.000000\n\n\n25%\n1.001492e+06\n27.000000\n0.000000\n1.500000\n5443.000000\n\n\n50%\n1.003065e+06\n33.000000\n0.000000\n2.000000\n8109.000000\n\n\n75%\n1.004430e+06\n43.000000\n1.000000\n3.000000\n12675.000000\n\n\nmax\n1.006040e+06\n92.000000\n1.000000\n4.000000\n23952.000000\n\n\n\n\n\n\n\nAdd narrative here…\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-02-diwali-sales-A.html#exercise-2",
    "href": "ae/ae-02-diwali-sales-A.html#exercise-2",
    "title": "AE 02: Diwali sales + EDA",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nExploring unique levels, outliers, and missing values\n\nExploring Unique Levels:\n\nIdentify and display the unique values in each categorical column.\n\n\n\n# Select categorical columns\ncategorical_cols = diwali.select_dtypes(include=['object']).columns\n\n# Display unique levels for each categorical column\nfor col in categorical_cols:\n    print(f\"Unique levels in {col}: {diwali[col].unique()}\")\n\nUnique levels in Cust_name: ['Sanskriti' 'Kartik' 'Bindu' ... 'Chris' 'Madan Mohan' 'Nicole']\nUnique levels in Product_ID: ['P00125942' 'P00110942' 'P00118542' ... 'P00307142' 'P00044742'\n 'P00296942']\nUnique levels in Gender: ['F' 'M']\nUnique levels in Age Group: ['26-35' '0-17' '18-25' '55+' '46-50' '51-55' '36-45']\nUnique levels in State: ['Maharashtra' 'Andhra Pradesh' 'Uttar Pradesh' 'Karnataka' 'Gujarat'\n 'Himachal Pradesh' 'Delhi' 'Madhya Pradesh' 'Jharkhand' 'Kerala'\n 'Haryana' 'Bihar' 'Rajasthan' 'Uttarakhand' 'Telangana' 'Punjab']\nUnique levels in Zone: ['Western' 'Southern' 'Central' 'Northern' 'Eastern']\nUnique levels in Occupation: ['Healthcare' 'Govt' 'Automobile' 'Construction' 'Food Processing'\n 'Lawyer' 'IT Sector' 'Media' 'Banking' 'Retail' 'Hospitality' 'Aviation'\n 'Agriculture' 'Textile' 'Chemical']\nUnique levels in Product_Category: ['Auto' 'Hand & Power Tools' 'Stationery' 'Tupperware' 'Footwear & Shoes'\n 'Furniture' 'Food' 'Games & Toys' 'Sports Products' 'Books'\n 'Electronics & Gadgets' 'Decor' 'Clothing & Apparel' 'Beauty'\n 'Household items' 'Pet Care' 'Veterinary' 'Office']\n\n\n\nIdentifying and Visualizing Outliers:\n\nCreate a box plot to visualize outliers in the ‘Amount’ column.\nIdentify outliers using the IQR method and count the number of outliers for each numerical column.\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Boxplot to visualize outliers\nsns.boxplot(data=diwali, x='Amount')\nplt.show()\n\n# Identify outliers using the IQR method\nfor col in diwali.select_dtypes(include='number').columns:\n    q25 = diwali[col].quantile(0.25)\n    q75 = diwali[col].quantile(0.75)\n    iqr = q75 - q25\n    lower_bound = q25 - 1.5 * iqr\n    upper_bound = q75 + 1.5 * iqr\n    outliers = diwali[(diwali[col] &lt; lower_bound) | (diwali[col] &gt; upper_bound)]\n    print(f\"{col}: {outliers.shape[0]} outliers\")\n\n\n\n\n\n\n\n\nUser_ID: 0 outliers\nAge: 283 outliers\nMarital_Status: 0 outliers\nOrders: 0 outliers\nAmount: 19 outliers\n\n\n\nHandling Missing Values:\n\nCheck for missing values in the dataset.\n\n\n\n# Check for missing values\ndiwali.isnull().sum()\n\nUser_ID              0\nCust_name            0\nProduct_ID           0\nGender               0\nAge Group            0\nAge                  0\nMarital_Status       0\nState                0\nZone                 0\nOccupation           0\nProduct_Category     0\nOrders               0\nAmount              12\ndtype: int64\n\n\nAdd narrative here…\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-00-unvotes.html",
    "href": "ae/ae-00-unvotes.html",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the pandas, matplotlib, and seaborn packages for data wrangling and visualization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nThe data we’re using originally come from the unvotes R package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nun_votes = pd.read_csv('data/un_votes.csv')\nun_roll_calls = pd.read_csv('data/un_roll_calls.csv')\nun_roll_call_issues = pd.read_csv('data/un_roll_call_issues.csv')\n\nunvotes = un_votes.merge(un_roll_calls, on='rcid').merge(un_roll_call_issues, on='rcid')"
  },
  {
    "objectID": "ae/ae-00-unvotes.html#introduction",
    "href": "ae/ae-00-unvotes.html#introduction",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the pandas, matplotlib, and seaborn packages for data wrangling and visualization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nThe data we’re using originally come from the unvotes R package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nun_votes = pd.read_csv('data/un_votes.csv')\nun_roll_calls = pd.read_csv('data/un_roll_calls.csv')\nun_roll_call_issues = pd.read_csv('data/un_roll_call_issues.csv')\n\nunvotes = un_votes.merge(un_roll_calls, on='rcid').merge(un_roll_call_issues, on='rcid')"
  },
  {
    "objectID": "ae/ae-00-unvotes.html#un-voting-patterns",
    "href": "ae/ae-00-unvotes.html#un-voting-patterns",
    "title": "UN Votes",
    "section": "UN voting patterns",
    "text": "UN voting patterns\nLet’s create a data visualization that displays how the voting record of the UK & NI changed over time on a variety of issues and compares it to two other countries: US and Turkey.\nWe can easily change which countries are being plotted by changing which countries the code filters for. Note that the country name should be spelled and capitalized exactly the same way as it appears in the data. See the Appendix for a list of the countries in the data.\n\n# Filter the data for the selected countries and prepare for plotting\nfiltered_unvotes = unvotes[unvotes['country'].isin(['United Kingdom', 'United States', 'Turkey'])]\nfiltered_unvotes['year'] = pd.to_datetime(filtered_unvotes['date']).dt.year\n\n# Calculate the percentage of 'yes' votes per year, per country, per issue\npercent_yes = filtered_unvotes.groupby(['country', 'year', 'issue'])['vote'].apply(lambda x: (x == 'yes').mean()).reset_index()\npercent_yes.rename(columns={'vote': 'percent_yes'}, inplace=True)\n\n# Create the faceted plot\ng = sns.FacetGrid(percent_yes, col=\"issue\", hue=\"country\", col_wrap=3)\ng.map(sns.scatterplot, \"year\", \"percent_yes\", alpha=0.4)\ng.map(sns.regplot, \"year\", \"percent_yes\", lowess=True, scatter=False)\n\n# Adjust the labels and titles \ng.set_axis_labels(\"Year\", \"% Yes\")\ng.set_titles(col_template=\"{col_name}\")\ng.add_legend(title=\"Country\")\ng.fig.suptitle(\"Percentage of 'Yes' votes in the UN General Assembly\", y=1.02)\nplt.subplots_adjust(top=0.9)\n\nplt.show()"
  },
  {
    "objectID": "ae/ae-00-unvotes.html#references",
    "href": "ae/ae-00-unvotes.html#references",
    "title": "UN Votes",
    "section": "References",
    "text": "References\n\nDavid Robinson (2017). unvotes: United Nations General Assembly Voting Data. R package version 0.2.0.\nErik Voeten “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013).\nMuch of the analysis has been modeled on the examples presented in the unvotes R package vignette."
  },
  {
    "objectID": "ae/ae-00-unvotes.html#appendix",
    "href": "ae/ae-00-unvotes.html#appendix",
    "title": "UN Votes",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of countries in the dataset:\n\n\n\n\n\n\n\n\n\ncountry\n\n\n\n\n0\nAfghanistan\n\n\n1\nAlbania\n\n\n2\nAlgeria\n\n\n3\nAndorra\n\n\n4\nAngola\n\n\n...\n...\n\n\n195\nYemen People's Republic\n\n\n196\nYugoslavia\n\n\n197\nZambia\n\n\n198\nZanzibar\n\n\n199\nZimbabwe\n\n\n\n\n200 rows × 1 columns"
  },
  {
    "objectID": "ae/ae-01-meet-the-penguins.html",
    "href": "ae/ae-01-meet-the-penguins.html",
    "title": "AE 01: Meet the penguins",
    "section": "",
    "text": "For this application exercise, we’ll use the pandas and seaborn packages.\n\nimport pandas as pd\nimport seaborn as sns\n\nThe dataset we will visualize is called penguins. Let’s .info() it.\n\nYour turn: Replace #add code here with the code for “info”ing at the data penguins data frame – penguins.info(). Render the document and view the output.\n\n\npenguins = sns.load_dataset(\"data/penguins\")\n# add code here\n\n\nDemo: First, replace the blank below with the number of rows in the penguins data frame based on the output of the chunk below. Then, replace it with “inline code” and render again.\n\n\nnum_rows = len(penguins)\nnum_rows\n\n344\n\n\nThere are ___ penguins in the penguins data frame.\n\nx = 2\nx * 3\n\n6"
  },
  {
    "objectID": "ae/ae-06-wildcat-scrape.html",
    "href": "ae/ae-06-wildcat-scrape.html",
    "title": "AE 06: Opinion articles in The Arizona Daily Wildcat",
    "section": "",
    "text": "This will be done in the wildcat-scrape.py Python script within the ae-06-wildcat-scrape folder. Save the resulting data frame in the data/ folder."
  },
  {
    "objectID": "ae/ae-06-wildcat-scrape.html#part-1---data-scraping",
    "href": "ae/ae-06-wildcat-scrape.html#part-1---data-scraping",
    "title": "AE 06: Opinion articles in The Arizona Daily Wildcat",
    "section": "",
    "text": "This will be done in the wildcat-scrape.py Python script within the ae-06-wildcat-scrape folder. Save the resulting data frame in the data/ folder."
  },
  {
    "objectID": "ae/ae-06-wildcat-scrape.html#part-2---data-analysis",
    "href": "ae/ae-06-wildcat-scrape.html#part-2---data-analysis",
    "title": "AE 06: Opinion articles in The Arizona Daily Wildcat",
    "section": "Part 2 - Data analysis",
    "text": "Part 2 - Data analysis\nLet’s start by loading the packages we will need:\n\nimport pandas as pd\n\n\nLoad the data you saved into the data folder and name it wildcat.\n\n\n# add code here\n\n\nWho are the most prolific authors of the 100 most recent opinion articles in The Arizona Daily Wildcat?\n\n\n# add code here\n\n\nDraw a line plot of the number of opinion articles published per day in The Arizona Daily Wildcat.\n\n\n# add code here\n\n\nWhat percent of the most recent 100 opinion articles in The Arizona Daily Wildcat mention “climate” in their title?\n\n\n# add code here\n\n\nWhat percent of the most recent 100 opinion articles in The Arizona Daily Wildcat mention “election” in their title or abstract?\n\n\n# add code here\n\n\nCome up with another question and try to answer it using the data.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-13-derivation.html",
    "href": "ae/ae-13-derivation.html",
    "title": "AE 13: Derivation",
    "section": "",
    "text": "In this exercise, we will:"
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-1",
    "href": "ae/ae-13-derivation.html#exercise-1",
    "title": "AE 13: Derivation",
    "section": "Exercise 1:",
    "text": "Exercise 1:\nFunction:\n\\[\nf(x) = 5x^3\n\\]\nSolution:\n\nThe power rule states that \\(\\frac{d}{dx}(x^n)=nx^{n-1}\\)\nApplying the power rule:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-2",
    "href": "ae/ae-13-derivation.html#exercise-2",
    "title": "AE 13: Derivation",
    "section": "Exercise 2",
    "text": "Exercise 2\nFunction:\n\\[\ng(x) = \\sqrt{x}\n\\]\nSolution:\n\nRewrite the function with a fractional exponent: \\(\\sqrt{x}=x^{1/2}\\).\nApply the power rule:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-3",
    "href": "ae/ae-13-derivation.html#exercise-3",
    "title": "AE 13: Derivation",
    "section": "Exercise 3",
    "text": "Exercise 3\nFunction:\n\\[\nh(x)=\\ln(x)\n\\]\nSolution:\n\nThe derivative of the natural logarithm function is ___\n\nadd response here."
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-4",
    "href": "ae/ae-13-derivation.html#exercise-4",
    "title": "AE 13: Derivation",
    "section": "Exercise 4",
    "text": "Exercise 4\nFunction:\n\\[\nf(x)=(2x^3+3x)^4\n\\]\nSolution\n\nIdentify the outer function and inner function:\n\nOuter function: add response here.\nInner function: add response here.\n\nApply the chain rule:\n\nadd response here.\n\nDifferentiate the inner function:\n\nadd response here.\n\nCombine the results:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-5",
    "href": "ae/ae-13-derivation.html#exercise-5",
    "title": "AE 13: Derivation",
    "section": "Exercise 5",
    "text": "Exercise 5\nFunction:\n\\[\n\\frac{d}{dx}(x^2 e^x)\n\\]\nSolution:\n\nIdentify the product of two functions: ___ and ___\nApply the product rule: \\((u \\cdot v)^{'}=u^{'} \\cdot v + u \\cdot v^{'}\\)\nDifferentiate each function:\n\nadd response here.\n\nCombine the results:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-6",
    "href": "ae/ae-13-derivation.html#exercise-6",
    "title": "AE 13: Derivation",
    "section": "Exercise 6",
    "text": "Exercise 6\nFunction:\n\\[\nh(x)=\\sin (x^2)\n\\]\nSolution:\n\nIdentify the outer function and inner function:\n\nOuter function: add response here.\nInner function: add response here.\n\nApply the chain rule: add response here.\nDifferentiate the inner function:\n\nadd response here.\n\nCombine the results:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-13-derivation.html#exercise-7",
    "href": "ae/ae-13-derivation.html#exercise-7",
    "title": "AE 13: Derivation",
    "section": "Exercise 7",
    "text": "Exercise 7\nFunction:\n\\[\nf(x)=(\\ln (x) \\cdot e^{2x})^3\n\\]\nSolution\n\nIdentify the outer function and inner function\n\nOuter function: add response here.\nInner function: add response here.\n\nApply the chain rule: add response here.\nUse the product rule to differentiate the inner function:\n\nadd response here.\n\nDifferentiate each function:\n\nadd response here.\n\nApply the product rule:\n\nadd response here.\n\nCombine the outer function derivative:\n\nadd response here.\n\nSimplify:\n\nadd response here."
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html",
    "href": "ae/ae-10-modeling-fish.html",
    "title": "AE 10: Modelling fish",
    "section": "",
    "text": "For this application exercise, we will work with data on fish. The dataset we will use, called fish, is on two common fish species in fish market sales.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nfish = pd.read_csv(\"data/fish.csv\")\nThe data dictionary is below:"
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html#visualizing-the-model",
    "href": "ae/ae-10-modeling-fish.html#visualizing-the-model",
    "title": "AE 10: Modelling fish",
    "section": "Visualizing the model",
    "text": "Visualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish.\n\nCreate an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\n# add code here\n\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\nAdd response here.\n\nNow, let Python draw the line for you. Refer to the documentation at https://seaborn.pydata.org/generated/seaborn.lmplot.html. Specifically, refer to the method section.\n\n\n# add code here\n\n\nWhat types of questions can this plot help answer?\n\nAdd response here.\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\nAdd response here.\n\nWhat is a residual?\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html#model-fitting",
    "href": "ae/ae-10-modeling-fish.html#model-fitting",
    "title": "AE 10: Modelling fish",
    "section": "Model fitting",
    "text": "Model fitting\n\nFit a model to predict fish weights from their heights.\n\n\n# add code here\n\n\nPredict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\n# add code here\n\n\nCalculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html#model-summary",
    "href": "ae/ae-10-modeling-fish.html#model-summary",
    "title": "AE 10: Modelling fish",
    "section": "Model summary",
    "text": "Model summary\n\nDisplay the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\n# add code here\n\n\nWrite out your model using mathematical notation.\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html#correlation",
    "href": "ae/ae-10-modeling-fish.html#correlation",
    "title": "AE 10: Modelling fish",
    "section": "Correlation",
    "text": "Correlation\nWe can also assess correlation between two quantitative variables.\n\nWhat is correlation? What are values correlation can take?\nAdd response here.\n\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\n\nWhat is the correlation between heights and weights of fish?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html#adding-a-third-variable",
    "href": "ae/ae-10-modeling-fish.html#adding-a-third-variable",
    "title": "AE 10: Modelling fish",
    "section": "Adding a third variable",
    "text": "Adding a third variable\n\nDoes the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html#fitting-other-models",
    "href": "ae/ae-10-modeling-fish.html#fitting-other-models",
    "title": "AE 10: Modelling fish",
    "section": "Fitting other models",
    "text": "Fitting other models\n\nWe can fit more models than just a straight line. Change the sns.regplot() code you previously used to include lowess=True. What is different from the plot created before?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-pca.html",
    "href": "ae/ae-16-pca.html",
    "title": "AE 16: Principal component analysis",
    "section": "",
    "text": "In this application exercise, we will:"
  },
  {
    "objectID": "ae/ae-16-pca.html#exercise-1",
    "href": "ae/ae-16-pca.html#exercise-1",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nWatch this video on Principal Component Analysis:\n\n\n\nWhat were three takeaways from this video? Include how you think linear algebra contributes to PCA:\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-pca.html#packages",
    "href": "ae/ae-16-pca.html#packages",
    "title": "AE 16: Principal component analysis",
    "section": "Packages",
    "text": "Packages\nWe will primarily use the seaborn and sklearn packages.\n\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ae/ae-16-pca.html#exercise-2",
    "href": "ae/ae-16-pca.html#exercise-2",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nLoad the Penguins Dataset using seaborn\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-pca.html#exercise-3",
    "href": "ae/ae-16-pca.html#exercise-3",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 3",
    "text": "Exercise 3\nPreprocess the data\nWe need to handle missing values and select the numerical features for PCA.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-pca.html#exercise-4",
    "href": "ae/ae-16-pca.html#exercise-4",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 4",
    "text": "Exercise 4\nPerform PCA\nUse PCA from sklearn to reduce the dimensionality of the data. Hint: use two principal components\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-pca.html#exercise-5",
    "href": "ae/ae-16-pca.html#exercise-5",
    "title": "AE 16: Principal component analysis",
    "section": "Exercise 5",
    "text": "Exercise 5\nVisualize the PCA Result\nUse seaborn to visualize the principal components.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html",
    "href": "ae/ae-09-hypothesis-testing.html",
    "title": "AE 09: Hypothesis testing",
    "section": "",
    "text": "An article in the The Tucson Citizen-Times published in the summer of 2020 claims that the average price per guest (ppg) for properties in Tucson is $100 on Airbnb. To evaluate their claim we will use a dataset on 50 randomly selected Asheville Airbnb listings in July 2024. These data can be found in data/tucson.csv.\nLet’s load the packages we’ll use first.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nAnd then the data.\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html#hypotheses",
    "href": "ae/ae-09-hypothesis-testing.html#hypotheses",
    "title": "AE 09: Hypothesis testing",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nWrite out the correct null and alternative hypothesis. Do this in both words and in proper notation.\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html#observed-data",
    "href": "ae/ae-09-hypothesis-testing.html#observed-data",
    "title": "AE 09: Hypothesis testing",
    "section": "Observed data",
    "text": "Observed data\nOur goal is to use calculate the probability of a sample statistic at least as extreme as the one observed in our data if in fact the null hypothesis is true.\n\nCalculate and report the sample statistic below using proper notation.\n\n\n# add code here\n\n\\[\\bar{x} = 116.24\\]"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html#the-null-distribution",
    "href": "ae/ae-09-hypothesis-testing.html#the-null-distribution",
    "title": "AE 09: Hypothesis testing",
    "section": "The null distribution",
    "text": "The null distribution\nLet’s use simulation-based methods to conduct the hypothesis test specified above.\n\nGenerate\nWe’ll start by generating the null distribution.\n\nGenerate the null distribution and call it null_dist\n\n\nnp.random.seed(4321)\n\n\n# add code here\n\n\nTake a look at null_dist. What does each element in this distribution represent?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html#visualize",
    "href": "ae/ae-09-hypothesis-testing.html#visualize",
    "title": "AE 09: Hypothesis testing",
    "section": "Visualize",
    "text": "Visualize\n\nQuestion: Before you visualize the distribution of null_dist – at what value would you expect this distribution to be centered? Why?\n\nAdd response here.\n\nCreate an appropriate visualization for your null distribution. Does the center of the distribution match what you guessed in the previous question?\n\n\n# add code here\n\n\nNow, add a vertical red line on your null distribution that represents your sample statistic.\n\n\n# add code here\n\n\nQuestion: Based on the position of this line, does your observed sample mean appear to be an unusual observation under the assumption of the null hypothesis?\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html#p-value",
    "href": "ae/ae-09-hypothesis-testing.html#p-value",
    "title": "AE 09: Hypothesis testing",
    "section": "p-value",
    "text": "p-value\nAbove, we eyeballed how likely/unlikely our observed mean is. Now, let’s actually quantify it using a p-value.\n\nQuestion: What is a p-value?\n\nAdd response here.\n\nDemo: Visualize the p-value. Note that the two-sided approach would visualize two lines, one for the sample mean and another for \\(H_0 - (\\mu - H_0)\\) or \\(100 - (\\mu - 100)\\)\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-09-hypothesis-testing.html#conclusion",
    "href": "ae/ae-09-hypothesis-testing.html#conclusion",
    "title": "AE 09: Hypothesis testing",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat is the conclusion of the hypothesis test based on the p-value you calculated? Make sure to frame it in context of the data and the research question. Use a significance level of 5% to make your conclusion.\n\nAdd response here.\n\nInterpret the p-value in context of the data and the research question.\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-07-probability.html",
    "href": "ae/ae-07-probability.html",
    "title": "AE 07: Practicing with probabilities",
    "section": "",
    "text": "Understand and calculate basic probabilities using a real-world dataset."
  },
  {
    "objectID": "ae/ae-07-probability.html#goal",
    "href": "ae/ae-07-probability.html#goal",
    "title": "AE 07: Practicing with probabilities",
    "section": "",
    "text": "Understand and calculate basic probabilities using a real-world dataset."
  },
  {
    "objectID": "ae/ae-07-probability.html#data",
    "href": "ae/ae-07-probability.html#data",
    "title": "AE 07: Practicing with probabilities",
    "section": "Data",
    "text": "Data\nA cohort study on coffee consumption and mortality from:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid not die\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601\n\n\n\nSource: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5788283/"
  },
  {
    "objectID": "ae/ae-07-probability.html#definitions",
    "href": "ae/ae-07-probability.html#definitions",
    "title": "AE 07: Practicing with probabilities",
    "section": "Definitions:",
    "text": "Definitions:\n\nEvent A: The person died.\nEvent B: The person is a non-coffee drinker."
  },
  {
    "objectID": "ae/ae-07-probability.html#two-important-rules",
    "href": "ae/ae-07-probability.html#two-important-rules",
    "title": "AE 07: Practicing with probabilities",
    "section": "Two important rules",
    "text": "Two important rules\nSuppose we have events \\(A\\) and \\(B\\), with probabilities \\(P(A)\\) and \\(P(B)\\) of occurring. Based on the Kolmogorov axioms:\n\nComplement Rule: \\(P(A^c) = 1 - P(A)\\)\nInclusion-Exclusion: \\(P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\\)"
  },
  {
    "objectID": "ae/ae-07-probability.html#exercises",
    "href": "ae/ae-07-probability.html#exercises",
    "title": "AE 07: Practicing with probabilities",
    "section": "Exercises",
    "text": "Exercises\nCalculate the following probabilities for a randomly selected person in the cohort:\n\nCalculate the total number of people in the cohort:\n\\(Total=5438+1039+29712+4440+24934+3601\\)\n\nAdd response here.\n\nCalculate \\(\\small{P(A)}\\): Probability that the person died\n\\(\\small{P(A)} = \\frac{\\text{Total number of people who died}}{\\text{Total number of people in the cohort}}\\)\n\nAdd response here.\n\nCalculate \\(\\small{P(B)}\\): Probability that the person is a non-coffee drinker\n\\(\\small{P(B)} = \\frac{\\text{Total number of non-coffee drinkers}}{\\text{Total number of people in the cohort}}\\)\n\nAdd response here.\n\nCalculate \\(\\small{P(A \\text{ and } B)}\\): Probability that the person died and is a non-coffee drinker\n\\(\\small{P(A \\cap B)}=\\frac{\\text{Number of people who died and are non-coffee drinks}}{\\text{Total number of people in the cohort}}\\)\n\nAdd response here.\n\nCalculate \\(\\small{P(A \\text{ or } B)}\\) using the formula for the union of two events:\n\\(\\small{P(A \\cup B)}=\\small{P(A)}+\\small{P(B)}-\\small{P(A \\cap B)}\\)\n\nAdd response here.\n\nCalculate \\(\\small{P(A \\text{ or } B^c)}\\): Probability that the person died or is a non-coffee drinker\n\\(\\small{P(A \\cup B^c)}=\\small{P(A)}+\\small{P(B^c)}-\\small{P(A \\cap B^c)}\\)\n\nAdd response here.\nNote: \\(\\small{P(B^c)}\\) is the complement of event \\(B\\) (i.e., people who drink coffee occasionally or regularly).\n\nDiscussion Questions:\n\nWhat do these probabilities tell us about the relationship between coffee consumption and mortality in this cohort?\nAre there any limitations to interpreting these probabilities as causal effects?\nHow might additional factors or confounders influence these probabilities?"
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html",
    "href": "ae/ae-12-candy-ranking.html",
    "title": "AE 12: Ultimate candy ranking",
    "section": "",
    "text": "In this application exercise, we will:\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-1",
    "href": "ae/ae-12-candy-ranking.html#exercise-1",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 1",
    "text": "Exercise 1\nCreate the full model and show the \\(R^2_{adj}\\):\n\n# add code here\n\nIs the model a good fit of the data?\nAdd response here."
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-2",
    "href": "ae/ae-12-candy-ranking.html#exercise-2",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 2",
    "text": "Exercise 2\nProduce all possible models removing 1 term at a time from the full model. Describe what is being removed above each code cell.\n\n# Blank dictionary to store new models\nmodels = {}\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-3",
    "href": "ae/ae-12-candy-ranking.html#exercise-3",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 3",
    "text": "Exercise 3\nCompare all models using the framework (also use the same below):\n\nbest_model_step1 = max(models, key=models.get)\nprint(f'Best model in Exercise 2: {best_model_step1} with Adjusted R-squared: {models[best_model_step1]}')\n\n\nWhich model is best:\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-4",
    "href": "ae/ae-12-candy-ranking.html#exercise-4",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate all possible models removing 1 term at a time from the model selected in the previous exercise. Again, describe what is being removed above each code cell.\n\n# Blank dictionary to store new models\nmodels = {}\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-5",
    "href": "ae/ae-12-candy-ranking.html#exercise-5",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 5",
    "text": "Exercise 5\nCompare all models using the framework best_model_step2 = max(models, key=models.get):\n\n# add code here\n\n\nWhich model is best:\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-6",
    "href": "ae/ae-12-candy-ranking.html#exercise-6",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate all possible models removing 1 term at a time from the model selected in the previous step. Again, describe what is being removed above each code cell.\n\n# Blank dictionary to store new models\nmodels = {}\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here\n\n\nAdd what is being removed here.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-12-candy-ranking.html#exercise-7",
    "href": "ae/ae-12-candy-ranking.html#exercise-7",
    "title": "AE 12: Ultimate candy ranking",
    "section": "Exercise 7",
    "text": "Exercise 7\nCompare all models using the framework best_model_step3 = max(models, key=models.get):\n\n# add code here\n\n\nWhich model is best:\n\nAdd response here\n\nShow the final model summary and coefficients:\n\n\n# add code here"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for INFO 511 - Fundamentals of Data Science taught by Dr. Shannon McWaters in Fall 2025 at The University of Arizona.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nOnline\nAsynchronous",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Shannon McWaters (she/her) is an instructor at the College of Information Science at The University of Arizona. Shanon is a data scientist, educator, and behavioral ecologist.\n\n\n\nOffice hours\nLocation\n\n\n\n\nBy appointment\nZoom",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Shannon McWaters (she/her) is an instructor at the College of Information Science at The University of Arizona. Shanon is a data scientist, educator, and behavioral ecologist.\n\n\n\nOffice hours\nLocation\n\n\n\n\nBy appointment\nZoom",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "slides/25-final-review.html#final-assignment",
    "href": "slides/25-final-review.html#final-assignment",
    "title": "Final review",
    "section": "Final Assignment",
    "text": "Final Assignment\nSame as Midterm assignment!\n\nAssignment format / flow\nAsking questions!"
  },
  {
    "objectID": "slides/25-final-review.html#concepts-covered-midterm",
    "href": "slides/25-final-review.html#concepts-covered-midterm",
    "title": "Final review",
    "section": "Concepts covered (+ Midterm)",
    "text": "Concepts covered (+ Midterm)\n\n\nA hypothesis test is a statistical technique used to evaluate competing claims (null and alternative hypotheses) using data.\nWe simulate a null distribution using our original data.\nWe use our sample statistic and direction of the alternative hypothesis to calculate the p-value.\nWe use the p-value to determine conclusions about the alternative hypotheses.\nDerivative Calculation: Computing the derivative of a given function.\nIntegral Calculation: Solving an integral involving exponential and power functions.\nVector and Matrix Transpose: Writing down the transpose of vectors and matrices.\nMatrix Dimensions: Determining the dimensions of matrices and the validity of matrix products.\nMatrix Multiplication: Performing matrix multiplication and understanding the resulting matrix dimensions.\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/19-validation.html#setup",
    "href": "slides/19-validation.html#setup",
    "title": "Model validation",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\", rc={\"figure.figsize\": (8, 6), \"axes.labelsize\": 16, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})"
  },
  {
    "objectID": "slides/19-validation.html#data-candy-rankings",
    "href": "slides/19-validation.html#data-candy-rankings",
    "title": "Model validation",
    "section": "Data: Candy Rankings",
    "text": "Data: Candy Rankings\n\ncandy_rankings = pd.read_csv(\"data/candy_rankings.csv\")\n\ncandy_rankings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 85 entries, 0 to 84\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   competitorname    85 non-null     object \n 1   chocolate         85 non-null     bool   \n 2   fruity            85 non-null     bool   \n 3   caramel           85 non-null     bool   \n 4   peanutyalmondy    85 non-null     bool   \n 5   nougat            85 non-null     bool   \n 6   crispedricewafer  85 non-null     bool   \n 7   hard              85 non-null     bool   \n 8   bar               85 non-null     bool   \n 9   pluribus          85 non-null     bool   \n 10  sugarpercent      85 non-null     float64\n 11  pricepercent      85 non-null     float64\n 12  winpercent        85 non-null     float64\ndtypes: bool(9), float64(3), object(1)\nmemory usage: 3.5+ KB"
  },
  {
    "objectID": "slides/19-validation.html#overfitting",
    "href": "slides/19-validation.html#overfitting",
    "title": "Model validation",
    "section": "Overfitting",
    "text": "Overfitting\n\n\nThe data we are using to construct our models come from a larger population.\nUltimately we want our model to tell us how the population works, not just the sample we have.\nIf the model we fit is too tailored to our sample, it might not perform as well with the remaining population. This means the model is “overfitting” our data.\nWe measure this using model validation techniques.\nNote: Overfitting is not a huge concern with linear models with low level interactions, however it can be with more complex and flexible models. The following is just an example of model validation, even though we’re using it in a scenario where the concern for overfitting is not high."
  },
  {
    "objectID": "slides/19-validation.html#model-validation-1",
    "href": "slides/19-validation.html#model-validation-1",
    "title": "Model validation",
    "section": "Model validation",
    "text": "Model validation\n\n\nOne commonly used model validation technique is to partition your data into training and testing set\nThat is, fit the model on the training data\nAnd test it on the testing data\nEvaluate model performance using \\(RMSE\\), root-mean squared error\n\n\n\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} \\]\n\n\n\nDo you think we should prefer low or high RMSE?"
  },
  {
    "objectID": "slides/19-validation.html#random-sampling-and-reproducibility",
    "href": "slides/19-validation.html#random-sampling-and-reproducibility",
    "title": "Model validation",
    "section": "Random sampling and reproducibility",
    "text": "Random sampling and reproducibility\nGotta set a seed!\n\nnp.random.seed(1234)\n\n\n\nUse different seeds from each other\nNeed inspiration? https://www.random.org/"
  },
  {
    "objectID": "slides/19-validation.html#cross-validation",
    "href": "slides/19-validation.html#cross-validation",
    "title": "Model validation",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, k-fold cross validation\n\n\n\n\nSplit your data into k folds.\nUse 1 fold for testing and the remaining (k - 1) folds for training.\nRepeat k times."
  },
  {
    "objectID": "slides/19-validation.html#prepping-your-data-for-5-fold-cv",
    "href": "slides/19-validation.html#prepping-your-data-for-5-fold-cv",
    "title": "Model validation",
    "section": "Prepping your data for 5-fold CV",
    "text": "Prepping your data for 5-fold CV\n\ncandy_rankings['id'] = np.arange(len(candy_rankings))\ncandy_rankings = candy_rankings.sample(frac=1).reset_index(drop=True)\ncandy_rankings['fold'] = (np.arange(len(candy_rankings)) % 5) + 1\n\ncandy_rankings_cv = candy_rankings.groupby('fold').size().reset_index(name='count')\nprint(candy_rankings_cv)\n\n   fold  count\n0     1     17\n1     2     17\n2     3     17\n3     4     17\n4     5     17"
  },
  {
    "objectID": "slides/19-validation.html#cv-1",
    "href": "slides/19-validation.html#cv-1",
    "title": "Model validation",
    "section": "CV 1",
    "text": "CV 1\n\ntest_fold = 1\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\n\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\nrmse_test1 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nprint(f'RMSE Test 1: {rmse_test1}')\n\nRMSE Test 1: 10.15009006066606"
  },
  {
    "objectID": "slides/19-validation.html#rmse-on-training-vs.-testing",
    "href": "slides/19-validation.html#rmse-on-training-vs.-testing",
    "title": "Model validation",
    "section": "RMSE on training vs. testing",
    "text": "RMSE on training vs. testing\n\nWould you expect the RMSE to be higher for your training data or your testing data? Why?"
  },
  {
    "objectID": "slides/19-validation.html#rmse-on-training-vs.-testing-1",
    "href": "slides/19-validation.html#rmse-on-training-vs.-testing-1",
    "title": "Model validation",
    "section": "RMSE on training vs. testing",
    "text": "RMSE on training vs. testing\nRMSE for testing:\n\nrmse_test1 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nprint(f'RMSE Test 1: {rmse_test1}')\n\nRMSE Test 1: 10.15009006066606\n\n\nRMSE for training:\n\nrmse_train1 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Train 1: {rmse_train1}')\n\nRMSE Train 1: 10.117840760774037"
  },
  {
    "objectID": "slides/19-validation.html#cv-2",
    "href": "slides/19-validation.html#cv-2",
    "title": "Model validation",
    "section": "CV 2",
    "text": "CV 2\n\ntest_fold = 2\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n\n\nrmse_test2 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train2 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 2: {rmse_test2}')\nprint(f'RMSE Train 2: {rmse_train2}')\n\nRMSE Test 2: 10.432240639004073\nRMSE Train 2: 10.027804484574576"
  },
  {
    "objectID": "slides/19-validation.html#cv-3",
    "href": "slides/19-validation.html#cv-3",
    "title": "Model validation",
    "section": "CV 3",
    "text": "CV 3\n\ntest_fold = 3\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n\n\nrmse_test3 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train3 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 3: {rmse_test3}')\nprint(f'RMSE Train 3: {rmse_train3}')\n\nRMSE Test 3: 11.95850312007085\nRMSE Train 3: 9.801042089669558"
  },
  {
    "objectID": "slides/19-validation.html#cv-4",
    "href": "slides/19-validation.html#cv-4",
    "title": "Model validation",
    "section": "CV 4",
    "text": "CV 4\n\ntest_fold = 4\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n\n\nrmse_test4 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train4 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 4: {rmse_test4}')\nprint(f'RMSE Train 4: {rmse_train4}')\n\nRMSE Test 4: 12.39965858487449\nRMSE Train 4: 9.60646325325732"
  },
  {
    "objectID": "slides/19-validation.html#cv-5",
    "href": "slides/19-validation.html#cv-5",
    "title": "Model validation",
    "section": "CV 5",
    "text": "CV 5\n\ntest_fold = 5\ntest = candy_rankings[candy_rankings['fold'] == test_fold]\ntrain = candy_rankings[candy_rankings['fold'] != test_fold]\nmodel = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train).fit()\n\n\nrmse_test5 = np.sqrt(mean_squared_error(test['winpercent'], model.predict(test)))\nrmse_train5 = np.sqrt(mean_squared_error(train['winpercent'], model.predict(train)))\nprint(f'RMSE Test 5: {rmse_test5}')\nprint(f'RMSE Train 5: {rmse_train5}')\n\nRMSE Test 5: 10.01290509309602\nRMSE Train 5: 10.13805597882432"
  },
  {
    "objectID": "slides/19-validation.html#putting-it-altogether",
    "href": "slides/19-validation.html#putting-it-altogether",
    "title": "Model validation",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\n\nDataframe\nrmse_candy = pd.DataFrame({\n    'test_fold': np.arange(1, 6),\n    'rmse_train': [rmse_train1, rmse_train2, rmse_train3, rmse_train4, rmse_train5],\n    'rmse_test': [rmse_test1, rmse_test2, rmse_test3, rmse_test4, rmse_test5]\n})\n\n\n\n\nVisual\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=rmse_candy, x='test_fold', y='rmse_test', marker='o', label='Test RMSE')\nplt.xlabel('Fold')\nplt.ylabel('RMSE')\nplt.title('Test RMSE for each fold')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "slides/19-validation.html#how-does-rmse-compare-to-y",
    "href": "slides/19-validation.html#how-does-rmse-compare-to-y",
    "title": "Model validation",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\n\n\nwinpercent summary stats:\n\n\n\ncount    85.000000\nmean     50.316764\nstd      14.714357\nmin      22.445341\n25%      39.141056\n50%      47.829754\n75%      59.863998\nmax      84.180290\nName: winpercent, dtype: float64\n\n\n\n\n\nrmse_test summary stats:\n\n\n\ncount     5.000000\nmean     10.990679\nstd       1.106390\nmin      10.012905\n25%      10.150090\n50%      10.432241\n75%      11.958503\nmax      12.399659\nName: rmse_test, dtype: float64"
  },
  {
    "objectID": "slides/19-validation.html#model_selection-in-scikit-learn",
    "href": "slides/19-validation.html#model_selection-in-scikit-learn",
    "title": "Model validation",
    "section": "model_selection in scikit-learn",
    "text": "model_selection in scikit-learn\n\n\n\n\nThe scikit-learn package provides functions that help you create pipelines when modeling.\n\nfrom sklearn.model_selection import KFold\n\n\n\n\nmodel selection via scikit-learn"
  },
  {
    "objectID": "slides/19-validation.html#cross-validation---faster",
    "href": "slides/19-validation.html#cross-validation---faster",
    "title": "Model validation",
    "section": "Cross Validation - Faster",
    "text": "Cross Validation - Faster\n\nsklearn.model_selection.KFold: Partition data into k folds\nCalculate RMSEs for each of the models on the testing set"
  },
  {
    "objectID": "slides/19-validation.html#partition-data-into-k-folds",
    "href": "slides/19-validation.html#partition-data-into-k-folds",
    "title": "Model validation",
    "section": "Partition data into k folds",
    "text": "Partition data into k folds\nk = 5:\n\nkf = KFold(n_splits=5, shuffle=True, random_state=102319)\nfolds = list(kf.split(candy_rankings))"
  },
  {
    "objectID": "slides/19-validation.html#fit-model-on-each-of-training-set",
    "href": "slides/19-validation.html#fit-model-on-each-of-training-set",
    "title": "Model validation",
    "section": "Fit model on each of training set",
    "text": "Fit model on each of training set\n\nrmses = []\nfor train_index, test_index in folds:\n    train_data = candy_rankings.iloc[train_index]\n    test_data = candy_rankings.iloc[test_index]\n    model = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=train_data).fit()\n    rmse = np.sqrt(mean_squared_error(test_data['winpercent'], model.predict(test_data)))\n    rmses.append(rmse)"
  },
  {
    "objectID": "slides/19-validation.html#calculate-rmses",
    "href": "slides/19-validation.html#calculate-rmses",
    "title": "Model validation",
    "section": "Calculate RMSEs",
    "text": "Calculate RMSEs\n\n\n\n\nCode\nfold_ids = list(range(1, 6))\nplt.figure(figsize=(8, 5))\nplt.plot(fold_ids, rmses, marker='o', label='Test RMSE')\nplt.xlabel('Fold')\nplt.ylabel('RMSE')\nplt.title('Test RMSE for each fold')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n \n\nrmse_summary = pd.Series(rmses).describe()\nprint(rmse_summary)\n\ncount     5.000000\nmean     10.644100\nstd       3.751177\nmin       5.336119\n25%       8.319019\n50%      11.885830\n75%      13.232239\nmax      14.447295\ndtype: float64\n\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/00-welcome-511.html#meet-the-prof",
    "href": "slides/00-welcome-511.html#meet-the-prof",
    "title": "Welcome to INFO 511",
    "section": "Meet the prof",
    "text": "Meet the prof\n\n\nDr. Greg Chism\nAssistant Professor of the Practice\nHarvill 420"
  },
  {
    "objectID": "slides/00-welcome-511.html#meet-each-other",
    "href": "slides/00-welcome-511.html#meet-each-other",
    "title": "Welcome to INFO 511",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\n\n\n\n\nImportant\n\n\nIntroduce yourself via the #general channel in the course Slack"
  },
  {
    "objectID": "slides/00-welcome-511.html#meet-data-science",
    "href": "slides/00-welcome-511.html#meet-data-science",
    "title": "Welcome to INFO 511",
    "section": "Meet data science",
    "text": "Meet data science\n\nData science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.\nWe’re going to learn to do this in a formulaic way – more on that later!\nThis is a course on introduction to data science, with an emphasis on statistical thinking and mathematics."
  },
  {
    "objectID": "slides/00-welcome-511.html#excel---not",
    "href": "slides/00-welcome-511.html#excel---not",
    "title": "Welcome to INFO 511",
    "section": "Excel - not…",
    "text": "Excel - not…"
  },
  {
    "objectID": "slides/00-welcome-511.html#python",
    "href": "slides/00-welcome-511.html#python",
    "title": "Welcome to INFO 511",
    "section": "Python",
    "text": "Python"
  },
  {
    "objectID": "slides/00-welcome-511.html#vs-code",
    "href": "slides/00-welcome-511.html#vs-code",
    "title": "Welcome to INFO 511",
    "section": "VS Code",
    "text": "VS Code"
  },
  {
    "objectID": "slides/00-welcome-511.html#data-science-life-cycle-1",
    "href": "slides/00-welcome-511.html#data-science-life-cycle-1",
    "title": "Welcome to INFO 511",
    "section": "Data science life cycle",
    "text": "Data science life cycle"
  },
  {
    "objectID": "slides/00-welcome-511.html#import",
    "href": "slides/00-welcome-511.html#import",
    "title": "Welcome to INFO 511",
    "section": "Import",
    "text": "Import"
  },
  {
    "objectID": "slides/00-welcome-511.html#tidy-transform",
    "href": "slides/00-welcome-511.html#tidy-transform",
    "title": "Welcome to INFO 511",
    "section": "Tidy + transform",
    "text": "Tidy + transform"
  },
  {
    "objectID": "slides/00-welcome-511.html#visualize",
    "href": "slides/00-welcome-511.html#visualize",
    "title": "Welcome to INFO 511",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/00-welcome-511.html#model",
    "href": "slides/00-welcome-511.html#model",
    "title": "Welcome to INFO 511",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "slides/00-welcome-511.html#understand",
    "href": "slides/00-welcome-511.html#understand",
    "title": "Welcome to INFO 511",
    "section": "Understand",
    "text": "Understand"
  },
  {
    "objectID": "slides/00-welcome-511.html#section",
    "href": "slides/00-welcome-511.html#section",
    "title": "Welcome to INFO 511",
    "section": "",
    "text": "date  season\n0   23 January 2017  winter\n1      4 March 2017  spring\n2      14 June 2017  summer\n3  1 September 2017    fall\n4               ...     ..."
  },
  {
    "objectID": "slides/00-welcome-511.html#communicate",
    "href": "slides/00-welcome-511.html#communicate",
    "title": "Welcome to INFO 511",
    "section": "Communicate",
    "text": "Communicate"
  },
  {
    "objectID": "slides/00-welcome-511.html#understand-communicate",
    "href": "slides/00-welcome-511.html#understand-communicate",
    "title": "Welcome to INFO 511",
    "section": "Understand + communicate",
    "text": "Understand + communicate"
  },
  {
    "objectID": "slides/00-welcome-511.html#program",
    "href": "slides/00-welcome-511.html#program",
    "title": "Welcome to INFO 511",
    "section": "Program",
    "text": "Program"
  },
  {
    "objectID": "slides/00-welcome-511.html#application-exercise",
    "href": "slides/00-welcome-511.html#application-exercise",
    "title": "Welcome to INFO 511",
    "section": "Application exercise",
    "text": "Application exercise\n\nOr more like demo for today…\n📋 github.com/INFO-511-F24/ae-00-unvotes"
  },
  {
    "objectID": "slides/00-welcome-511.html#homepage",
    "href": "slides/00-welcome-511.html#homepage",
    "title": "Welcome to INFO 511",
    "section": "Homepage",
    "text": "Homepage\nhttps://datasciaz.netlify.app/\n\nAll course materials\nLinks to GitHub, D2L, Posit Cloud (can run Jupyter), etc."
  },
  {
    "objectID": "slides/00-welcome-511.html#course-toolkit",
    "href": "slides/00-welcome-511.html#course-toolkit",
    "title": "Welcome to INFO 511",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nGitHub org: github.com/INFO-511-F24\nPosit Cloud: posit.cloud\nCommunication: Slack\nAssignment submission and feedback: Github"
  },
  {
    "objectID": "slides/00-welcome-511.html#activities",
    "href": "slides/00-welcome-511.html#activities",
    "title": "Welcome to INFO 511",
    "section": "Activities",
    "text": "Activities\n\nIntroduce new content and prepare for lectures by watching the videos and completing the readings\nActively participate office hours, team meetings\nPractice applying data science concepts and computing with application exercises during lecture, graded for completion\nPut together what you’ve learned to analyze real-world data\n\nLab assignments x 7\nExams x 2\nTerm project presented in the last lab session"
  },
  {
    "objectID": "slides/00-welcome-511.html#exams",
    "href": "slides/00-welcome-511.html#exams",
    "title": "Welcome to INFO 511",
    "section": "Exams",
    "text": "Exams\n\nTwo exams, each 20%\nTake home: Focus on the analysis of a dataset introduced in the take home exam, or solve mathematical prompts\n\n\n\n\n\n\n\nCaution\n\n\nExam dates cannot be changed and no make-up exams will be given. If you can’t take the exams on these dates, you should drop this class."
  },
  {
    "objectID": "slides/00-welcome-511.html#project",
    "href": "slides/00-welcome-511.html#project",
    "title": "Welcome to INFO 511",
    "section": "Project",
    "text": "Project\n\nDataset of your choice, method of your choice\nTeamwork\nPresentation and write-up\nPresentations in the last week (video recordings)\nInterim deadlines, peer review on content, peer evaluation for team contribution\n\n\n\n\n\n\n\nCaution\n\n\nFinal presentation date cannot be changed. If you can’t present on that date, you should drop this class. You must complete the project to pass this class."
  },
  {
    "objectID": "slides/00-welcome-511.html#teams",
    "href": "slides/00-welcome-511.html#teams",
    "title": "Welcome to INFO 511",
    "section": "Teams",
    "text": "Teams\n\nAssigned by me\nProject\nPeer evaluation during teamwork and after completion\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "slides/00-welcome-511.html#grading",
    "href": "slides/00-welcome-511.html#grading",
    "title": "Welcome to INFO 511",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nLabs\n30%\n\n\nProject\n25%\n\n\nExam 1\n20%\n\n\nExam 2\n20%\n\n\nApplication Exercises\n5%\n\n\n\nNo specific points allocated to participation, but participation via Slack, particularly Discussions, will be recorded periodically throughout the semester, and this information will be used as “extra credit” if you’re in between two grades and a minor bump would help.\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/00-welcome-511.html#support",
    "href": "slides/00-welcome-511.html#support",
    "title": "Welcome to INFO 511",
    "section": "Support",
    "text": "Support\n\nAttend office hours (by appointment)\nAsk and answer questions on the discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/00-welcome-511.html#announcements",
    "href": "slides/00-welcome-511.html#announcements",
    "title": "Welcome to INFO 511",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on D2L (Announcements tool) but primarily sent via Slack, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day"
  },
  {
    "objectID": "slides/00-welcome-511.html#diversity-inclusion",
    "href": "slides/00-welcome-511.html#diversity-inclusion",
    "title": "Welcome to INFO 511",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nMy goal is to ensure that students from all diverse backgrounds are well-served by this course, addressing their learning needs in and out of class, and recognizing the diversity they bring as a valuable resource and strength.\n\n\nPlease let me know your preferred name and pronouns on the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers, and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me."
  },
  {
    "objectID": "slides/00-welcome-511.html#accessibility",
    "href": "slides/00-welcome-511.html#accessibility",
    "title": "Welcome to INFO 511",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Disability Resource Center is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/00-welcome-511.html#late-work-waivers-regrades-policy",
    "href": "slides/00-welcome-511.html#late-work-waivers-regrades-policy",
    "title": "Welcome to INFO 511",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it"
  },
  {
    "objectID": "slides/00-welcome-511.html#academic-integrity",
    "href": "slides/00-welcome-511.html#academic-integrity",
    "title": "Welcome to INFO 511",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the UArizona InfoSci Community Standard:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/00-welcome-511.html#this-weeks-tasks",
    "href": "slides/00-welcome-511.html#this-weeks-tasks",
    "title": "Welcome to INFO 511",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nComplete Lab 0\n\nComputational setup\nGetting to know you survey\n\nRead the syllabus\nStart readings for next week\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/02-python-git.html#what-is-python",
    "href": "slides/02-python-git.html#what-is-python",
    "title": "Intro to Python",
    "section": "What is Python?",
    "text": "What is Python?\n\n\n“Python is the second best language at everything.” - Van Lindberg\nVersatile and popular programming language with simple syntax\nLarge collection of frameworks and libraries\nLarge, active community\nWidely used for web development, data analysis, artificial intelligence, scientific computing, and more."
  },
  {
    "objectID": "slides/02-python-git.html#basic-syntax-and-comments",
    "href": "slides/02-python-git.html#basic-syntax-and-comments",
    "title": "Intro to Python",
    "section": "Basic syntax and comments",
    "text": "Basic syntax and comments\n\n# This is a comment\nprint(\"Hello, Python!\")  # This prints a message\n\nHello, Python!\n\n\n\n\nIndentation for code blocks (instead of brackets)\nComments start with a # (used to explain code)"
  },
  {
    "objectID": "slides/02-python-git.html#variables-and-data-types",
    "href": "slides/02-python-git.html#variables-and-data-types",
    "title": "Intro to Python",
    "section": "Variables and data types",
    "text": "Variables and data types\n\n# Integer\nx = 5\n\n# Float\ny = 3.14\n\n# String\nname = \"Python\"\n\n# Boolean\nis_easy = True\n\n\n\nVariables store data values.\nPython uses integers (whole numbers), floats (non-whole numbers), strings (text), and booleans (true/false)."
  },
  {
    "objectID": "slides/02-python-git.html#factors-categorical-data",
    "href": "slides/02-python-git.html#factors-categorical-data",
    "title": "Intro to Python",
    "section": "Factors (categorical data)",
    "text": "Factors (categorical data)\n\n\n\nimport pandas as pd\n\nx = pd.Categorical([\"a\", \"b\", \"b\", \"a\"])\nprint(x)\nprint(type(x))\nprint(x.categories)\nprint(x.codes)\n\n['a', 'b', 'b', 'a']\nCategories (2, object): ['a', 'b']\n&lt;class 'pandas.core.arrays.categorical.Categorical'&gt;\nIndex(['a', 'b'], dtype='object')\n[0 1 1 0]"
  },
  {
    "objectID": "slides/02-python-git.html#other-classes",
    "href": "slides/02-python-git.html#other-classes",
    "title": "Intro to Python",
    "section": "Other classes",
    "text": "Other classes\n\n\nDate\n\nimport datetime\n\ntoday = datetime.date.today()\nprint(today)\nprint(type(today))\nprint(dir(today))\n\n2024-08-19\n&lt;class 'datetime.date'&gt;\n['__add__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rsub__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', 'ctime', 'day', 'fromisocalendar', 'fromisoformat', 'fromordinal', 'fromtimestamp', 'isocalendar', 'isoformat', 'isoweekday', 'max', 'min', 'month', 'replace', 'resolution', 'strftime', 'timetuple', 'today', 'toordinal', 'weekday', 'year']\n\n\n\nDate-Time\n\nnow = pd.Timestamp(\"2024-02-08 11:45:00\", tz=\"EST\")\nprint(now)\nprint(type(now))\nprint(now.tz)\n\n2024-02-08 11:45:00-05:00\n&lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;\nEST"
  },
  {
    "objectID": "slides/02-python-git.html#basic-operations",
    "href": "slides/02-python-git.html#basic-operations",
    "title": "Intro to Python",
    "section": "Basic operations",
    "text": "Basic operations\n\n# Arithmetic Operations\na = 10\nb = 3\nsum = a + b\ndifference = a - b\nproduct = a * b\nquotient = a / b\n\n# Logical Operations\nis_greater = a &gt; b\nis_equal = (a == b)\n\n\n\nPython supports various arithmetic and logical operations."
  },
  {
    "objectID": "slides/02-python-git.html#control-structures-if-else",
    "href": "slides/02-python-git.html#control-structures-if-else",
    "title": "Intro to Python",
    "section": "Control structures: if-else",
    "text": "Control structures: if-else\n\nage = 20\nif age &gt;= 18:\n    print(\"Adult\")\nelse:\n    print(\"Minor\")\n\nAdult\n\n\n\n\nPython uses if, elif, and else for decision-making."
  },
  {
    "objectID": "slides/02-python-git.html#loops",
    "href": "slides/02-python-git.html#loops",
    "title": "Intro to Python",
    "section": "Loops",
    "text": "Loops\n\n# For Loop\nfor i in range(5):\n    print(i)\n\n# While Loop\nj = 0\nwhile j &lt; 5:\n    print(j)\n    j += 1\n\n\n\nPython has two types of loops: for and while."
  },
  {
    "objectID": "slides/02-python-git.html#lists",
    "href": "slides/02-python-git.html#lists",
    "title": "Intro to Python",
    "section": "Lists",
    "text": "Lists\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits[0])  # Accessing the first item\n\n\n\nLists store multiple items in a single variable.\nAccess elements using index (starting at 0)."
  },
  {
    "objectID": "slides/02-python-git.html#functions",
    "href": "slides/02-python-git.html#functions",
    "title": "Intro to Python",
    "section": "Functions",
    "text": "Functions\n\ndef greet(name):\n    return \"Hello \" + name\n\nprint(greet(\"Alice\"))\n\n\n\nFunctions perform specific tasks.\nCall a function with its name and arguments."
  },
  {
    "objectID": "slides/02-python-git.html#conclusion",
    "href": "slides/02-python-git.html#conclusion",
    "title": "Intro to Python",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nPython is a versatile and user-friendly language.\nIdeal for beginners and widely used.\nEncourages readable and maintainable code.\nExtensive libraries and community support."
  },
  {
    "objectID": "slides/02-python-git.html#git-and-github",
    "href": "slides/02-python-git.html#git-and-github",
    "title": "Intro to Python",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\n\n\n\n\n\n\n\nGit is a version control system – like “Track Changes” features from Microsoft Word, on steroids\nIt’s not the only version control system, but it’s a very popular one\n\n\n\n\n\n\n\n\nGitHub is the home for your Git-based projects on the internet – like DropBox but much, much better\nWe will use GitHub as a platform for web hosting and collaboration (and as our course management system!)"
  },
  {
    "objectID": "slides/02-python-git.html#versioning---done-badly",
    "href": "slides/02-python-git.html#versioning---done-badly",
    "title": "Intro to Python",
    "section": "Versioning - done badly",
    "text": "Versioning - done badly"
  },
  {
    "objectID": "slides/02-python-git.html#versioning---done-better",
    "href": "slides/02-python-git.html#versioning---done-better",
    "title": "Intro to Python",
    "section": "Versioning - done better",
    "text": "Versioning - done better"
  },
  {
    "objectID": "slides/02-python-git.html#versioning---done-even-better",
    "href": "slides/02-python-git.html#versioning---done-even-better",
    "title": "Intro to Python",
    "section": "Versioning - done even better",
    "text": "Versioning - done even better\nwith human readable messages"
  },
  {
    "objectID": "slides/02-python-git.html#how-will-we-use-git-and-github",
    "href": "slides/02-python-git.html#how-will-we-use-git-and-github",
    "title": "Intro to Python",
    "section": "How will we use Git and GitHub?",
    "text": "How will we use Git and GitHub?"
  },
  {
    "objectID": "slides/02-python-git.html#how-will-we-use-git-and-github-1",
    "href": "slides/02-python-git.html#how-will-we-use-git-and-github-1",
    "title": "Intro to Python",
    "section": "How will we use Git and GitHub?",
    "text": "How will we use Git and GitHub?"
  },
  {
    "objectID": "slides/02-python-git.html#how-will-we-use-git-and-github-2",
    "href": "slides/02-python-git.html#how-will-we-use-git-and-github-2",
    "title": "Intro to Python",
    "section": "How will we use Git and GitHub?",
    "text": "How will we use Git and GitHub?"
  },
  {
    "objectID": "slides/02-python-git.html#how-will-we-use-git-and-github-3",
    "href": "slides/02-python-git.html#how-will-we-use-git-and-github-3",
    "title": "Intro to Python",
    "section": "How will we use Git and GitHub?",
    "text": "How will we use Git and GitHub?"
  },
  {
    "objectID": "slides/02-python-git.html#git-and-github-tips",
    "href": "slides/02-python-git.html#git-and-github-tips",
    "title": "Intro to Python",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\nWe will be doing Git things and interfacing with GitHub through VS Code, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\nThere is a great resource for working with git and Python: git-github-python. Some of the content in there is beyond the scope of this course, but it’s a good place to look for help."
  },
  {
    "objectID": "slides/02-python-git.html#tour-git-github",
    "href": "slides/02-python-git.html#tour-git-github",
    "title": "Intro to Python",
    "section": "Tour: Git + GitHub",
    "text": "Tour: Git + GitHub\n\nJust one option for now:\nSit back and enjoy the show!\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/20-calculus-I.html#calculus-in-data-science",
    "href": "slides/20-calculus-I.html#calculus-in-data-science",
    "title": "Calculus I",
    "section": "Calculus in data science",
    "text": "Calculus in data science\n\n\nOptimization Algorithms: Calculus is essential for understanding and implementing optimization algorithms like gradient descent, which are used to minimize error functions in machine learning models.\nModeling Change: Derivatives help in modeling and understanding the rate of change in various phenomena, which is crucial for predictive analytics and dynamic systems in data science.\nIntegral Applications: Integrals are used in calculating areas under curves, which is fundamental for probability distributions, statistical inference, and understanding cumulative effects in data analysis."
  },
  {
    "objectID": "slides/20-calculus-I.html#functions-and-their-graphs",
    "href": "slides/20-calculus-I.html#functions-and-their-graphs",
    "title": "Calculus I",
    "section": "Functions and their graphs",
    "text": "Functions and their graphs\n\nDefinitionExamplesPlottingCode\n\n\n\n\nA function is a relation between a set of inputs and a set of permissible outputs, where each input is related to exactly one output.\nMathematical notation: \\(f(x)\\) denotes a function named \\(f\\) with \\(x\\) as the input variable.\n\n\n\n\n\nLinear function: \\(f(x)=2x+3\\)\nQuadratic function: \\(f(x)=x^2-4x+4\\)\nExponential function: \\(f(x)=e^x\\)\nLogarithmic function: \\(f(x)=log(x)\\)\n\n\n\n\n\nUsing matplotlib\n\n\n\n\n\n\n\n\n\n\nUsing SymPy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing matplotlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the function\ndef f(x):\n    return 2 * x + 3\n\n# Generate x values\nx = np.linspace(-10, 10, 400)\ny = f(x)\n\n# Plot the function\nplt.plot(x, y, label='f(x) = 2x + 3')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Graph of the Linear Function')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nUsing SymPy\n\nfrom sympy import symbols, plot\n\n# Define the variable and function\nx = symbols('x')\nf = 2 * x + 3\n\n# Plot the function\nplot(f)"
  },
  {
    "objectID": "slides/20-calculus-I.html#importance-of-functions-in-modeling",
    "href": "slides/20-calculus-I.html#importance-of-functions-in-modeling",
    "title": "Calculus I",
    "section": "Importance of functions in modeling",
    "text": "Importance of functions in modeling\n\n\nPredictive Modeling:\n\nFunctions predict outputs from inputs, essential for machine learning.\nExample: Linear regression predicts continuous outcomes.\n\nDescriptive Analysis:\n\nFunctions describe relationships, revealing patterns and trends.\nExample: Growth functions model population or business growth.\n\nDecision Making:\n\nFunctions formulate decision rules and optimization problems.\nExample: Cost functions minimize expenses or maximize profits."
  },
  {
    "objectID": "slides/20-calculus-I.html#overview-of-calculus",
    "href": "slides/20-calculus-I.html#overview-of-calculus",
    "title": "Calculus I",
    "section": "Overview of Calculus",
    "text": "Overview of Calculus\n\nBranch of mathematics that studies continuous change.\n\n\n\nDifferential (rates of change & slopes of curves)\n\n\n\n\n\n\nIntegral (accumulation of quantities & areas under curves)"
  },
  {
    "objectID": "slides/20-calculus-I.html#differentiation-and-integration",
    "href": "slides/20-calculus-I.html#differentiation-and-integration",
    "title": "Calculus I",
    "section": "Differentiation and Integration",
    "text": "Differentiation and Integration\n\nDifferentiationIntegration\n\n\n\n\nMeasures the rate at which a quantity changes.\nExample: In machine learning, the derivative of the loss function with respect to model parameters helps in finding the optimal parameters.\nSymbol: \\(\\frac{dy}{dx}\\) of \\(f^{'}(x)\\)\nPractical Application: Gradient Descent Algorithm\n\n\n\n\n\n\nMeasures the accumulation of quantities and the area under a curve.\nExample: Used to compute the area under probability distribution functions, which is essential in statistics and data analysis.\nSymbol: \\(\\int f(x) dx\\)\nPractical Application: Calculating Cumulative Distribution Functions (CDFs)"
  },
  {
    "objectID": "slides/20-calculus-I.html#calculating-the-slope",
    "href": "slides/20-calculus-I.html#calculating-the-slope",
    "title": "Calculus I",
    "section": "Calculating the slope",
    "text": "Calculating the slope\n\n\n\n\n\n\n\n\n\n\\(\\text{slope}=\\frac{\\text{rise}}{\\text{run}}\\)\n\n\n\\(\\text{slope}=\\frac{\\text{change in distance}(\\Delta x)}{\\text{change in time}(\\Delta t)}\\)\n\n\n\\(\\text{slope}=\\frac{x(15)-x(10)}{t(15)-t(10)}\\)\n\n\n\\(\\text{slope}=\\frac{202m - 122m}{15s-10s}\\)\n\n\n\\(\\text{slope}=\\frac{80m}{5s}=16m/s\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-derivative",
    "href": "slides/20-calculus-I.html#the-derivative",
    "title": "Calculus I",
    "section": "The derivative",
    "text": "The derivative"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-derivative-1",
    "href": "slides/20-calculus-I.html#the-derivative-1",
    "title": "Calculus I",
    "section": "The derivative",
    "text": "The derivative"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-derivative-2",
    "href": "slides/20-calculus-I.html#the-derivative-2",
    "title": "Calculus I",
    "section": "The derivative",
    "text": "The derivative"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-derivative-3",
    "href": "slides/20-calculus-I.html#the-derivative-3",
    "title": "Calculus I",
    "section": "The derivative",
    "text": "The derivative"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-derivative-4",
    "href": "slides/20-calculus-I.html#the-derivative-4",
    "title": "Calculus I",
    "section": "The derivative",
    "text": "The derivative"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-derivative-5",
    "href": "slides/20-calculus-I.html#the-derivative-5",
    "title": "Calculus I",
    "section": "The derivative",
    "text": "The derivative"
  },
  {
    "objectID": "slides/20-calculus-I.html#derivatives-in-python",
    "href": "slides/20-calculus-I.html#derivatives-in-python",
    "title": "Calculus I",
    "section": "Derivatives in Python",
    "text": "Derivatives in Python\nCalculating derivatives using SymPy\n\nfrom sympy import symbols, diff\n\nx = symbols('x')\nf = x**2 # x^2\ndf = diff(f)\nprint(df)\n\n2*x"
  },
  {
    "objectID": "slides/20-calculus-I.html#solving-derivatives",
    "href": "slides/20-calculus-I.html#solving-derivatives",
    "title": "Calculus I",
    "section": "Solving derivatives",
    "text": "Solving derivatives\nDifferentiation rules\n\n\nConstant rule: \\(\\frac{d}{dx} (c) = 0\\)\nPower rule: \\(\\frac{d}{dx} (x^n) = nx^{n-1}\\)\nConstant multiple rule: \\(\\frac{d}{dx} [c \\cdot f(x)] = c \\cdot f'(x)\\)\nSum rule: \\(\\frac{d}{dx} [f(x) + g(x)] = f'(x) + g'(x)\\)\nDifference rule: \\(\\frac{d}{dx} [f(x) - g(x)] = f'(x) - g'(x)\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#example-1-differentiating-a-constant",
    "href": "slides/20-calculus-I.html#example-1-differentiating-a-constant",
    "title": "Calculus I",
    "section": "Example 1: Differentiating a Constant",
    "text": "Example 1: Differentiating a Constant\n\n\nFunction: \\(f(x) = 7\\)\nDerivative: \\(f'(x) = 0\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#example-2-power-rule",
    "href": "slides/20-calculus-I.html#example-2-power-rule",
    "title": "Calculus I",
    "section": "Example 2: Power rule",
    "text": "Example 2: Power rule\n\n\nFunction: \\(f(x) = x^3\\)\nDerivative: \\(f'(x) = \\frac{d}{dx} (x^3) = 3x^2\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#example-3-constant-multiple-rule",
    "href": "slides/20-calculus-I.html#example-3-constant-multiple-rule",
    "title": "Calculus I",
    "section": "Example 3: Constant multiple rule",
    "text": "Example 3: Constant multiple rule\n\n\nFunction: \\(f(x) = 5x^2\\)\nDerivative: \\(f'(x) = 5 \\cdot \\frac{d}{dx} (x^2) = 5 \\cdot 2x = 10x\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#example-4-sum-and-difference-rule",
    "href": "slides/20-calculus-I.html#example-4-sum-and-difference-rule",
    "title": "Calculus I",
    "section": "Example 4: Sum and difference rule",
    "text": "Example 4: Sum and difference rule\n\n\nFunction: \\(f(x) = x^3 + 4x - 5\\)\nDerivative: \\(f'(x) = \\frac{d}{dx} (x^3) + \\frac{d}{dx} (4x) - \\frac{d}{dx} (5) = 3x^2 + 4 - 0 = 3x^2 + 4\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#solving-complex-derivatives",
    "href": "slides/20-calculus-I.html#solving-complex-derivatives",
    "title": "Calculus I",
    "section": "Solving complex derivatives",
    "text": "Solving complex derivatives\nComplex Derivatives:\n\n\nInvolves functions composed of multiple less complex functions.\nRequires application of rules like the chain rule and product rule for differentiation.\n\n\n\nExample Function: \\[\nh(x)=(\\ln(x) \\cdot e^{ax})^k\n\\]\n\n\n\nObjective: Find the derivative \\(\\frac{d}{dx}h(x)\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule",
    "href": "slides/20-calculus-I.html#the-chain-rule",
    "title": "Calculus I",
    "section": "The Chain Rule",
    "text": "The Chain Rule\n\\[\n(f(g(x)))^{'}=f{'}(g(x)) \\cdot g{'}(x)\n\\]\n\nUsed when differentiating a composition of functions"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-composition",
    "href": "slides/20-calculus-I.html#the-chain-rule-composition",
    "title": "Calculus I",
    "section": "The Chain Rule: Composition",
    "text": "The Chain Rule: Composition\nFunction: \\(f(x) = (3x^{2} + 2)^{5}\\)\n\n\nIdentify the Outer and Inner Functions\n\n\n\nOuter function: \\(u^5\\)\nInner function: \\(u = 3x^2 + 2\\)\n\n\n\n\n\nApply the Chain Rule\n\n\\(f{'}(x) = 5(3x^{2}+2)^{4} \\cdot \\frac{d}{dx}(3x^2 + 2)\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-composition-1",
    "href": "slides/20-calculus-I.html#the-chain-rule-composition-1",
    "title": "Calculus I",
    "section": "The Chain Rule: Composition",
    "text": "The Chain Rule: Composition\nFunction: \\(f(x) = (3x^{2} + 2)^{5}\\)\n\n\nDifferentiate the Inner Function\n\n\\(\\frac{d}{dx}(3x^2 + 2) = 6x\\)\n\n\n\nCombine the results\n\n\\(f{'}(x)=5(3x^2 + 2)^{4} \\cdot 6x\\)\n\\(f{'}(x)=30x(3x^2 + 2)^{4}\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-nested-composition",
    "href": "slides/20-calculus-I.html#the-chain-rule-nested-composition",
    "title": "Calculus I",
    "section": "The Chain Rule: Nested composition",
    "text": "The Chain Rule: Nested composition\nFunction: \\(g(x) = \\sin(x^3 + 4x)\\)\n\n\nIdentify the Outer and Inner Functions\n\n\n\nOuter function: \\(\\sin(u)\\)\nInner function: \\(u=x^3+4x\\)\n\n\n\n\n\nApply the Chain Rule\n\n\\(g'(x) = \\cos(x^3 + 4x) \\cdot \\frac{d}{dx}(x^3 + 4x)\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-nested-composition-1",
    "href": "slides/20-calculus-I.html#the-chain-rule-nested-composition-1",
    "title": "Calculus I",
    "section": "The Chain Rule: Nested composition",
    "text": "The Chain Rule: Nested composition\nFunction: \\(g(x) = \\sin(x^3 + 4x)\\)\n\n\nDifferentiate the Inner Function\n\n\\(\\frac{d}{dx}(x^3 + 4x) = 3x^2 + 4\\)\n\n\n\nCombine the Results\n\n\\(g'(x) = \\cos(x^3 + 4x) \\cdot (3x^2 + 4)\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition",
    "href": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition",
    "title": "Calculus I",
    "section": "The Chain Rule: Complex nested composition",
    "text": "The Chain Rule: Complex nested composition\nFunction: \\(h(x) = \\left( e^{x^2} \\cdot \\ln(x) \\right)^2\\)\n\n\nIdentify the Outer and Inner Functions\n\n\n\nOuter function: \\(u^2\\)\nInner function: \\(u=e^{x^{2}} \\cdot \\ln(x)\\)\n\n\n\n\n\nApply the Chain Rule\n\n\\(h'(x) = 2\\left( e^{x^2} \\cdot \\ln(x) \\right) \\cdot \\frac{d}{dx}(e^{x^2} \\cdot \\ln(x))\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition-1",
    "href": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition-1",
    "title": "Calculus I",
    "section": "The Chain Rule: Complex nested composition",
    "text": "The Chain Rule: Complex nested composition\nFunction: \\(h(x) = \\left( e^{x^2} \\cdot \\ln(x) \\right)^2\\)\n\n\nDifferentiate the Inner Function using the Product Rule\n\n\n\nInner function: \\(u=e^{x^{2}} \\cdot \\ln(x)\\)\nProduct rule: \\((u \\cdot v)' = u' \\cdot v + u \\cdot v'\\)\nLet \\(u = e^{x^2}\\) and \\(\\quad v = \\ln(x)\\)\n\\(u' = \\frac{d}{dx}(e^{x^2}) = 2xe^{x^2}\\)\n\\(v' = \\frac{d}{dx}(\\ln(x)) = \\frac{1}{x}\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition-2",
    "href": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition-2",
    "title": "Calculus I",
    "section": "The Chain Rule: Complex nested composition",
    "text": "The Chain Rule: Complex nested composition\nFunction: \\(h(x) = \\left( e^{x^2} \\cdot \\ln(x) \\right)^2\\)\n\n\nCombine the Produce Rule Results\n\n\n\n\\(\\frac{d}{dx}(e^{x^2} \\cdot \\ln(x)) = (2xe^{x^2}) \\cdot \\ln(x) + e^{x^2} \\cdot \\frac{1}{x}\\)\n\\(= 2xe^{x^2} \\ln(x) + \\frac{e^{x^2}}{x}\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition-3",
    "href": "slides/20-calculus-I.html#the-chain-rule-complex-nested-composition-3",
    "title": "Calculus I",
    "section": "The Chain Rule: Complex nested composition",
    "text": "The Chain Rule: Complex nested composition\nFunction: \\(h(x) = \\left( e^{x^2} \\cdot \\ln(x) \\right)^2\\)\n\n\nCombine with the Outer Function Derivative\n\n\n\n\\(h'(x) = 2\\left( e^{x^2} \\cdot \\ln(x) \\right) \\cdot \\left( 2xe^{x^2} \\ln(x) + \\frac{e^{x^2}}{x} \\right)\\)\nSimplify:\n\\(h'(x) = 2e^{x^2} \\ln(x) \\left( 2xe^{x^2} \\ln(x) + \\frac{e^{x^2}}{x} \\right)\\)\n\\(h'(x) = 2e^{x^2} \\ln(x) \\left( 2xe^{x^2} \\ln(x) + e^{x^2} \\cdot x^{-1} \\right)\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#partial-derivatives-1",
    "href": "slides/20-calculus-I.html#partial-derivatives-1",
    "title": "Calculus I",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nDefinition:\n\n\nA partial derivative represents the rate of change of a function with respect to one variable while keeping other variables constant.\nNotation: \\(\\frac{\\partial f}{\\partial x}\\) denotes the partial derivative of \\(f\\) with respect to \\(x\\)."
  },
  {
    "objectID": "slides/20-calculus-I.html#partial-derivatives-2",
    "href": "slides/20-calculus-I.html#partial-derivatives-2",
    "title": "Calculus I",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nSignificance:\n\n\nEssential in understanding functions of multiple variables.\nCrucial for optimization in multivariable calculus.\nUsed in various fields such as physics, engineering, and economics to model complex systems."
  },
  {
    "objectID": "slides/20-calculus-I.html#application-in-multi-variable-functions",
    "href": "slides/20-calculus-I.html#application-in-multi-variable-functions",
    "title": "Calculus I",
    "section": "Application in multi-variable functions",
    "text": "Application in multi-variable functions\n\nMulti-variable functions:\n\n\nFunctions that depend on two or more variables, e.g., \\(f(x,y)=x^2+y^2\\)\n\n\n\n\nGradient:\n\n\nThe vector of all partial derivatives in a function.\nIndicates the direction of the steepest ascent\nNotation: \\(\\nabla f=(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})\\)"
  },
  {
    "objectID": "slides/20-calculus-I.html#partial-derivatives-in-python",
    "href": "slides/20-calculus-I.html#partial-derivatives-in-python",
    "title": "Calculus I",
    "section": "Partial derivatives in Python",
    "text": "Partial derivatives in Python\nGiven the function \\(f(x,y)=x^3+3xy+y^3\\), calculate the partial derivatives with respect to \\(x\\) and \\(y\\):\n\nfrom sympy import symbols, diff\n\n# Define the variables and function\nx, y = symbols('x y')\nf = x**3 + 3*x*y + y**3\n\n# Calculate partial derivatives\npartial_x = diff(f, x)\npartial_y = diff(f, y)\n\nprint(partial_x)  # Output: 3*x**2 + 3*y\nprint(partial_y)  # Output: 3*x + 3*y**2\n\n3*x**2 + 3*y\n3*x + 3*y**2"
  },
  {
    "objectID": "slides/20-calculus-I.html#gradient-descent",
    "href": "slides/20-calculus-I.html#gradient-descent",
    "title": "Calculus I",
    "section": "Gradient descent",
    "text": "Gradient descent\nYou’ll learn more about this in INFO 521: Introduction to Machine Learning"
  },
  {
    "objectID": "slides/20-calculus-I.html#ae-13-derivation",
    "href": "slides/20-calculus-I.html#ae-13-derivation",
    "title": "Calculus I",
    "section": "ae-13-derivation",
    "text": "ae-13-derivation\nDerivations (you will be tested on this in Exam 2)\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/12-conditional-probability.html#conditional-probability",
    "href": "slides/12-conditional-probability.html#conditional-probability",
    "title": "Conditional Probability",
    "section": "Conditional probability",
    "text": "Conditional probability\nThe probability an event will occur given that another event has already occurred is a conditional probability. The conditional probability of event \\(A\\) given event \\(B\\) is:\n\n\\[P(A | B) = \\frac{P(A \\text{ and } B)}{P(B)}\\]"
  },
  {
    "objectID": "slides/12-conditional-probability.html#conditional-probabilities",
    "href": "slides/12-conditional-probability.html#conditional-probabilities",
    "title": "Conditional Probability",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\n\\[P(A | B) = \\frac{P(A \\text{ and } B)}{P(B)}\\]\n\nExamples come up all the time in the real world:\n\n\nGiven that it rained yesterday, what is the probability that it will rain today?\nGiven that a mammogram comes back positive, what is the probability that a woman has breast cancer?\nGiven that I’ve already watched six episodes of How I Met Your Mother tonight, what is the probability that I’ll get any work done this evening?"
  },
  {
    "objectID": "slides/12-conditional-probability.html#coffee-and-mortality",
    "href": "slides/12-conditional-probability.html#coffee-and-mortality",
    "title": "Conditional Probability",
    "section": "Coffee and mortality",
    "text": "Coffee and mortality\n\n\n\n\n\n\n\n\n\n\n\nDid not die\n\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601"
  },
  {
    "objectID": "slides/12-conditional-probability.html#three-probabilities",
    "href": "slides/12-conditional-probability.html#three-probabilities",
    "title": "Conditional Probability",
    "section": "Three probabilities",
    "text": "Three probabilities\n\n\n\n\n\n\n\n\n\n\nDid not die\n\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601\n\n\n\n\nDefine events \\(A\\) = died and \\(B\\) = non-coffee drinker. Calculate the following for a randomly selected person in the cohort:\n\n\nMarginal probability: \\(P(A)\\), \\(P(B)\\)\nJoint probability: \\(P(A \\text{ and } B)\\)\nConditional probability: \\(P(A | B)\\), \\(P(B | A)\\)"
  },
  {
    "objectID": "slides/12-conditional-probability.html#the-multiplicative-rule",
    "href": "slides/12-conditional-probability.html#the-multiplicative-rule",
    "title": "Conditional Probability",
    "section": "The multiplicative rule",
    "text": "The multiplicative rule\nWe can write the definition of condition probability\n\n\\[P(A | B) = \\frac{P(A \\text{ and } B)}{P(B)}\\]\n\n\n\n\nUsing the equation above, we get…\n\\[P(B) \\times P(A | B) = P(A \\text{ and } B)\\]\n\nWhat does the multiplicative rule mean in plain English?"
  },
  {
    "objectID": "slides/12-conditional-probability.html#defining-independence",
    "href": "slides/12-conditional-probability.html#defining-independence",
    "title": "Conditional Probability",
    "section": "Defining independence",
    "text": "Defining independence\nEvents \\(A\\) and \\(B\\) are said to be independent when\n\\[P(A | B) = P(A) \\hspace{10mm} \\textbf{OR} \\hspace{10mm}\nP(B | A) = P(B)\\]\n\n\nIn other words, knowing that one event has occurred doesn’t cause us to “adjust” the probability we assign to another event."
  },
  {
    "objectID": "slides/12-conditional-probability.html#checking-independence",
    "href": "slides/12-conditional-probability.html#checking-independence",
    "title": "Conditional Probability",
    "section": "Checking independence",
    "text": "Checking independence\nWe can use the multiplicative rule to see if two events are independent.\n\nIf events \\(A\\) and \\(B\\) are independent, then\n\\[P(A \\text{ and } B) = P(A) \\times P(B)\\]"
  },
  {
    "objectID": "slides/12-conditional-probability.html#independent-vs.-disjoint-events",
    "href": "slides/12-conditional-probability.html#independent-vs.-disjoint-events",
    "title": "Conditional Probability",
    "section": "Independent vs. disjoint events",
    "text": "Independent vs. disjoint events\nSince for two independent events \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\), knowing that one event has occurred tells us nothing more about the probability of the other occurring.\n\nFor two disjoint events \\(A\\) and \\(B\\), knowing that one has occurred tells us that the other definitely has not occurred: \\(P(A \\text{ and } B) = 0\\).\n\n\n\nDisjoint events are not independent!"
  },
  {
    "objectID": "slides/12-conditional-probability.html#checking-independence-1",
    "href": "slides/12-conditional-probability.html#checking-independence-1",
    "title": "Conditional Probability",
    "section": "Checking independence",
    "text": "Checking independence\n\n\n\n\n\n\n\n\n\n\nDid not die\n\nDied\n\n\n\n\nDoes not drink coffee\n5438\n1039\n\n\nDrinks coffee occasionally\n29712\n4440\n\n\nDrinks coffee regularly\n24934\n3601\n\n\n\n\n\n\nAre dying and abstaining from coffee independent events? How might we check?"
  },
  {
    "objectID": "slides/12-conditional-probability.html#an-example",
    "href": "slides/12-conditional-probability.html#an-example",
    "title": "Conditional Probability",
    "section": "An example",
    "text": "An example\nIn an introductory statistics course, 50% of students were first years, 30% were sophomores, and 20% were upperclassmen.\n\n80% of the first years didn’t get enough sleep, 40% of the sophomores didn’t get enough sleep, and 10% of the upperclassmen didn’t get enough sleep.\n\n\n\nWhat is the probability that a randomly selected student in this class didn’t get enough sleep?"
  },
  {
    "objectID": "slides/12-conditional-probability.html#bayes-rule-1",
    "href": "slides/12-conditional-probability.html#bayes-rule-1",
    "title": "Conditional Probability",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nAs we saw before, the two conditional probabilities \\(P(A | B)\\) and \\(P(B | A)\\) are not the same. But are they related in some way?\n\nYes they are (!) using Bayes’ rule:\n\nBayes’ rule:\n\\[\\begin{align}P(A | B) &= \\frac{P(A \\text{ and } B)}{P(B)}\\\\[10pt]\n&= \\frac{P(B | A)P(A)}{P(B)}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/12-conditional-probability.html#bayes-rule-continued",
    "href": "slides/12-conditional-probability.html#bayes-rule-continued",
    "title": "Conditional Probability",
    "section": "Bayes’ Rule (continued)",
    "text": "Bayes’ Rule (continued)\nPutting together a few rules of probability…\n\\[\\begin{align}P(A | B) &= \\frac{P(A \\text{ and } B)}{P(B)}\\\\[10pt]\n&= \\frac{P(B | A)P(A)}{P(B)}\\\\[15pt]\n&= \\frac{P(B | A)P(A)}{P(B | A)P(A) + P(B | A^c)P(A^c)}\\end{align}\\]\nLet’s took at an example to see how this works."
  },
  {
    "objectID": "slides/12-conditional-probability.html#definitions",
    "href": "slides/12-conditional-probability.html#definitions",
    "title": "Conditional Probability",
    "section": "Definitions",
    "text": "Definitions\nSuppose we’re interested in the performance of a diagnostic test. Let \\(D\\) be the event that a patient has the disease, and let \\(T\\) be the event that the test is positive for that disease.\n\n\nPrevalence: \\(P(D)\\)\nSensitivity: \\(P(T | D)\\)\nSpecificity: \\(P(T^c | D^c)\\)\nPositive predictive value: \\(P(D | T)\\)\nNegative predictive value: \\(P(D^c | T^c)\\)\n\n\n\n\nWhat do these probabilities mean in plain English?"
  },
  {
    "objectID": "slides/12-conditional-probability.html#rapid-self-administered-covid-19-tests",
    "href": "slides/12-conditional-probability.html#rapid-self-administered-covid-19-tests",
    "title": "Conditional Probability",
    "section": "Rapid self-administered COVID-19 tests",
    "text": "Rapid self-administered COVID-19 tests\n\n\nFor a Abbott BinaxNOW COVID-19 Rapid antigen tests,\n\n\nSensitivity, \\(P(T | D)\\), is 64.2% in symptomatic individuals\nSpecificity, \\(P(T^c | D^c)\\), is 99.8%\nFrom CDC statistics in 2021, with 8.7% prevalence from Pima County, Arizona persons aged ≥10 years.\n\n\n\n\n\n\n\n\n\n\n\nSuppose a randomly selected American aged 13+ has a positive test result. What is the probability they have COVID-19?"
  },
  {
    "objectID": "slides/12-conditional-probability.html#using-bayes-rule",
    "href": "slides/12-conditional-probability.html#using-bayes-rule",
    "title": "Conditional Probability",
    "section": "Using Bayes’ Rule",
    "text": "Using Bayes’ Rule\n\\[\\begin{align*}\nP(D | T) &= \\frac{P(D \\text{ and } T)}{P(T)}\\\\\n&= \\frac{P(T | D)P(D)}{P(T)}\\\\[5pt]\n&= \\frac{P(T | D)P(D)}{P(T | D)P(D) + P(T | D^c)P(D^c)}\\\\[5pt]\n&= \\frac{P(T | D)P(D)}{P(T | D)P(D) + (1 - P(T^c | D^c))(1 - P(D))}\n\\end{align*}\\]\n\n\n\nWhat does all of this mean? Let’s take a look!"
  },
  {
    "objectID": "slides/12-conditional-probability.html#ae-08",
    "href": "slides/12-conditional-probability.html#ae-08",
    "title": "Conditional Probability",
    "section": "ae-08",
    "text": "ae-08\nGiven:\n\n\nPrevalence: \\(P(D)\\) = 8.7% = 0.087\nSensitivity: \\(P(T | D)\\) = 64.2% = 0.642\nSpecificity: \\(P(T^c | D^c)\\) = 99.8% = 0.998\n\n\n\nWork through ae-08 then move on to the discussion questions"
  },
  {
    "objectID": "slides/12-conditional-probability.html#a-discussion",
    "href": "slides/12-conditional-probability.html#a-discussion",
    "title": "Conditional Probability",
    "section": "A discussion",
    "text": "A discussion\nThink about the following questions:\n\n\nIs this calculation surprising?\nWhat is the explanation?\nWas this calculation actually reasonable to perform?\nWhat if we tested in a different population, such as high-risk individuals?\nWhat if we were to test a random individual in a county where the prevalence of COVID-19 is approximately 25%?\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/17-logistic-regression.html#setup",
    "href": "slides/17-logistic-regression.html#setup",
    "title": "Logistic regression",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set_theme(style=\"whitegrid\", rc={\"figure.figsize\": (10, 6), \"axes.labelsize\": 16, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})"
  },
  {
    "objectID": "slides/17-logistic-regression.html#recap-modeling-loans",
    "href": "slides/17-logistic-regression.html#recap-modeling-loans",
    "title": "Logistic regression",
    "section": "Recap: Modeling Loans",
    "text": "Recap: Modeling Loans\n\nWhat is the practical difference between a model with parallel and non-parallel lines?\nWhat is the definition of R-squared?\nWhy do we choose models based on adjusted R-squared and not R-squared?"
  },
  {
    "objectID": "slides/17-logistic-regression.html#predict-interest-rate",
    "href": "slides/17-logistic-regression.html#predict-interest-rate",
    "title": "Logistic regression",
    "section": "Predict interest rate…",
    "text": "Predict interest rate…\nfrom credit utilization and homeownership\n\nX = loans[['credit_util', 'homeownership']]\nX = pd.get_dummies(X, drop_first=True).astype(float)\ny = loans['interest_rate']\n\nX = sm.add_constant(X)  \nmodel = sm.OLS(y, X).fit()\n\n\nprint(model.summary2())\n\n                   Results: Ordinary least squares\n=====================================================================\nModel:                OLS              Adj. R-squared:     0.068     \nDependent Variable:   interest_rate    AIC:                59859.3779\nDate:                 2024-08-19 13:43 BIC:                59888.2185\nNo. Observations:     9998             Log-Likelihood:     -29926.   \nDf Model:             3                F-statistic:        243.7     \nDf Residuals:         9994             Prob (F-statistic): 1.25e-152 \nR-squared:            0.068            Scale:              23.309    \n---------------------------------------------------------------------\n                       Coef.  Std.Err.    t    P&gt;|t|   [0.025  0.975]\n---------------------------------------------------------------------\nconst                  9.9250   0.1401 70.8498 0.0000  9.6504 10.1996\ncredit_util            5.3356   0.2074 25.7266 0.0000  4.9291  5.7421\nhomeownership_Mortgage 0.6956   0.1208  5.7590 0.0000  0.4588  0.9323\nhomeownership_Own      0.1283   0.1552  0.8266 0.4085 -0.1760  0.4326\n---------------------------------------------------------------------\nOmnibus:              1150.070       Durbin-Watson:          1.981   \nProb(Omnibus):        0.000          Jarque-Bera (JB):       1616.376\nSkew:                 0.900          Prob(JB):               0.000   \nKurtosis:             3.800          Condition No.:          6       \n=====================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified."
  },
  {
    "objectID": "slides/17-logistic-regression.html#intercept",
    "href": "slides/17-logistic-regression.html#intercept",
    "title": "Logistic regression",
    "section": "Intercept",
    "text": "Intercept\n\n\n                   Results: Ordinary least squares\n=====================================================================\nModel:                OLS              Adj. R-squared:     0.068     \nDependent Variable:   interest_rate    AIC:                59859.3779\nDate:                 2024-08-19 13:43 BIC:                59888.2185\nNo. Observations:     9998             Log-Likelihood:     -29926.   \nDf Model:             3                F-statistic:        243.7     \nDf Residuals:         9994             Prob (F-statistic): 1.25e-152 \nR-squared:            0.068            Scale:              23.309    \n---------------------------------------------------------------------\n                       Coef.  Std.Err.    t    P&gt;|t|   [0.025  0.975]\n---------------------------------------------------------------------\nconst                  9.9250   0.1401 70.8498 0.0000  9.6504 10.1996\ncredit_util            5.3356   0.2074 25.7266 0.0000  4.9291  5.7421\nhomeownership_Mortgage 0.6956   0.1208  5.7590 0.0000  0.4588  0.9323\nhomeownership_Own      0.1283   0.1552  0.8266 0.4085 -0.1760  0.4326\n---------------------------------------------------------------------\nOmnibus:              1150.070       Durbin-Watson:          1.981   \nProb(Omnibus):        0.000          Jarque-Bera (JB):       1616.376\nSkew:                 0.900          Prob(JB):               0.000   \nKurtosis:             3.800          Condition No.:          6       \n=====================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified.\n\n\n\nIntercept: Loan applicants who rent and have 0 credit utilization are predicted to receive an interest rate of 9.93%, on average."
  },
  {
    "objectID": "slides/17-logistic-regression.html#slopes",
    "href": "slides/17-logistic-regression.html#slopes",
    "title": "Logistic regression",
    "section": "Slopes",
    "text": "Slopes\n\nModelSlope\n\n\n\n\n                   Results: Ordinary least squares\n=====================================================================\nModel:                OLS              Adj. R-squared:     0.068     \nDependent Variable:   interest_rate    AIC:                59859.3779\nDate:                 2024-08-19 13:43 BIC:                59888.2185\nNo. Observations:     9998             Log-Likelihood:     -29926.   \nDf Model:             3                F-statistic:        243.7     \nDf Residuals:         9994             Prob (F-statistic): 1.25e-152 \nR-squared:            0.068            Scale:              23.309    \n---------------------------------------------------------------------\n                       Coef.  Std.Err.    t    P&gt;|t|   [0.025  0.975]\n---------------------------------------------------------------------\nconst                  9.9250   0.1401 70.8498 0.0000  9.6504 10.1996\ncredit_util            5.3356   0.2074 25.7266 0.0000  4.9291  5.7421\nhomeownership_Mortgage 0.6956   0.1208  5.7590 0.0000  0.4588  0.9323\nhomeownership_Own      0.1283   0.1552  0.8266 0.4085 -0.1760  0.4326\n---------------------------------------------------------------------\nOmnibus:              1150.070       Durbin-Watson:          1.981   \nProb(Omnibus):        0.000          Jarque-Bera (JB):       1616.376\nSkew:                 0.900          Prob(JB):               0.000   \nKurtosis:             3.800          Condition No.:          6       \n=====================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified.\n\n\n\n\n\n\nAll else held constant, for each additional percent credit utilization is higher, interest rate is predicted to be higher, on average, by 0.0534%.\nAll else held constant, the model predicts that loan applicants who have a mortgage for their home receive 0.696% higher interest rate than those who rent their home, on average.\nAll else held constant, the model predicts that loan applicants who own their home receive 0.128% higher interest rate than those who rent their home, on average."
  },
  {
    "objectID": "slides/17-logistic-regression.html#predict-loginterest-rate",
    "href": "slides/17-logistic-regression.html#predict-loginterest-rate",
    "title": "Logistic regression",
    "section": "Predict log(interest rate)",
    "text": "Predict log(interest rate)\n\nX_log = loans[['credit_checks']]\nX_log = sm.add_constant(X_log)\ny_log = np.log(loans['interest_rate'])\n\nmodel_log = sm.OLS(y_log, X_log).fit()"
  },
  {
    "objectID": "slides/17-logistic-regression.html#model-1",
    "href": "slides/17-logistic-regression.html#model-1",
    "title": "Logistic regression",
    "section": "Model",
    "text": "Model\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          interest_rate   R-squared:                       0.020\nModel:                            OLS   Adj. R-squared:                  0.020\nMethod:                 Least Squares   F-statistic:                     202.2\nDate:                Mon, 19 Aug 2024   Prob (F-statistic):           1.91e-45\nTime:                        13:43:48   Log-Likelihood:                -4912.6\nNo. Observations:                9998   AIC:                             9829.\nDf Residuals:                    9996   BIC:                             9844.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             2.3947      0.005    467.428      0.000       2.385       2.405\ncredit_checks     0.0236      0.002     14.220      0.000       0.020       0.027\n==============================================================================\nOmnibus:                      329.756   Durbin-Watson:                   2.002\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              152.256\nSkew:                          -0.010   Prob(JB):                     8.67e-34\nKurtosis:                       2.396   Cond. No.                         4.17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\\[\n\\widehat{log(interest~rate)} = 2.39 + 0.0236 \\times credit~checks\n\\]"
  },
  {
    "objectID": "slides/17-logistic-regression.html#slope-1",
    "href": "slides/17-logistic-regression.html#slope-1",
    "title": "Logistic regression",
    "section": "Slope",
    "text": "Slope\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          interest_rate   R-squared:                       0.020\nModel:                            OLS   Adj. R-squared:                  0.020\nMethod:                 Least Squares   F-statistic:                     202.2\nDate:                Mon, 19 Aug 2024   Prob (F-statistic):           1.91e-45\nTime:                        13:43:48   Log-Likelihood:                -4912.6\nNo. Observations:                9998   AIC:                             9829.\nDf Residuals:                    9996   BIC:                             9844.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             2.3947      0.005    467.428      0.000       2.385       2.405\ncredit_checks     0.0236      0.002     14.220      0.000       0.020       0.027\n==============================================================================\nOmnibus:                      329.756   Durbin-Watson:                   2.002\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              152.256\nSkew:                          -0.010   Prob(JB):                     8.67e-34\nKurtosis:                       2.396   Cond. No.                         4.17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nFor each additional credit check, log of interest rate is predicted to be higher, on average, by 0.0236%."
  },
  {
    "objectID": "slides/17-logistic-regression.html#slope-2",
    "href": "slides/17-logistic-regression.html#slope-2",
    "title": "Logistic regression",
    "section": "Slope",
    "text": "Slope\n\\[\nlog(interest~rate_{x+1}) - log(interest~rate_{x}) =  0.0236\n\\]\n\n\\[\nlog(\\frac{interest~rate_{x+1}}{interest~rate_{x}}) = 0.0236\n\\]\n\n\n\\[\ne^{log(\\frac{interest~rate_{x+1}}{interest~rate_{x}})} = e^{0.0236}\n\\]\n\n\n\\[\n\\frac{interest~rate_{x+1}}{interest~rate_{x}} = 1.024\n\\]\n\n\nFor each additional credit check, interest rate is predicted to be higher, on average, by a factor of 1.024."
  },
  {
    "objectID": "slides/17-logistic-regression.html#what-is-logistic-regression",
    "href": "slides/17-logistic-regression.html#what-is-logistic-regression",
    "title": "Logistic regression",
    "section": "What is logistic regression?",
    "text": "What is logistic regression?\n\n\n\n\nSimilar to linear regression…. but\nModeling tool when our response is categorical"
  },
  {
    "objectID": "slides/17-logistic-regression.html#modelling-binary-outcomes",
    "href": "slides/17-logistic-regression.html#modelling-binary-outcomes",
    "title": "Logistic regression",
    "section": "Modelling binary outcomes",
    "text": "Modelling binary outcomes\n\n\nVariables with binary outcomes follow the Bernouilli distribution:\n\n\\(y_i \\sim Bern(p)\\)\n\\(p\\): Probability of success\n\\(1-p\\): Probability of failure\n\nWe can’t model \\(y\\) directly, so instead we model \\(p\\)"
  },
  {
    "objectID": "slides/17-logistic-regression.html#linear-model",
    "href": "slides/17-logistic-regression.html#linear-model",
    "title": "Logistic regression",
    "section": "Linear model",
    "text": "Linear model\n\\[\np_i = \\beta_o + \\beta_1 \\times X_1 + \\cdots + \\epsilon\n\\]\n\n\nBut remember that \\(p\\) must be between 0 and 1\nWe need a link function that transforms the linear model to have an appropriate range"
  },
  {
    "objectID": "slides/17-logistic-regression.html#logit-link-function",
    "href": "slides/17-logistic-regression.html#logit-link-function",
    "title": "Logistic regression",
    "section": "Logit link function",
    "text": "Logit link function\nThe logit function take values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:\n\\[\nlogit(p) = log \\bigg( \\frac{p}{1 - p} \\bigg)\n\\]"
  },
  {
    "objectID": "slides/17-logistic-regression.html#this-isnt-exactly-what-we-need-though..",
    "href": "slides/17-logistic-regression.html#this-isnt-exactly-what-we-need-though..",
    "title": "Logistic regression",
    "section": "This isn’t exactly what we need though…..",
    "text": "This isn’t exactly what we need though…..\n\n\nRecall, the goal is to take values between -\\(\\infty\\) and \\(\\infty\\) and map them to probabilities.\nWe need the opposite of the link function… or the inverse\nTaking the inverse of the logit function will map arbitrary real values back to the range [0, 1]"
  },
  {
    "objectID": "slides/17-logistic-regression.html#generalized-linear-model",
    "href": "slides/17-logistic-regression.html#generalized-linear-model",
    "title": "Logistic regression",
    "section": "Generalized linear model",
    "text": "Generalized linear model\n\n\nWe model the logit (log-odds) of \\(p\\) :\n\n\n\n\\[\nlogit(p) = log \\bigg( \\frac{p}{1 - p} \\bigg) = \\beta_o + \\beta_1 \\times X1_i + \\cdots + \\epsilon\n\\]\n\n\n\nThen take the inverse to obtain the predicted \\(p\\):\n\n\n\n\\[\np_i = \\frac{e^{\\beta_o + \\beta_1 \\times X1_i + \\cdots + \\epsilon}}{1 + e^{\\beta_o + \\beta_1 \\times X1_i + \\cdots + \\epsilon}}\n\\]"
  },
  {
    "objectID": "slides/17-logistic-regression.html#a-logistic-model-visualized",
    "href": "slides/17-logistic-regression.html#a-logistic-model-visualized",
    "title": "Logistic regression",
    "section": "A logistic model visualized",
    "text": "A logistic model visualized"
  },
  {
    "objectID": "slides/17-logistic-regression.html#takeaways",
    "href": "slides/17-logistic-regression.html#takeaways",
    "title": "Logistic regression",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nGeneralized linear models allow us to fit models to predict non-continuous outcomes\nPredicting binary outcomes requires modeling the log-odds of success, where p = probability of success\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/04-eda.html#setup",
    "href": "slides/04-eda.html#setup",
    "title": "Exploratory data analysis",
    "section": "Setup",
    "text": "Setup\n\n# Import all required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import skewnorm\nfrom scipy.stats import kurtosis, norm\nfrom scipy.stats import gamma\nimport missingno as msno\nimport random\nimport statsmodels.api as sm\n\n# Load in UK Smoking Data\nbirths14 = pd.read_csv(\"data/births14.csv\")\n\n# Set seed\nrandom.seed(123)"
  },
  {
    "objectID": "slides/04-eda.html#what-is-exploratory-data-analysis",
    "href": "slides/04-eda.html#what-is-exploratory-data-analysis",
    "title": "Exploratory data analysis",
    "section": "What is exploratory data analysis?",
    "text": "What is exploratory data analysis?\n\nExploratory Data Analysis is a statistical approach to analyzing datasets to summarize their main characteristics, often using visual methods."
  },
  {
    "objectID": "slides/04-eda.html#examining-data",
    "href": "slides/04-eda.html#examining-data",
    "title": "Exploratory data analysis",
    "section": "Examining data",
    "text": "Examining data\n\nHeadInfoDescribe\n\n\n\nbirths14.head()\n\n\n\n\n\n\n\n\nfage\nmage\nmature\nweeks\npremie\nvisits\ngained\nweight\nlowbirthweight\nsex\nhabit\nmarital\nwhitemom\n\n\n\n\n0\n34.0\n34\nyounger mom\n37\nfull term\n14.0\n28.0\n6.96\nnot low\nmale\nnonsmoker\nmarried\nwhite\n\n\n1\n36.0\n31\nyounger mom\n41\nfull term\n12.0\n41.0\n8.86\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n2\n37.0\n36\nmature mom\n37\nfull term\n10.0\n28.0\n7.51\nnot low\nfemale\nnonsmoker\nmarried\nnot white\n\n\n3\nNaN\n16\nyounger mom\n38\nfull term\nNaN\n29.0\n6.19\nnot low\nmale\nnonsmoker\nnot married\nwhite\n\n\n4\n32.0\n31\nyounger mom\n36\npremie\n12.0\n48.0\n6.75\nnot low\nfemale\nnonsmoker\nmarried\nwhite\n\n\n\n\n\n\n\n\n\n\nbirths14.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 13 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   fage            886 non-null    float64\n 1   mage            1000 non-null   int64  \n 2   mature          1000 non-null   object \n 3   weeks           1000 non-null   int64  \n 4   premie          1000 non-null   object \n 5   visits          944 non-null    float64\n 6   gained          958 non-null    float64\n 7   weight          1000 non-null   float64\n 8   lowbirthweight  1000 non-null   object \n 9   sex             1000 non-null   object \n 10  habit           981 non-null    object \n 11  marital         1000 non-null   object \n 12  whitemom        1000 non-null   object \ndtypes: float64(4), int64(2), object(7)\nmemory usage: 101.7+ KB\n\n\n\n\n\nbirths14.describe()\n\n\n\n\n\n\n\n\nfage\nmage\nweeks\nvisits\ngained\nweight\n\n\n\n\ncount\n886.000000\n1000.000000\n1000.000000\n944.000000\n958.000000\n1000.000000\n\n\nmean\n31.133183\n28.449000\n38.666000\n11.351695\n30.425887\n7.198160\n\n\nstd\n7.058135\n5.759737\n2.564961\n4.108192\n15.242527\n1.306775\n\n\nmin\n15.000000\n14.000000\n21.000000\n0.000000\n0.000000\n0.750000\n\n\n25%\n26.000000\n24.000000\n38.000000\n9.000000\n20.000000\n6.545000\n\n\n50%\n31.000000\n28.000000\n39.000000\n12.000000\n30.000000\n7.310000\n\n\n75%\n35.000000\n33.000000\n40.000000\n14.000000\n38.000000\n8.000000\n\n\nmax\n85.000000\n47.000000\n46.000000\n30.000000\n98.000000\n10.620000"
  },
  {
    "objectID": "slides/04-eda.html#visualizing-data-relationships",
    "href": "slides/04-eda.html#visualizing-data-relationships",
    "title": "Exploratory data analysis",
    "section": "Visualizing data relationships",
    "text": "Visualizing data relationships\n\nsns.pairplot(births14[['fage', 'mage', 'weeks', 'mature']], hue='mature', height=2)\nplt.show()"
  },
  {
    "objectID": "slides/04-eda.html#group-descriptive-statistics",
    "href": "slides/04-eda.html#group-descriptive-statistics",
    "title": "Exploratory data analysis",
    "section": "Group descriptive statistics",
    "text": "Group descriptive statistics\n\n# Example with the premie column\nbirths14.groupby('premie').describe()\n\n\n\n\n\n\n\n\nfage\nmage\n...\ngained\nweight\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\npremie\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfull term\n775.0\n30.967742\n6.681591\n15.0\n26.0\n31.0\n35.0\n49.0\n876.0\n28.329909\n...\n38.0\n98.0\n876.0\n7.434178\n1.021699\n3.93\n6.77\n7.44\n8.0825\n10.62\n\n\npremie\n111.0\n32.288288\n9.226826\n15.0\n27.0\n32.0\n36.0\n85.0\n124.0\n29.290323\n...\n41.0\n85.0\n124.0\n5.530806\n1.801182\n0.75\n4.50\n5.75\n6.5725\n9.25\n\n\n\n\n2 rows × 48 columns"
  },
  {
    "objectID": "slides/04-eda.html#outliers",
    "href": "slides/04-eda.html#outliers",
    "title": "Exploratory data analysis",
    "section": "Outliers",
    "text": "Outliers\nOutliers are data points that are significantly different from others. Identifying and handling outliers is important in data analysis.\n\n\n\nOutliers = 1.5 * Interquartile range"
  },
  {
    "objectID": "slides/04-eda.html#assess-outliers-visually",
    "href": "slides/04-eda.html#assess-outliers-visually",
    "title": "Exploratory data analysis",
    "section": "Assess outliers visually",
    "text": "Assess outliers visually\n\nsns.boxplot(data = births14, x = 'weight', width = 0.20)\nplt.show()"
  },
  {
    "objectID": "slides/04-eda.html#find-outliers",
    "href": "slides/04-eda.html#find-outliers",
    "title": "Exploratory data analysis",
    "section": "Find outliers",
    "text": "Find outliers\n\nOutputCode\n\n\n\n\nfage: 7 outliers\nmage: 1 outliers\nweeks: 72 outliers\nvisits: 30 outliers\ngained: 26 outliers\nweight: 32 outliers\n\n\n\n\n\nfor column in births14.select_dtypes(include=np.number).columns:\n    q25 = births14[column].quantile(0.25)\n    q75 = births14[column].quantile(0.75)\n    iqr = q75 - q25\n    lower_bound = q25 - 1.5 * iqr\n    upper_bound = q75 + 1.5 * iqr\n    outliers = births14[(births14[column] &lt; lower_bound) | (births14[column] &gt; upper_bound)]\n    print(f\"{column}: {outliers.shape[0]} outliers\")\n\n\n\n\n\n\nq25: 1/4 quartile, 25th percentile; q75: 3/4 quartile, 75th percentile\nIQR: interquartile range, \\(IQR = q75-q25\\)\nlower; upper: lower, upper limit of \\(1.5\\times IQR\\) used to calculate outliers"
  },
  {
    "objectID": "slides/04-eda.html#remove-outliers",
    "href": "slides/04-eda.html#remove-outliers",
    "title": "Exploratory data analysis",
    "section": "Remove outliers",
    "text": "Remove outliers\n\nCleaningPlot\n\n\n\n# Select numerical columns\nnumerical_cols = births14.select_dtypes(include = ['number']).columns\n\nfor col in numerical_cols:\n    # Find Q1, Q3, and interquartile range (IQR) for each column\n    Q1 = births14[col].quantile(0.25)\n    Q3 = births14[col].quantile(0.75)\n    IQR = Q3 - Q1\n    # Upper and lower bounds for each column\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Filter out the outliers from the DataFrame\n    births14_clean = births14[(births14[col] &gt;= lower_bound) & (births14[col] &lt;= upper_bound)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are there still outliers?"
  },
  {
    "objectID": "slides/04-eda.html#missing-values-nan",
    "href": "slides/04-eda.html#missing-values-nan",
    "title": "Exploratory data analysis",
    "section": "Missing values (NaN)",
    "text": "Missing values (NaN)\n\n# Count missing values in each column\nbirths14.isnull().sum()\n\nfage              114\nmage                0\nmature              0\nweeks               0\npremie              0\nvisits             56\ngained             42\nweight              0\nlowbirthweight      0\nsex                 0\nhabit              19\nmarital             0\nwhitemom            0\ndtype: int64"
  },
  {
    "objectID": "slides/04-eda.html#visualizing-nan",
    "href": "slides/04-eda.html#visualizing-nan",
    "title": "Exploratory data analysis",
    "section": "Visualizing (NaN)",
    "text": "Visualizing (NaN)\nWe can use the missingno library to visualize missing data.\n\nmsno.bar(births14, figsize = (7, 5), fontsize = 10)\nplt.show()"
  },
  {
    "objectID": "slides/04-eda.html#describe-categorical-variables",
    "href": "slides/04-eda.html#describe-categorical-variables",
    "title": "Exploratory data analysis",
    "section": "Describe categorical variables",
    "text": "Describe categorical variables\n\nDescribeUnique levelsCode\n\n\n\nbirths14.describe(exclude = [np.number])\n\n\n\n\n\n\n\n\nmature\npremie\nlowbirthweight\nsex\nhabit\nmarital\nwhitemom\n\n\n\n\ncount\n1000\n1000\n1000\n1000\n981\n1000\n1000\n\n\nunique\n2\n2\n2\n2\n2\n2\n2\n\n\ntop\nyounger mom\nfull term\nnot low\nmale\nnonsmoker\nmarried\nwhite\n\n\nfreq\n841\n876\n919\n505\n867\n594\n765\n\n\n\n\n\n\n\n\n\n\n\nmature: ['younger mom' 'mature mom']\npremie: ['full term' 'premie']\nlowbirthweight: ['not low' 'low']\nsex: ['male' 'female']\nhabit: ['nonsmoker' 'smoker' nan]\nmarital: ['married' 'not married']\nwhitemom: ['white' 'not white']\n\n\n\n\n\nfor column in births14.select_dtypes(include=['object', 'category']).columns:\n    print(f\"{column}: {births14[column].unique()}\")"
  },
  {
    "objectID": "slides/04-eda.html#normality-check-1",
    "href": "slides/04-eda.html#normality-check-1",
    "title": "Exploratory data analysis",
    "section": "Normality check",
    "text": "Normality check\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram: bell-shaped curve\nSkewness: Close to 0 for symmetry; Kurtosis: Close to 3 for normal “tailedness.”\nSample Size: Larger samples are less sensitive to non-normality.\nEmpirical Rule: 68-95-99.7% rule (1, 2, and 3 st dev. of the mean)."
  },
  {
    "objectID": "slides/04-eda.html#skewness",
    "href": "slides/04-eda.html#skewness",
    "title": "Exploratory data analysis",
    "section": "Skewness",
    "text": "Skewness\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral definitions\nSensitive to outliers\nDesigned for one peak (unimodal)"
  },
  {
    "objectID": "slides/04-eda.html#kurtosis",
    "href": "slides/04-eda.html#kurtosis",
    "title": "Exploratory data analysis",
    "section": "Kurtosis",
    "text": "Kurtosis\n\n\n\n\n\n\n\n\n\n\n\n\nSensitive to outliers\nDesigned for one peak (unimodal)"
  },
  {
    "objectID": "slides/04-eda.html#q-q-plot",
    "href": "slides/04-eda.html#q-q-plot",
    "title": "Exploratory data analysis",
    "section": "Q-Q plot",
    "text": "Q-Q plot\n\nNormalNegative skewPositive skew"
  },
  {
    "objectID": "slides/04-eda.html#testing-normality-data-shape",
    "href": "slides/04-eda.html#testing-normality-data-shape",
    "title": "Exploratory data analysis",
    "section": "Testing normality: data shape",
    "text": "Testing normality: data shape\n\n\nCode\n# Make a copy of the data \ndataCopy = births14.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Q-Q plot\nsm.qqplot(dataCopyFin.weight, line='s')\nplt.title('Newborn Weight Q-Q plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNegative-skew (left-tailed)"
  },
  {
    "objectID": "slides/04-eda.html#conclusions",
    "href": "slides/04-eda.html#conclusions",
    "title": "Exploratory data analysis",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nAlways inspect your data first.\nVisualize relationships and distributions.\nIdentify and handle outliers and missing values.\nCheck for normality and understand the distribution of your data."
  },
  {
    "objectID": "slides/24-communicate.html#project",
    "href": "slides/24-communicate.html#project",
    "title": "Communicating data science results effectively",
    "section": "Project",
    "text": "Project\n\nReview peer evaluations left by your peers, implement updates as you see fit, close the issue once you review them.\nHave a clear plan for who is doing what, open issues on your repo, and assign them to individuals who can then close the issues as they finish a task.\nSchedule at least one team meeting between today and your presentation to practice your presentation together.\n\n\nAny project questions?"
  },
  {
    "objectID": "slides/24-communicate.html#take-a-sad-plot-make-it-better",
    "href": "slides/24-communicate.html#take-a-sad-plot-make-it-better",
    "title": "Communicating data science results effectively",
    "section": "Take A Sad Plot & Make It Better",
    "text": "Take A Sad Plot & Make It Better\n\n\n\n\n\n\n\nSource: https://alison.netlify.app/rlm-sad-plot-better"
  },
  {
    "objectID": "slides/24-communicate.html#recap-of-data-viz",
    "href": "slides/24-communicate.html#recap-of-data-viz",
    "title": "Communicating data science results effectively",
    "section": "Recap of data viz",
    "text": "Recap of data viz\n\n\nRepresent percentages as parts of a whole\nPlace variables representing time on the x-axis when possible\nPay attention to data types, e.g., represent time as time on a continuous scale, not years as levels of a categorical variable\nPrefer direct labeling over legends\nUse accessible colors\nUse color to draw attention\nPick a purpose and label, color, annotate for that purpose\nCommunicate your main message directly in the plot labels\nSimplify before you call it done (a.k.a. “Before you leave the house, look in the mirror and take one thing off”)"
  },
  {
    "objectID": "slides/24-communicate.html#project-presentations-due-dec-18",
    "href": "slides/24-communicate.html#project-presentations-due-dec-18",
    "title": "Communicating data science results effectively",
    "section": "Project presentations due Dec 18! 🥳",
    "text": "Project presentations due Dec 18! 🥳\n\nMake sure your presentation is pushed to your GitHub repo before the due date.\nAll team members must take part in the presentation\nRecord your 10-minute presentation – you’ll lose 1 point/ minute over 10.\nFill out feedback forms while you listen to others’ presentations."
  },
  {
    "objectID": "slides/24-communicate.html#project-write-ups-due-dec-20",
    "href": "slides/24-communicate.html#project-write-ups-due-dec-20",
    "title": "Communicating data science results effectively",
    "section": "Project write-ups due Dec 20",
    "text": "Project write-ups due Dec 20\n\nThere’s a good chance you’ll be done with these on Monday as well\nBut you might want to improve your write-up based on inspiration from other teams’ presentations and/or ideas that came up during your peer-reviews."
  },
  {
    "objectID": "slides/24-communicate.html#expectations",
    "href": "slides/24-communicate.html#expectations",
    "title": "Communicating data science results effectively",
    "section": "Expectations",
    "text": "Expectations\n\nThe goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like) and apply them to a novel dataset in a meaningful way.\n\nBeyond, if you like – “you” is the whole team!\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/description.html#introduction"
  },
  {
    "objectID": "slides/24-communicate.html#expectations-1",
    "href": "slides/24-communicate.html#expectations-1",
    "title": "Communicating data science results effectively",
    "section": "Expectations",
    "text": "Expectations\n\nThe goal is not to do an exhaustive data analysis i.e., do not calculate every statistic and procedure you have learned for every variable, but rather let me know that you are proficient at asking meaningful questions and answering them with results of data analysis, that you are proficient in using Python, and that you are proficient at interpreting and presenting the results.\n\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/description.html#introduction"
  },
  {
    "objectID": "slides/24-communicate.html#requirements",
    "href": "slides/24-communicate.html#requirements",
    "title": "Communicating data science results effectively",
    "section": "Requirements",
    "text": "Requirements\n\nFocus on methods that help you begin to answer your research questions. You do not have to apply every statistical procedure we learned.\n\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/description.html#introduction"
  },
  {
    "objectID": "slides/24-communicate.html#tip",
    "href": "slides/24-communicate.html#tip",
    "title": "Communicating data science results effectively",
    "section": "Tip",
    "text": "Tip\n\nCritique your own methods and provide suggestions for improving your analysis. Discuss issues pertaining to the reliability and validity of your data, and appropriateness of the statistical analysis.\n\n\n\n\n\n\n\nTip\n\n\nYou can critique the current research without talking about a hypothetical future research.\n\n\n\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/description.html#introduction"
  },
  {
    "objectID": "slides/24-communicate.html#how-many-plots",
    "href": "slides/24-communicate.html#how-many-plots",
    "title": "Communicating data science results effectively",
    "section": "How many plots",
    "text": "How many plots\n\nYou do not need to visualize all of the data at once. A single high-quality visualization will receive a much higher grade than a large number of poor-quality visualizations.\n\nThere is no specific, secret number of visualizations I’m expecting, the right number is the number that it takes to answer your question.\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/description.html#introduction"
  },
  {
    "objectID": "slides/24-communicate.html#submission",
    "href": "slides/24-communicate.html#submission",
    "title": "Communicating data science results effectively",
    "section": "Submission",
    "text": "Submission\n\nSubmission of these deliverables will happen on GitHub and feedback will be provided as GitHub issues that you need to engage with and close. The collection of the documents in your GitHub repo will create a webpage for your project. To create the webpage go to VS Code, open a terminal and type quarto publish gh-pages or if you have published via Quarto Pubs `quarto publish quarto-pub\n\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/description.html#introduction"
  },
  {
    "objectID": "slides/24-communicate.html#writeup",
    "href": "slides/24-communicate.html#writeup",
    "title": "Communicating data science results effectively",
    "section": "Writeup",
    "text": "Writeup\n\n\nIs there any paper that is required as well as the presentation?\nWhat is the project write up?\nAre write ups usually around the 10 page limit?\nIs there a recommended outline to the project?\n\n\n\n\n\n\n\n\nRead more\n\n\nhttps://datasciaz.netlify.app/project/4-writeup-presentation.html#report-components"
  },
  {
    "objectID": "slides/24-communicate.html#grading-rubric",
    "href": "slides/24-communicate.html#grading-rubric",
    "title": "Communicating data science results effectively",
    "section": "Grading / rubric",
    "text": "Grading / rubric\n\n\n\n\n\n\nRead more\n\n\n\nOverall: https://datasciaz.netlify.app/project/description.html#grading-summary\nWrite-up: https://datasciaz.netlify.app/project/4-writeup-presentation.html#report-components\nPresentation: https://datasciaz.netlify.app/project/4-writeup-presentation.html#presentation-slides"
  },
  {
    "objectID": "slides/24-communicate.html#your-project-write-up-with-quarto",
    "href": "slides/24-communicate.html#your-project-write-up-with-quarto",
    "title": "Communicating data science results effectively",
    "section": "Your project write-up with Quarto",
    "text": "Your project write-up with Quarto\n\nChunk options around what makes it in your final report: message, echo, etc.\nCitations.\nFinalizing your report with echo: false."
  },
  {
    "objectID": "slides/24-communicate.html#building-your-project-website-with-quarto",
    "href": "slides/24-communicate.html#building-your-project-website-with-quarto",
    "title": "Communicating data science results effectively",
    "section": "Building your project website with Quarto",
    "text": "Building your project website with Quarto\n\nThe _site folder.\nMaking sure your website reflects your latest changes.\nCustomizing the look of your website."
  },
  {
    "objectID": "slides/24-communicate.html#slides",
    "href": "slides/24-communicate.html#slides",
    "title": "Communicating data science results effectively",
    "section": "Slides",
    "text": "Slides\n\nOption 1: Make your slides not in Quarto but make sure they’re available in your Quarto project website.\nOption 2: Make your slides with Quarto."
  },
  {
    "objectID": "slides/24-communicate.html#something-else",
    "href": "slides/24-communicate.html#something-else",
    "title": "Communicating data science results effectively",
    "section": "Something else 💛",
    "text": "Something else 💛\n\nI have enjoyed this semester, and I want to continue learning Python. What classes do you recommend I take to continue my learning?\n\n\nINFO 523: Data Mining and Discovery - Python essentially for applied ML\nINFO 521: Intro to Machine Learning - Python as a part of the ML curriculum"
  },
  {
    "objectID": "slides/15-linear-regression.html#setup",
    "href": "slides/15-linear-regression.html#setup",
    "title": "Linear regression",
    "section": "Setup",
    "text": "Setup\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom great_tables import GT, style, loc, exibble\n\n# Setting the theme for plots\nsns.set_theme(style=\"whitegrid\", font_scale=1.2)"
  },
  {
    "objectID": "slides/15-linear-regression.html#goals",
    "href": "slides/15-linear-regression.html#goals",
    "title": "Linear regression",
    "section": "Goals",
    "text": "Goals\n\n\nWhat is a model?\nWhy do we model?\nWhat is correlation?"
  },
  {
    "objectID": "slides/15-linear-regression.html#lets-drive-a-tesla",
    "href": "slides/15-linear-regression.html#lets-drive-a-tesla",
    "title": "Linear regression",
    "section": "Let’s drive a Tesla!",
    "text": "Let’s drive a Tesla!"
  },
  {
    "objectID": "slides/15-linear-regression.html#semi-or-garage",
    "href": "slides/15-linear-regression.html#semi-or-garage",
    "title": "Linear regression",
    "section": "Semi or garage?",
    "text": "Semi or garage?\n\ni love how Tesla thinks the wall in my garage is a semi. 😅\n\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/15-linear-regression.html#semi-or-garage-1",
    "href": "slides/15-linear-regression.html#semi-or-garage-1",
    "title": "Linear regression",
    "section": "Semi or garage?",
    "text": "Semi or garage?\n\nNew owner here. Just parked in my garage. Tesla thinks I crashed onto a semi.\n\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/15-linear-regression.html#car-or-trash",
    "href": "slides/15-linear-regression.html#car-or-trash",
    "title": "Linear regression",
    "section": "Car or trash?",
    "text": "Car or trash?\n\nTesla calls Mercedes trash\n\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/15-linear-regression.html#leisure-commute-physical-activity-and-bp",
    "href": "slides/15-linear-regression.html#leisure-commute-physical-activity-and-bp",
    "title": "Linear regression",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\n\nRelation Between Leisure Time, Commuting, and Occupational Physical Activity With Blood Pressure in 125,402 Adults: The Lifelines Cohort\nByambasukh, Oyuntugs, Harold Snieder, and Eva Corpeleijn. “Relation between leisure time, commuting, and occupational physical activity with blood pressure in 125 402 adults: the lifelines cohort.” Journal of the American Heart Association 9.4 (2020): e014313."
  },
  {
    "objectID": "slides/15-linear-regression.html#leisure-commute-physical-activity-and-bp-1",
    "href": "slides/15-linear-regression.html#leisure-commute-physical-activity-and-bp-1",
    "title": "Linear regression",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\nBackground: Whether all domains of daily‐life moderate‐to‐vigorous physical activity (MVPA) are associated with lower blood pressure (BP) and how this association depends on age and body mass index remains unclear.\nMethods and Results: In the population‐based Lifelines cohort (N=125,402), MVPA was assessed by the Short Questionnaire to Assess Health‐Enhancing Physical Activity, a validated questionnaire in different domains such as commuting, leisure‐time, and occupational PA. BP was assessed using the last 3 of 10 measurements after 10 minutes’ rest in the supine position. Hypertension was defined as systolic BP ≥140 mm Hg and/or diastolic BP ≥90 mm Hg and/or use of antihypertensives. In regression analysis, higher commuting and leisure‐time but not occupational MVPA related to lower BP and lower hypertension risk. Commuting‐and‐leisure‐time MVPA was associated with BP in a dose‐dependent manner. β Coefficients (95% CI) from linear regression analyses were −1.64 (−2.03 to −1.24), −2.29 (−2.68 to −1.90), and finally −2.90 (−3.29 to −2.50) mm Hg systolic BP for the low, middle, and highest tertile of MVPA compared with “No MVPA” as the reference group after adjusting for age, sex, education, smoking and alcohol use. Further adjustment for body mass index attenuated the associations by 30% to 50%, but more MVPA remained significantly associated with lower BP and lower risk of hypertension. This association was age dependent. β Coefficients (95% CI) for the highest tertiles of commuting‐and‐leisure‐time MVPA were −1.67 (−2.20 to −1.15), −3.39 (−3.94 to −2.82) and −4.64 (−6.15 to −3.14) mm Hg systolic BP in adults &lt;40, 40 to 60, and &gt;60 years, respectively.\nConclusions: Higher commuting and leisure‐time but not occupational MVPA were significantly associated with lower BP and lower hypertension risk at all ages, but these associations were stronger in older adults."
  },
  {
    "objectID": "slides/15-linear-regression.html#modeling-cars",
    "href": "slides/15-linear-regression.html#modeling-cars",
    "title": "Linear regression",
    "section": "Modeling cars",
    "text": "Modeling cars\n\nQuestionsPlot\n\n\n\n\nWhat is the relationship between cars’ weights and their mileage?\nWhat is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/15-linear-regression.html#modelling-cars",
    "href": "slides/15-linear-regression.html#modelling-cars",
    "title": "Linear regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "slides/15-linear-regression.html#modelling-cars-1",
    "href": "slides/15-linear-regression.html#modelling-cars-1",
    "title": "Linear regression",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/15-linear-regression.html#modelling",
    "href": "slides/15-linear-regression.html#modelling",
    "title": "Linear regression",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "slides/15-linear-regression.html#modelling-vocabulary",
    "href": "slides/15-linear-regression.html#modelling-vocabulary",
    "title": "Linear regression",
    "section": "Modelling vocabulary",
    "text": "Modelling vocabulary\n\nPredictor (explanatory variable)\nOutcome (response variable)\nRegression line\n\nSlope\nIntercept\n\nCorrelation"
  },
  {
    "objectID": "slides/15-linear-regression.html#predictor-explanatory-variable",
    "href": "slides/15-linear-regression.html#predictor-explanatory-variable",
    "title": "Linear regression",
    "section": "Predictor (explanatory variable)",
    "text": "Predictor (explanatory variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nweight\n\n\n\n\n18.0\n3504\n\n\n15.0\n3693\n\n\n18.0\n3436\n\n\n16.0\n3433\n\n\n17.0\n3449\n\n\n15.0\n4341\n\n\n...\n..."
  },
  {
    "objectID": "slides/15-linear-regression.html#outcome-response-variable",
    "href": "slides/15-linear-regression.html#outcome-response-variable",
    "title": "Linear regression",
    "section": "Outcome (response variable)",
    "text": "Outcome (response variable)\n\n\n\n\n\n\n\n\n\n\nmpg\nweight\n\n\n\n\n18.0\n3504\n\n\n15.0\n3693\n\n\n18.0\n3436\n\n\n16.0\n3433\n\n\n17.0\n3449\n\n\n15.0\n4341\n\n\n...\n..."
  },
  {
    "objectID": "slides/15-linear-regression.html#regression-line",
    "href": "slides/15-linear-regression.html#regression-line",
    "title": "Linear regression",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "slides/15-linear-regression.html#regression-line-slope",
    "href": "slides/15-linear-regression.html#regression-line-slope",
    "title": "Linear regression",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "slides/15-linear-regression.html#regression-line-intercept",
    "href": "slides/15-linear-regression.html#regression-line-intercept",
    "title": "Linear regression",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "slides/15-linear-regression.html#correlation",
    "href": "slides/15-linear-regression.html#correlation",
    "title": "Linear regression",
    "section": "Correlation",
    "text": "Correlation\n\n\nCorrelation coefficient: -0.83"
  },
  {
    "objectID": "slides/15-linear-regression.html#correlation-1",
    "href": "slides/15-linear-regression.html#correlation-1",
    "title": "Linear regression",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope."
  },
  {
    "objectID": "slides/15-linear-regression.html#visualizing-the-model",
    "href": "slides/15-linear-regression.html#visualizing-the-model",
    "title": "Linear regression",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\n\nCode\nsns.lmplot(x=\"weight\", y=\"mpg\", data=mtcars, ci=None, scatter_kws={\"s\": 50, \"alpha\": 0.5}, line_kws={\"color\": \"#325b74\"})\nplt.xlabel(\"Weight (1,000 lbs)\")\nplt.ylabel(\"Miles per gallon (MPG)\")\nplt.title(\"MPG vs. weights of cars\")\nplt.show()"
  },
  {
    "objectID": "slides/15-linear-regression.html#data-prep",
    "href": "slides/15-linear-regression.html#data-prep",
    "title": "Linear regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nfandango = pd.read_csv(\"data/fandango.csv\")\nmovie_scores = fandango.rename(columns={\"rt_norm\": \"critics\", \"rt_user_norm\": \"audience\"})"
  },
  {
    "objectID": "slides/15-linear-regression.html#data-overview",
    "href": "slides/15-linear-regression.html#data-overview",
    "title": "Linear regression",
    "section": "Data overview",
    "text": "Data overview\n\nprint(movie_scores[[\"critics\", \"audience\"]].head())\n\n   critics  audience\n0     3.70       4.3\n1     4.25       4.0\n2     4.00       4.5\n3     0.90       4.2\n4     0.70       1.4"
  },
  {
    "objectID": "slides/15-linear-regression.html#data-visualization",
    "href": "slides/15-linear-regression.html#data-visualization",
    "title": "Linear regression",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/15-linear-regression.html#regression-model-1",
    "href": "slides/15-linear-regression.html#regression-model-1",
    "title": "Linear regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/15-linear-regression.html#regression-model",
    "href": "slides/15-linear-regression.html#regression-model",
    "title": "Linear regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{#325b74}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{#325b74}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{#325b74}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-linear-regression.html#simple-linear-regression",
    "href": "slides/15-linear-regression.html#simple-linear-regression",
    "title": "Linear regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationship between a quantitative outcome (\\(Y\\)) and a single quantitative predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/15-linear-regression.html#simple-linear-regression-1",
    "href": "slides/15-linear-regression.html#simple-linear-regression-1",
    "title": "Linear regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\\Large{\\hat{Y} = b_0 + b_1 X}\\]\n\n\\(b_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(b_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/15-linear-regression.html#residuals",
    "href": "slides/15-linear-regression.html#residuals",
    "title": "Linear regression",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\n\n\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\\]"
  },
  {
    "objectID": "slides/15-linear-regression.html#least-squares-line",
    "href": "slides/15-linear-regression.html#least-squares-line",
    "title": "Linear regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/15-linear-regression.html#least-squares-line-1",
    "href": "slides/15-linear-regression.html#least-squares-line-1",
    "title": "Linear regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nslope, intercept = model.coef_[0], model.intercept_\nprint(f\"Slope: {slope:.2f}, Intercept: {intercept:.2f}\")\n\nSlope: 0.52, Intercept: 1.62"
  },
  {
    "objectID": "slides/15-linear-regression.html#properties-of-least-squares-regression",
    "href": "slides/15-linear-regression.html#properties-of-least-squares-regression",
    "title": "Linear regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nThe regression line goes through the center of mass point (the coordinates corresponding to average \\(X\\) and average \\(Y\\)): \\(b_0 = \\bar{Y} - b_1~\\bar{X}\\)\nSlope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_Y}{s_X}\\)\nSum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nResiduals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/15-linear-regression.html#interpreting-slope-intercept",
    "href": "slides/15-linear-regression.html#interpreting-slope-intercept",
    "title": "Linear regression",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\\widehat{\\text{audience}} = 32.3 + 0.519 \\times \\text{critics}\\]\n\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\nIntercept: If the critics score is 0 points, we expect the audience score to be 32.3 points."
  },
  {
    "objectID": "slides/15-linear-regression.html#is-the-intercept-meaningful",
    "href": "slides/15-linear-regression.html#is-the-intercept-meaningful",
    "title": "Linear regression",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n✅ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\n🛑 Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/15-linear-regression.html#application-exercise-ae-10-modeling-fish",
    "href": "slides/15-linear-regression.html#application-exercise-ae-10-modeling-fish",
    "title": "Linear regression",
    "section": "Application exercise: ae-10-modeling-fish",
    "text": "Application exercise: ae-10-modeling-fish\n\n\nGo back to your project called ae.\nIf there are any uncommitted files, commit them, and push.\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/10-ethics.html#today",
    "href": "slides/10-ethics.html#today",
    "title": "Data science ethics",
    "section": "Today",
    "text": "Today\nData science ethics:\n\nMisrepresentation\nData privacy\nAlgorithmic bias"
  },
  {
    "objectID": "slides/10-ethics.html#stand-your-ground",
    "href": "slides/10-ethics.html#stand-your-ground",
    "title": "Data science ethics",
    "section": "Stand your ground",
    "text": "Stand your ground\n\nIn 2005, the Florida legislature passed the controversial “Stand Your Ground” law that broadened the situations in which citizens can use lethal force to protect themselves against perceived threats. Advocates believed that the new law would ultimately reduce crime; opponents feared an increase in the use of lethal force."
  },
  {
    "objectID": "slides/10-ethics.html#stand-your-ground-1",
    "href": "slides/10-ethics.html#stand-your-ground-1",
    "title": "Data science ethics",
    "section": "Stand your ground",
    "text": "Stand your ground\n\nQuestion\n\n\n\nWhat does the visualization, published by Reuters on Feb 16, 2014, say about the number of firearm murders in Florida over time?"
  },
  {
    "objectID": "slides/10-ethics.html#your-data",
    "href": "slides/10-ethics.html#your-data",
    "title": "Data science ethics",
    "section": "“Your” data",
    "text": "“Your” data\n\nEvery time we use apps, websites, and devices, our data is being collected and used or sold to others.\nMore importantly, decisions are made by law enforcement, financial institutions, and governments based on data that directly affect the lives of people."
  },
  {
    "objectID": "slides/10-ethics.html#privacy-of-your-data",
    "href": "slides/10-ethics.html#privacy-of-your-data",
    "title": "Data science ethics",
    "section": "Privacy of your data",
    "text": "Privacy of your data\n\nWhat pieces of data have you left on the internet today? Think through everything you’ve logged into, clicked on, checked in, either actively or automatically, that might be tracking you. Do you know where that data is stored? Who it can be accessed by? Whether it’s shared with others?"
  },
  {
    "objectID": "slides/10-ethics.html#sharing-your-data",
    "href": "slides/10-ethics.html#sharing-your-data",
    "title": "Data science ethics",
    "section": "Sharing your data",
    "text": "Sharing your data\n\nWhat are you OK with sharing?\n\n\n\n\n\nName\nAge\nEmail\nPhone Number\nList of every video you watch\nList of every video you comment on\n\n\n\n\n\nHow you type: speed, accuracy\nHow long you spend on different content\nList of all your private messages (date, time, person sent to)\nInfo about your photos (how it was taken, where it was taken (GPS), when it was taken)"
  },
  {
    "objectID": "slides/10-ethics.html#what-does-google-thinkknow-about-you",
    "href": "slides/10-ethics.html#what-does-google-thinkknow-about-you",
    "title": "Data science ethics",
    "section": "What does Google think/know about you?",
    "text": "What does Google think/know about you?\n\nHave you ever thought about why you’re seeing an ad on Google? Google it! Try to figure out if you have ad personalization on and how your ads are personalized."
  },
  {
    "objectID": "slides/10-ethics.html#your-browing-history",
    "href": "slides/10-ethics.html#your-browing-history",
    "title": "Data science ethics",
    "section": "Your browing history",
    "text": "Your browing history\n\nWhich of the following are you OK with your browsing history to be used towards?\n\n\n\nFor serving you targeted ads\nTo score you as a candidate for a job\nTo predict your race/ethnicity for voting purposes"
  },
  {
    "objectID": "slides/10-ethics.html#who-else-gets-to-use-your-data",
    "href": "slides/10-ethics.html#who-else-gets-to-use-your-data",
    "title": "Data science ethics",
    "section": "Who else gets to use your data?",
    "text": "Who else gets to use your data?\n\nSuppose you create a profile on a social media site and share your personal information on your profile. Who else gets to use that data?\n\n\n\nCompanies the social media company has a connection to?\nCompanies the social media company sells your data to?\nResearchers?"
  },
  {
    "objectID": "slides/10-ethics.html#ok-cupid-data-breach",
    "href": "slides/10-ethics.html#ok-cupid-data-breach",
    "title": "Data science ethics",
    "section": "OK Cupid data breach",
    "text": "OK Cupid data breach\n\nIn 2016, researchers published data of 70,000 OkCupid users—including usernames, political leanings, drug usage, and intimate sexual details\nResearchers didn’t release the real names and pictures of OKCupid users, but their identities could easily be uncovered from the details provided, e.g. usernames\n\n\n\n\n\nSome may object to the ethics of gathering and releasing this data. However, all the data found in the dataset are or were already publicly available, so releasing this dataset merely presents it in a more useful form.\nResearchers Emil Kirkegaard and Julius Daugbjerg Bjerrekær"
  },
  {
    "objectID": "slides/10-ethics.html#garbage-in-garbage-out",
    "href": "slides/10-ethics.html#garbage-in-garbage-out",
    "title": "Data science ethics",
    "section": "Garbage in, garbage out",
    "text": "Garbage in, garbage out\n\nIn statistical modeling and inference we talk about “garbage in, garbage out” – if you don’t have good (random, representative) data, results of your analysis will not be reliable or generalizable.\nCorollary: Bias in, bias out."
  },
  {
    "objectID": "slides/10-ethics.html#google-translate",
    "href": "slides/10-ethics.html#google-translate",
    "title": "Data science ethics",
    "section": "Google translate",
    "text": "Google translate\n\nWhat might be the reason for Google’s gendered translation? How do ethics play into this situation?\n\n\n\n\n\n\n\n\nSource: Engadget - Google is working to remove gender bias in its translations"
  },
  {
    "objectID": "slides/10-ethics.html#stochastic-parrots",
    "href": "slides/10-ethics.html#stochastic-parrots",
    "title": "Data science ethics",
    "section": "Stochastic parrots",
    "text": "Stochastic parrots\n\nExcerptQuestion\n\n\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 (Bender et. al., 2021)\n\nThe past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\n\n\n\n\nWhat is meant by “stochastic parrots” in the paper title?"
  },
  {
    "objectID": "slides/10-ethics.html#machine-bias",
    "href": "slides/10-ethics.html#machine-bias",
    "title": "Data science ethics",
    "section": "Machine Bias",
    "text": "Machine Bias\n2016 ProPublica article on algorithm used for rating a defendant’s risk of future crime:\n\n\n\nIn forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.\n\nThe formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\nWhite defendants were mislabeled as low risk more often than black defendants.\n\n\n\n\n\n\n\n\n\n\n\nSource: ProPublica"
  },
  {
    "objectID": "slides/10-ethics.html#risk-score-errors",
    "href": "slides/10-ethics.html#risk-score-errors",
    "title": "Data science ethics",
    "section": "Risk score errors",
    "text": "Risk score errors\n\n\n\nWhat is common among the defendants who were assigned a high/low risk score for reoffending?"
  },
  {
    "objectID": "slides/10-ethics.html#risk-scores",
    "href": "slides/10-ethics.html#risk-scores",
    "title": "Data science ethics",
    "section": "Risk scores",
    "text": "Risk scores\n\n\n\nHow can an algorithm that doesn’t use race as input data be racist?"
  },
  {
    "objectID": "slides/10-ethics.html#predicting-ethnicity",
    "href": "slides/10-ethics.html#predicting-ethnicity",
    "title": "Data science ethics",
    "section": "Predicting ethnicity",
    "text": "Predicting ethnicity\nImproving Ecological Inference by Predicting Individual Ethnicity from Voter Registration Record (Imran and Khan, 2016)\n\nIn both political behavior research and voting rights litigation, turnout and vote choice for different racial groups are often inferred using aggregate election results and racial composition. Over the past several decades, many statistical methods have been proposed to address this ecological inference problem. We propose an alternative method to reduce aggregation bias by predicting individual-level ethnicity from voter registration records. Building on the existing methodological literature, we use Bayes’s rule to combine the Census Bureau’s Surname List with various information from geocoded voter registration records. We evaluate the performance of the proposed methodology using approximately nine million voter registration records from Florida, where self-reported ethnicity is available. We find that it is possible to reduce the false positive rate among Black and Latino voters to 6% and 3%, respectively, while maintaining the true positive rate above 80%. Moreover, we use our predictions to estimate turnout by race and find that our estimates yields substantially less amounts of bias and root mean squared error than standard ecological inference estimates. We provide open-source software to implement the proposed methodology. The open-source software is available for implementing the proposed methodology."
  },
  {
    "objectID": "slides/10-ethics.html#wru-package-sorry-this-is-in-r",
    "href": "slides/10-ethics.html#wru-package-sorry-this-is-in-r",
    "title": "Data science ethics",
    "section": "wru package (sorry, this is in R)",
    "text": "wru package (sorry, this is in R)\nThe said “source software” is the wru package: https://github.com/kosukeimai/wru.\n\nDo you have any ethical concerns about installing this package?"
  },
  {
    "objectID": "slides/10-ethics.html#wru-package",
    "href": "slides/10-ethics.html#wru-package",
    "title": "Data science ethics",
    "section": "wru package",
    "text": "wru package\n\nWas the publication of this model ethical? Does the open-source nature of the code affect your answer? Is it ethical to use this software? Does your answer change depending on the intended use?\n\n\nlibrary(wru)\npredict_race(voter.file = voters, surname.only = TRUE) |&gt;\n  select(surname, contains(\"pred\"))\n\n      surname    pred.whi    pred.bla     pred.his    pred.asi    pred.oth\n1      Khanna 0.045110474 0.003067623 0.0068522723 0.860411906 0.084557725\n2        Imai 0.052645440 0.001334812 0.0558160072 0.719376581 0.170827160\n3      Rivera 0.043285692 0.008204605 0.9136195794 0.024316883 0.010573240\n4     Fifield 0.895405704 0.001911388 0.0337464844 0.011079323 0.057857101\n5        Zhou 0.006572555 0.001298962 0.0005388581 0.982365594 0.009224032\n6    Ratkovic 0.861236727 0.008212824 0.0095395642 0.011334635 0.109676251\n7     Johnson 0.543815322 0.344128607 0.0272403940 0.007405765 0.077409913\n8       Lopez 0.038939877 0.004920643 0.9318797791 0.012154125 0.012105576\n10 Wantchekon 0.330697188 0.194700665 0.4042849478 0.021379541 0.048937658\n9       Morse 0.866360147 0.044429853 0.0246568086 0.010219712 0.054333479"
  },
  {
    "objectID": "slides/10-ethics.html#wru-package-1",
    "href": "slides/10-ethics.html#wru-package-1",
    "title": "Data science ethics",
    "section": "wru package",
    "text": "wru package\n\nme &lt;- tibble(surname = \"Chism\")\n\npredict_race(voter.file = me, surname.only = TRUE)\n\nPredicting race for 2020\n\n\nWarning: Unknown or uninitialised column: `state`.\n\n\nProceeding with last name predictions...\n\n\nℹ All local files already up-to-date!\nℹ All local files already up-to-date!\n\n\n  surname pred.whi  pred.bla  pred.his    pred.asi   pred.oth\n1   Chism 0.579293 0.3224595 0.0271373 0.006381576 0.06472858\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/21-calculus-II.html#summary-of-derivative-rules",
    "href": "slides/21-calculus-II.html#summary-of-derivative-rules",
    "title": "Calculus II",
    "section": "Summary of Derivative Rules",
    "text": "Summary of Derivative Rules\nDifferentiation rules\n\n\nConstant rule: \\(\\frac{d}{dx} (c) = 0\\)\nPower rule: \\(\\frac{d}{dx} (x^n) = nx^{n-1}\\)\nConstant multiple rule: \\(\\frac{d}{dx} [c \\cdot f(x)] = c \\cdot f'(x)\\)\nSum rule: \\(\\frac{d}{dx} [f(x) + g(x)] = f'(x) + g'(x)\\)\nDifference rule: \\(\\frac{d}{dx} [f(x) - g(x)] = f'(x) - g'(x)\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration",
    "href": "slides/21-calculus-II.html#integration",
    "title": "Calculus II",
    "section": "Integration",
    "text": "Integration\nFrom last time:\n\n\nMeasures the accumulation of quantities and the area under a curve.\nExample: Used to compute the area under probability distribution functions, which is essential in statistics and data analysis.\nSymbol: \\(\\int f(x) dx\\)\nPractical Application: Calculating Cumulative Distribution Functions (CDFs)"
  },
  {
    "objectID": "slides/21-calculus-II.html#accumulating-quantities",
    "href": "slides/21-calculus-II.html#accumulating-quantities",
    "title": "Calculus II",
    "section": "Accumulating quantities",
    "text": "Accumulating quantities\n\n\n\n\n\n\n\n\nArea under the curve\n\n\nThe integral of a function represents the area under the curve of that function on a graph, between two points.\nExample: Finding the total distance traveled given a speed-time graph."
  },
  {
    "objectID": "slides/21-calculus-II.html#the-integral",
    "href": "slides/21-calculus-II.html#the-integral",
    "title": "Calculus II",
    "section": "The integral",
    "text": "The integral"
  },
  {
    "objectID": "slides/21-calculus-II.html#integrals-in-python",
    "href": "slides/21-calculus-II.html#integrals-in-python",
    "title": "Calculus II",
    "section": "Integrals in Python",
    "text": "Integrals in Python\nCalculating integrals using SymPy\n\nfrom sympy import symbols, integrate\n\nx = symbols('x')\nf = x**2 + 1\narea = integrate(f, (x, 0, 1))\nprint(area)  # Output: 4/3\n\n4/3"
  },
  {
    "objectID": "slides/21-calculus-II.html#solving-integrals",
    "href": "slides/21-calculus-II.html#solving-integrals",
    "title": "Calculus II",
    "section": "Solving integrals",
    "text": "Solving integrals\nIntegration rules\n\n\nConstant rule: \\(\\int c , dx = cx + C\\)\nPower rule: \\(\\int x^n , dx = \\frac{x^{n+1}}{n+1} + C\\)\nConstant multiple rule: \\(\\int c \\cdot f(x) , dx = c \\cdot \\int f(x) , dx\\)\nSum rule: \\(\\int [f(x) + g(x)] , dx = \\int f(x) , dx + \\int g(x) , dx\\)\nDifference rule: \\(\\int [f(x) - g(x)] , dx = \\int f(x) , dx - \\int g(x) , dx\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#example-1-integrating-a-constant",
    "href": "slides/21-calculus-II.html#example-1-integrating-a-constant",
    "title": "Calculus II",
    "section": "Example 1: Integrating a Constant",
    "text": "Example 1: Integrating a Constant\n\n\nFunction: \\(f(x) = 7\\)\nIntegral: \\(\\int 7 , dx = 7x + C\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#example-2-power-rule",
    "href": "slides/21-calculus-II.html#example-2-power-rule",
    "title": "Calculus II",
    "section": "Example 2: Power rule",
    "text": "Example 2: Power rule\n\n\nFunction: \\(f(x) = x^3\\)\nIntegral: \\(\\int x^3 , dx = \\frac{x^{4}}{4} + C\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#example-3-constant-multiple-rule",
    "href": "slides/21-calculus-II.html#example-3-constant-multiple-rule",
    "title": "Calculus II",
    "section": "Example 3: Constant multiple rule",
    "text": "Example 3: Constant multiple rule\n\n\nFunction: \\(f(x) = 5x^2\\)\nIntegral: \\(\\int 5x^2 , dx = 5 \\cdot \\frac{x^{3}}{3} + C = \\frac{5x^{3}}{3} + C\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#example-4-sum-and-difference-rule",
    "href": "slides/21-calculus-II.html#example-4-sum-and-difference-rule",
    "title": "Calculus II",
    "section": "Example 4: Sum and difference rule",
    "text": "Example 4: Sum and difference rule\n\n\nFunction: \\(f(x) = x^3 + 4x - 5\\)\nIntegral: \\(\\int (x^3 + 4x - 5) , dx = \\frac{x^{4}}{4} + 2x^2 - 5x + C\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#solving-complex-integrals",
    "href": "slides/21-calculus-II.html#solving-complex-integrals",
    "title": "Calculus II",
    "section": "Solving complex integrals",
    "text": "Solving complex integrals\nComplex Integrals:\n\n\nInvolves functions composed of multiple simpler functions.\nRequires application of rules like integration by parts and substitution for integration.\n\n\n\nExample Function:\n\\[\n\\int_{a}^{b} \\left( e^{cx} + \\frac{1}{x^n} \\right) \\, dx\n\\]\n\n\n\nObjective: Find the integral"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-parts",
    "href": "slides/21-calculus-II.html#integration-by-parts",
    "title": "Calculus II",
    "section": "Integration by parts",
    "text": "Integration by parts\n\\[\n\\int u \\space dv = uv - \\int v \\space du\n\\]\n\nUsed when integrating the product of two functions."
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-parts-1",
    "href": "slides/21-calculus-II.html#integration-by-parts-1",
    "title": "Calculus II",
    "section": "Integration by parts",
    "text": "Integration by parts\nFunction: \\(\\int x e^x , dx\\)\n\n\nIdentify the functions\n\n\n\n\\(u = x \\quad \\Rightarrow \\quad du = dx\\)\n\\(dv = e^x , dx \\quad \\Rightarrow \\quad v = e^x\\)\n\n\n\n\n\nApply Integration by Parts\n\n\\(\\int xe^x \\space dx=xe^x-\\int e^x \\space dx=xe^x-e^x + C\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-parts-example",
    "href": "slides/21-calculus-II.html#integration-by-parts-example",
    "title": "Calculus II",
    "section": "Integration by parts: Example",
    "text": "Integration by parts: Example\nFunction: \\(\\int xe^x \\space dx\\)\n\n\nIdentify the functions\n\n\n\n\\(\\text{Let } u=x\\) and \\(dv=e^x \\space dx\\)\n\n\n\n\n\nDifferentiate and integrate\n\n\n\nDifferentiate: \\(u\\colon du=dx\\)\nIntegrate: \\(dv\\colon v = e^x\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-parts-example-1",
    "href": "slides/21-calculus-II.html#integration-by-parts-example-1",
    "title": "Calculus II",
    "section": "Integration by parts: Example",
    "text": "Integration by parts: Example\nFunction: \\(\\int xe^x \\space dx\\)\n\n\nApply the integration by parts formula\n\n\\[\n\\int u \\space dv = uv - \\int v \\space du\n\\]\n\n\n\nSubstitute the values\n\n\\[\n\\int x e^x \\, dx = x e^x - \\int e^x \\, dx\n\\]"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-parts-example-2",
    "href": "slides/21-calculus-II.html#integration-by-parts-example-2",
    "title": "Calculus II",
    "section": "Integration by parts: Example",
    "text": "Integration by parts: Example\nFunction: \\(\\int xe^x \\space dx\\)\n\n\nSimplify the integral\n\n\\[\n\\int x e^x \\, dx = x e^x - e^x + C\n\\]\n\n\n\nFinal answer\n\n\\[\n\\int x e^x \\, dx = e^x (x - 1) + C\n\\]"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-substitution",
    "href": "slides/21-calculus-II.html#integration-by-substitution",
    "title": "Calculus II",
    "section": "Integration by substitution",
    "text": "Integration by substitution\nFunction: \\(\\int f(g(x))g^{'}(x)dx=\\int f(u)\\space du\\)\n\nUsed when integrating a composite function."
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-substitution-example-1",
    "href": "slides/21-calculus-II.html#integration-by-substitution-example-1",
    "title": "Calculus II",
    "section": "Integration by substitution: Example 1",
    "text": "Integration by substitution: Example 1\nFunction: \\(\\int 2x \\sqrt{x^2 + 1} , dx\\)\n\n\nIdentify the substitution\n\n\n\nLet \\(u = x^2 + 1 \\quad \\Rightarrow \\quad du = 2x , dx\\)\n\n\n\n\n\nApply the substitution\n\n\\[\n\\int 2x \\sqrt{x^2 + 1} \\space dx=\\int \\sqrt{u} \\space du = \\frac{2}{3}(x^2 + 1)^{3/2} + C\n\\]"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-substitution-example-2",
    "href": "slides/21-calculus-II.html#integration-by-substitution-example-2",
    "title": "Calculus II",
    "section": "Integration by substitution: Example 2",
    "text": "Integration by substitution: Example 2\nFunction: \\[\\int x \\ln(x) , dx\\]\n\n\nIdentify the functions\n\n\n\n\\(u = \\ln(x) \\quad \\Rightarrow \\quad du = \\frac{1}{x} , dx\\)\n\\(dv = x , dx \\quad \\Rightarrow \\quad v = \\frac{x^2}{2}\\)"
  },
  {
    "objectID": "slides/21-calculus-II.html#integration-by-substitution-example-2-1",
    "href": "slides/21-calculus-II.html#integration-by-substitution-example-2-1",
    "title": "Calculus II",
    "section": "Integration by substitution: Example 2",
    "text": "Integration by substitution: Example 2\nFunction: \\[\\int x \\ln(x) , dx\\]\n\n\nApply integration by parts\n\n\\[\n\\int x \\ln(x) , dx = \\frac{x^2}{2}\\ln(x) - \\int \\frac{x^2}{2} \\cdot \\frac{1}{x} \\space dx = \\frac{x^2}{2} \\ln(x) - \\frac{1}{2} \\int x \\space dx\n\\]\n\\[\n= \\frac{x^2}{2} \\ln(x) - \\frac{x^2}{4} + C\n\\]"
  },
  {
    "objectID": "slides/21-calculus-II.html#regularization",
    "href": "slides/21-calculus-II.html#regularization",
    "title": "Calculus II",
    "section": "Regularization",
    "text": "Regularization\nYou’ll learn more about this in INFO 521: Introduction to Machine Learning and/or INFO 523: Data Mining and Discovery"
  },
  {
    "objectID": "slides/21-calculus-II.html#ae-14-integration",
    "href": "slides/21-calculus-II.html#ae-14-integration",
    "title": "Calculus II",
    "section": "ae-14-integration",
    "text": "ae-14-integration\nPractice integration (you will be tested on this in Exam 2)\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#setup",
    "href": "slides/18-prediction-uncertainty.html#setup",
    "title": "Prediction + uncertainty",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\", rc={\"figure.figsize\": (8, 6), \"axes.labelsize\": 16, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#data-candy-rankings",
    "href": "slides/18-prediction-uncertainty.html#data-candy-rankings",
    "title": "Prediction + uncertainty",
    "section": "Data: Candy Rankings",
    "text": "Data: Candy Rankings\n\ncandy_rankings = pd.read_csv(\"data/candy_rankings.csv\")\n\ncandy_rankings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 85 entries, 0 to 84\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   competitorname    85 non-null     object \n 1   chocolate         85 non-null     bool   \n 2   fruity            85 non-null     bool   \n 3   caramel           85 non-null     bool   \n 4   peanutyalmondy    85 non-null     bool   \n 5   nougat            85 non-null     bool   \n 6   crispedricewafer  85 non-null     bool   \n 7   hard              85 non-null     bool   \n 8   bar               85 non-null     bool   \n 9   pluribus          85 non-null     bool   \n 10  sugarpercent      85 non-null     float64\n 11  pricepercent      85 non-null     float64\n 12  winpercent        85 non-null     float64\ndtypes: bool(9), float64(3), object(1)\nmemory usage: 3.5+ KB"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#full-model",
    "href": "slides/18-prediction-uncertainty.html#full-model",
    "title": "Prediction + uncertainty",
    "section": "Full model",
    "text": "Full model\n\nWhat percent of the variability in win percentages is explained by the model?\n\n\nfull_model = smf.ols('winpercent ~ chocolate + fruity + caramel + peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent', data=candy_rankings).fit()\nprint(f'R-squared: {full_model.rsquared.round(3)}')\nprint(f'Adjusted R-squared: {full_model.rsquared_adj.round(3)}')\n\nR-squared: 0.54\nAdjusted R-squared: 0.471"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#akaike-information-criterion",
    "href": "slides/18-prediction-uncertainty.html#akaike-information-criterion",
    "title": "Prediction + uncertainty",
    "section": "Akaike Information Criterion",
    "text": "Akaike Information Criterion\n\\[ AIC = -2log(L) + 2k \\]\n\n\n\\(L\\): likelihood of the model\n\nLikelihood of seeing these data given the estimated model parameters\nWon’t go into calculating it in this course (but you will in future courses)\n\nUsed for model selection, lower the better\n\nValue is not informative on its own\n\nApplies a penalty for number of parameters in the model, \\(k\\)\n\nDifferent penalty than adjusted \\(R^2\\) but similar idea\n\n\n\n\n\nprint(f'AIC: {full_model.aic}')\n\nAIC: 655.2701107463729"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#model-selection-a-little-faster",
    "href": "slides/18-prediction-uncertainty.html#model-selection-a-little-faster",
    "title": "Prediction + uncertainty",
    "section": "Model selection – a little faster",
    "text": "Model selection – a little faster\n\nselected_model = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=candy_rankings).fit()\n\n\nprint(selected_model.summary2())\n\n                     Results: Ordinary least squares\n=========================================================================\nModel:                  OLS                Adj. R-squared:       0.492   \nDependent Variable:     winpercent         AIC:                  647.5113\nDate:                   2024-08-19 13:45   BIC:                  664.6099\nNo. Observations:       85                 Log-Likelihood:       -316.76 \nDf Model:               6                  F-statistic:          14.54   \nDf Residuals:           78                 Prob (F-statistic):   4.62e-11\nR-squared:              0.528              Scale:                110.07  \n-------------------------------------------------------------------------\n                          Coef.  Std.Err.    t    P&gt;|t|   [0.025   0.975]\n-------------------------------------------------------------------------\nIntercept                32.9406   3.5175  9.3647 0.0000  25.9377 39.9434\nchocolate[T.True]        19.1470   3.5870  5.3379 0.0000  12.0059 26.2882\nfruity[T.True]            8.8815   3.5606  2.4944 0.0147   1.7929 15.9701\npeanutyalmondy[T.True]    9.4829   3.4464  2.7516 0.0074   2.6217 16.3440\ncrispedricewafer[T.True]  8.3851   4.4843  1.8699 0.0653  -0.5425 17.3127\nhard[T.True]             -5.6693   3.2889 -1.7238 0.0887 -12.2170  0.8784\nsugarpercent              7.9789   4.1289  1.9325 0.0569  -0.2410 16.1989\n-------------------------------------------------------------------------\nOmnibus:                 0.545           Durbin-Watson:             1.735\nProb(Omnibus):           0.761           Jarque-Bera (JB):          0.682\nSkew:                    -0.093          Prob(JB):                  0.711\nKurtosis:                2.602           Condition No.:             6    \n=========================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified."
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#selected-variables",
    "href": "slides/18-prediction-uncertainty.html#selected-variables",
    "title": "Prediction + uncertainty",
    "section": "Selected variables",
    "text": "Selected variables\n\n\n\nvariable\nselected\n\n\n\n\nchocolate\nx\n\n\nfruity\nx\n\n\ncaramel\n\n\n\npeanutyalmondy\nx\n\n\nnougat\n\n\n\ncrispedricewafer\nx\n\n\nhard\nx\n\n\nbar\n\n\n\npluribus\n\n\n\nsugarpercent\nx\n\n\npricepercent"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#coefficient-interpretation",
    "href": "slides/18-prediction-uncertainty.html#coefficient-interpretation",
    "title": "Prediction + uncertainty",
    "section": "Coefficient interpretation",
    "text": "Coefficient interpretation\n\nInterpret the slopes of chocolate and sugarpercent in context of the data.\n\n\n\nIntercept                   32.940573\nchocolate[T.True]           19.147029\nfruity[T.True]               8.881496\npeanutyalmondy[T.True]       9.482858\ncrispedricewafer[T.True]     8.385138\nhard[T.True]                -5.669297\nsugarpercent                 7.978930\ndtype: float64"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#aic",
    "href": "slides/18-prediction-uncertainty.html#aic",
    "title": "Prediction + uncertainty",
    "section": "AIC",
    "text": "AIC\nAs expected, the selected model has a smaller AIC than the full model. In fact, the selected model has the minimum AIC of all possible main effects models.\n\nprint(f'AIC: {full_model.aic}')\n\nAIC: 655.2701107463729\n\n\n\nprint(f'AIC: {selected_model.aic}')\n\nAIC: 647.5113366053722"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#parismony",
    "href": "slides/18-prediction-uncertainty.html#parismony",
    "title": "Prediction + uncertainty",
    "section": "Parismony",
    "text": "Parismony\n\n\n\nLook at the variables in the full and the selected model. Can you guess why some of them may have been dropped? Remember: We like parsimonious models.\n\n\n\n\n\nvariable\nselected\n\n\n\n\nchocolate\nx\n\n\nfruity\nx\n\n\ncaramel\n\n\n\npeanutyalmondy\nx\n\n\nnougat\n\n\n\ncrispedricewafer\nx\n\n\nhard\nx\n\n\nbar\n\n\n\npluribus\n\n\n\nsugarpercent\nx\n\n\npricepercent"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#new-observation",
    "href": "slides/18-prediction-uncertainty.html#new-observation",
    "title": "Prediction + uncertainty",
    "section": "New observation",
    "text": "New observation\nTo make a prediction for a new observation we need to create a data frame with that observation.\n\nSuppose we want to make a prediction for a candy that contains chocolate, isn’t fruity, has no peanuts or almonds, has a wafer, isn’t hard, and has a sugar content in the 20th percentile.\n\nThe following will result in an incorrect prediction. Why? How would you correct it?\n\n\nnew_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [20]})"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#new-observation-corrected",
    "href": "slides/18-prediction-uncertainty.html#new-observation-corrected",
    "title": "Prediction + uncertainty",
    "section": "New observation, corrected",
    "text": "New observation, corrected\n\nnew_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [0.20]})\nnew_candy\n\n\n\n\n\n\n\n\nchocolate\nfruity\npeanutyalmondy\ncrispedricewafer\nhard\nsugarpercent\n\n\n\n\n0\n1\n0\n0\n1\n0\n0.2"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#prediction-1",
    "href": "slides/18-prediction-uncertainty.html#prediction-1",
    "title": "Prediction + uncertainty",
    "section": "Prediction",
    "text": "Prediction\n\nprediction = selected_model.predict(new_candy)\nprint(f'Prediction: {prediction}')\n\nPrediction: 0    62.068526\ndtype: float64"
  },
  {
    "objectID": "slides/18-prediction-uncertainty.html#uncertainty-around-prediction",
    "href": "slides/18-prediction-uncertainty.html#uncertainty-around-prediction",
    "title": "Prediction + uncertainty",
    "section": "Uncertainty around prediction",
    "text": "Uncertainty around prediction\n\n\nConfidence interval around \\(\\hat{y}\\) for new data (average win percentage for candy types with the given characteristics):\n\n\nconfidence_interval = selected_model.get_prediction(new_candy).conf_int()\nprint(f'Confidence Interval: {confidence_interval}')\n\nConfidence Interval: [[53.65186346 70.48518939]]\n\n\n\n\n\nPrediction interval around \\(\\hat{y}\\) for new data (predicted score for an individual type of candy with the given characteristics ):\n\n\nprediction_interval = selected_model.get_prediction(new_candy).summary_frame(alpha=0.05)\nprint(f'Prediction Interval: {prediction_interval}')\n\nPrediction Interval:         mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n0  62.068526  4.227679      53.651863      70.485189     39.549428   \n\n   obs_ci_upper  \n0     84.587625  \n\n\n\n\n\n\n\n🔗 datasciaz.netlify.app"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered face to face, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! You can find a list of everyone’s office hours here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#slack",
    "href": "course-support.html#slack",
    "title": "Course support",
    "section": "Slack",
    "text": "Slack\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The course Slack is the best venue for these! There is a chance another student has already asked a similar question, so please check the other posts on Slack before asking a new question. If you know the answer to a question that is posted, I encourage you to respond! I rarely respond to Slack messages sent Friday evening - Sunday, nor would I expect you to.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go to Slack), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Shannon McWaters at smcwaters@arizona.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “INFO 511” in the subject line. Barring extenuating circumstances, I will respond to INFO 511 emails within 48 hours Monday - Friday. I do not respond to emails sent Friday evening - Sunday, nor would I expect you to.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#how-to-ask-for-help",
    "href": "course-support.html#how-to-ask-for-help",
    "title": "Course support",
    "section": "How to Ask For Help",
    "text": "How to Ask For Help\nWe’ll see in this course that a key skill that you should develop as a data scientist is the ability to find solutions to problems. Knowing how to get help is part of that skill.\n\nBefore you ask for help:\n\nCheck for typos. One of the most common causes of errors are typos, which usually throw an error such as NameError: name '_____ ' is not defined due to a variable or function being misspelled.\nCheck imported modules. You also get errors like AttributeError: module '_____ ' has no attribute '_____ ' when you fail to import a module or incorrectly import it.\nRead the error message. Don’t ignore what Python is telling you. Be aware that red text that appears in your console is not always an indication of errors. Sometimes it’s just a warning or a message.\nGoogle is your friend. Copy and paste the exact error message on a Google search. (this step also includes read the documentation on the package you’re trying to use).\nIf you are still stuck, you an always try rubber duck debugging. Describe the problem aloud, explaining it line-by-line, to a rubber duck or another person (who might not have any experience with programming of data science). This is also a good preparation step to asking other people for help (next section).\n\n\n\nAsk other people for help\nLike mentioned before, you should ask your peers for help before you ask your instructor. Relying on a single person to solve all of your problems is dangerous, because that person won’t be available throughout your career as a data scientist.\n\nCheck our Slack to see if someone else has asked a question similar to yours, and whether there’s a solution posted for it.\nBe precise and informative. The more context you can provide about what you’re trying to do and what errors you’re getting, the better. Also describe the steps you took to try to solve the problem yourself.\n\n\n\nList of resources\n\nGetting Help with Python\nRoger Peng’s How To Get Help video\nRubber Duck Debugging",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Student Success & Retention Innovation (SOS) offers free services to all students during their graduate careers at UArizona. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at UArizona.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness are of primary importance at UArizona, and the university offers resources to support students in managing daily stress and self-care. Campus Health offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below. For medical appointments, call (520) 621-9202. For After Hours care, call (520) 570-7898. For the Counseling & Psych Services (CAPS) 24/7 hotline, call (520) 621-3334\n\nAcademic advising: If you have questions about your academic progress this semester, or your chosen degree program, consider contacting your department’s academic advisor(s).  Your academic advisor and the Advising Resource Center can guide you toward university resources to help you succeed.\nSpiritual Wellness: CAPS: (520) 621-3334, provides Moments of Mindfulness (stress management and resilience building) and meditation programming to assist students in developing a daily emotional well-being practice. To see schedules for programs please see https://caps.arizona.edu/spiritual-wellness. All are welcome and no experience necessary.\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. UArizona encourages all students to access these resources.\n\nStudent Supporter Hub: Provides comprehensive services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance: https://caps.arizona.edu/student-supporter.\nCounseling & Psychological Services (CAPS): CAPS services include individual and group counseling services, psychiatric services, and workshops. To initiate services, walk-in/call-in 8am-4:30pm M,Tu,Th,F and 9am-4:30pm Wednesdays; (520) 621-3334. CAPS also provides referral to off-campus resources for specialized care: https://caps.arizona.edu/caps-student-support-outreach-request.\nCrisis Support: After-hours crisis callers may speak with a licensed counselor by pressing 1 when prompted by the automatic message: https://caps.arizona.edu/crisis-resources-hotlines.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-costs",
    "href": "course-support.html#course-costs",
    "title": "Course support",
    "section": "Course costs",
    "text": "Course costs\n\nTextbooks: The textbooks for this course are freely available on the web.\nLaptops: Each student is expected to have a laptop they can bring to each lecture and lab.\n\nIf you are having difficulty with costs associated with this course, here are some resources:\n\nContact the financial aid office (whether or not you are on aid). They have loans and resources for connecting students with programs on campus that might be able to help alleviate these costs.\nFor course-specific technology needs such as Digital Voice Recorder, HD Video Camera, TI-84 Plus CE, DSLR camera kit, Tripod, Microphones, iPad Mini, a Handheld Projector, or a GoPro, you can reserve rental equipment from the Link.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-d2l",
    "href": "course-support.html#assistance-with-zoom-or-d2l",
    "title": "Course support",
    "section": "Assistance with Zoom or D2L",
    "text": "Assistance with Zoom or D2L\nFor technical help with D2L or Zoom, contact the UArizona OIT Service Desk at it.arizona.edu/get-support. You can also access the self-service help documentation for Zoom here and for D2L here.\nNote that we will be making minimal use of D2L in this course (primarily for announcements and grade book). All assignment submission will take place on GitHub and conversation on Slack.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "computing/computing-python.html",
    "href": "computing/computing-python.html",
    "title": "Setting up Python",
    "section": "",
    "text": "The steps to installing Python are seemingly straightforward but can result in an array of different bugs. For INFO 511, I’ve chosen a workflow that will standardize your installations and (hopefully) prevent these bugs.",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#introduction",
    "href": "computing/computing-python.html#introduction",
    "title": "Setting up Python",
    "section": "",
    "text": "The steps to installing Python are seemingly straightforward but can result in an array of different bugs. For INFO 511, I’ve chosen a workflow that will standardize your installations and (hopefully) prevent these bugs.",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#installing-vs-code",
    "href": "computing/computing-python.html#installing-vs-code",
    "title": "Setting up Python",
    "section": "Installing VS Code",
    "text": "Installing VS Code\n\nFor all OS:\n\nSee the following link: https://code.visualstudio.com/download\n\n\n\n\n\n\n\nRun through your device’s appropriate installation.",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#installing-bash",
    "href": "computing/computing-python.html#installing-bash",
    "title": "Setting up Python",
    "section": "Installing Bash",
    "text": "Installing Bash\n\nFor PC\n\nOpen Windows PowerShell as Administrator\nType wsl –install and press Enter\nRestart your computer\nOpen the Microsoft Store.\nInstall a Linux distribution (e.g., Ubuntu).\n\nSearch for “Ubuntu” (or any other preferred distribution) and click “Install”.\n\nSet up your Linux environment.\n\nOnce installed, open the Linux distribution from the Start menu and follow the setup instructions.\n\n\n\n\nFor Mac:\n\nYou already have zsh, which is a Bash Shell 😊",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#open-a-bash-shell-terminal",
    "href": "computing/computing-python.html#open-a-bash-shell-terminal",
    "title": "Setting up Python",
    "section": "Open a Bash Shell terminal",
    "text": "Open a Bash Shell terminal\n\nFor PC:\n\nPress CTRL + ALT + T simultaneously to open the terminal.\nType bash then press Enter\nYou should have a Bash shell ready.\n\n\n\nFor Mac:\nDo one of the following:\n\nClick the Launchpad icon  in the Dock, type Terminal in the search field, then click Terminal.\nIn the Finder , open the /Applications/Utilities folder, then double-click Terminal.",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#install-homebrew",
    "href": "computing/computing-python.html#install-homebrew",
    "title": "Setting up Python",
    "section": "Install Homebrew",
    "text": "Install Homebrew\n\nFor Mac and PC:\n\nHomebrew is a package manager for macOS and Linux that makes it easy to install software. In open Bash Terminal, type:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nRun through any installation steps found within the terminal.",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#install-miniconda-via-homebrew",
    "href": "computing/computing-python.html#install-miniconda-via-homebrew",
    "title": "Setting up Python",
    "section": "Install Miniconda (via Homebrew)",
    "text": "Install Miniconda (via Homebrew)\n\nFor both Mac and PC\n\nType the install command into the open terminal:\n\nbrew install --cask miniconda\n\nFun the following to setup your shell:\n\nconda init \"$(basename \"${SHELL}\")\"",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#install-python3",
    "href": "computing/computing-python.html#install-python3",
    "title": "Setting up Python",
    "section": "Install Python3",
    "text": "Install Python3\n\nFor Mac and PC:\n\nRun the following in your terminal (check the latest Python3 version):\n\nconda create -y -n py38 python=3.12.4 pip\n\n\nFor PC (Bash)\n\nRun the following in your terminal (this step may not work given permissions):\n\necho \"conda activate py38\" &gt;&gt; ~/.bashrc\n\n\nFor Mac (Zsh)\n\nRun the following in your terminal (this step may not work given permissions):\n\necho \"conda activate py38\" &gt;&gt; ~/.zshrc\n\n\nFor Mac and PC:\n\nRun the following in your terminal:\n\nconda activate py38\npip install -U pip\npip install pytest\npip cache purge\nconda clean -y -av\npython -V\npip -V\n\nRestart your computer + Open VSCode",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#prepare-vs-code-for-python",
    "href": "computing/computing-python.html#prepare-vs-code-for-python",
    "title": "Setting up Python",
    "section": "Prepare VS Code for Python",
    "text": "Prepare VS Code for Python\n\nOnce VS Code is open, click on the Explorer:\n\n\n\n\n\n\n\nClick “Clone Repository”:\n\n\n\n\n\n\n\nPaste the following into the text input that pops up in the top-middle: https://github.com/INFO-511-F24/ae-00-unvotes.git, which can be found at the ae-00-unvotes exercise GitHub page:\n\n\n\n\n\n\n\nClick “Open”:\n\n\n\n\n\n\n\nSelect the unvotes.ipynb file:\n\n\n\n\n\n\n\nClick the “Select Kernel” button in the top-right above the Jupyter Notebook:\n\n\n\n\n\n\n\nSelect “Python Environments” from the middle-center drop-down options:\n\n\n\n\n\n\n\nFrom the options, select the one with “Homebrew” or “Miniconda” within in (this may look different for you than me:",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#install-python-packages",
    "href": "computing/computing-python.html#install-python-packages",
    "title": "Setting up Python",
    "section": "Install Python Packages",
    "text": "Install Python Packages\n\nType the following into your terminal:\n\npip install numpy pandas matplotlib seaborn scikit-learn scipy statsmodels nltk jupyter notebook jupyterlab ipython plotly xlrd openpyxl requests beautifulsoup4 lxml",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-python.html#move-onto-setting-up-git",
    "href": "computing/computing-python.html#move-onto-setting-up-git",
    "title": "Setting up Python",
    "section": "Move onto setting up Git",
    "text": "Move onto setting up Git",
    "crumbs": [
      "Computing",
      "Setting up Python"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "Python cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from tsinghua-gongjing.github.io/posts/cheatsheet_collections. We will not covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "computing/computing-troubleshooting.html",
    "href": "computing/computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an JupyterLab, go to status.jupyter.org and find the Notebook Viewer dropdown section at the top.\n\nIf the status shows something other than Operational, this means there is a known incident with the cloud. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course Slack to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).",
    "crumbs": [
      "Computing",
      "Troubleshooting"
    ]
  }
]