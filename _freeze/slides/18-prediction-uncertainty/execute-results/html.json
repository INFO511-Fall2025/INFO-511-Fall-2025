{
  "hash": "5687d424014b88525ee66ffb67e65bb5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Prediction + uncertainty\nsubtitle: Lecture 18\nformat:\n  revealjs: default\neditor_options:\n  chunk_output_type: console\nexecute:\n  warning: false\n  error: false\n---\n\n## Setup {.smaller}\n\n::: {#1c4a0c5e .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\", rc={\"figure.figsize\": (8, 6), \"axes.labelsize\": 16, \"xtick.labelsize\": 14, \"ytick.labelsize\": 14})\n```\n:::\n\n\n# Model selection\n\n## Data: Candy Rankings {.smaller}\n\n::: {#e66f1eef .cell execution_count=2}\n``` {.python .cell-code}\ncandy_rankings = pd.read_csv(\"data/candy_rankings.csv\")\n\ncandy_rankings.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 85 entries, 0 to 84\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   competitorname    85 non-null     object \n 1   chocolate         85 non-null     bool   \n 2   fruity            85 non-null     bool   \n 3   caramel           85 non-null     bool   \n 4   peanutyalmondy    85 non-null     bool   \n 5   nougat            85 non-null     bool   \n 6   crispedricewafer  85 non-null     bool   \n 7   hard              85 non-null     bool   \n 8   bar               85 non-null     bool   \n 9   pluribus          85 non-null     bool   \n 10  sugarpercent      85 non-null     float64\n 11  pricepercent      85 non-null     float64\n 12  winpercent        85 non-null     float64\ndtypes: bool(9), float64(3), object(1)\nmemory usage: 3.5+ KB\n```\n:::\n:::\n\n\n## Full model {.smaller}\n\n::: question\nWhat percent of the variability in win percentages is explained by the model?\n:::\n\n::: {#a741f617 .cell execution_count=3}\n``` {.python .cell-code}\nfull_model = smf.ols('winpercent ~ chocolate + fruity + caramel + peanutyalmondy + nougat + crispedricewafer + hard + bar + pluribus + sugarpercent + pricepercent', data=candy_rankings).fit()\nprint(f'R-squared: {full_model.rsquared.round(3)}')\nprint(f'Adjusted R-squared: {full_model.rsquared_adj.round(3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared: 0.54\nAdjusted R-squared: 0.471\n```\n:::\n:::\n\n\n## Akaike Information Criterion {.smaller}\n\n$$ AIC = -2log(L) + 2k $$\n\n::: incremental\n-   $L$: likelihood of the model\n    -   Likelihood of seeing these data given the estimated model parameters\n    -   Won't go into calculating it in this course (but you will in future courses)\n-   Used for model selection, lower the better\n    -   Value is not informative on its own\n-   Applies a penalty for number of parameters in the model, $k$\n    -   Different penalty than adjusted $R^2$ but similar idea\n:::\n\n::: fragment\n\n::: {#aic-full-model .cell execution_count=4}\n``` {.python .cell-code}\nprint(f'AIC: {full_model.aic}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAIC: 655.2701107463729\n```\n:::\n:::\n\n\n:::\n\n## Model selection -- a little faster {.smaller}\n\n::: {#518a1b58 .cell execution_count=5}\n``` {.python .cell-code}\nselected_model = smf.ols('winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent', data=candy_rankings).fit()\n```\n:::\n\n\n::: {#e9b432bd .cell execution_count=6}\n``` {.python .cell-code}\nprint(selected_model.summary2())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     Results: Ordinary least squares\n=========================================================================\nModel:                  OLS                Adj. R-squared:       0.492   \nDependent Variable:     winpercent         AIC:                  647.5113\nDate:                   2024-08-19 13:45   BIC:                  664.6099\nNo. Observations:       85                 Log-Likelihood:       -316.76 \nDf Model:               6                  F-statistic:          14.54   \nDf Residuals:           78                 Prob (F-statistic):   4.62e-11\nR-squared:              0.528              Scale:                110.07  \n-------------------------------------------------------------------------\n                          Coef.  Std.Err.    t    P>|t|   [0.025   0.975]\n-------------------------------------------------------------------------\nIntercept                32.9406   3.5175  9.3647 0.0000  25.9377 39.9434\nchocolate[T.True]        19.1470   3.5870  5.3379 0.0000  12.0059 26.2882\nfruity[T.True]            8.8815   3.5606  2.4944 0.0147   1.7929 15.9701\npeanutyalmondy[T.True]    9.4829   3.4464  2.7516 0.0074   2.6217 16.3440\ncrispedricewafer[T.True]  8.3851   4.4843  1.8699 0.0653  -0.5425 17.3127\nhard[T.True]             -5.6693   3.2889 -1.7238 0.0887 -12.2170  0.8784\nsugarpercent              7.9789   4.1289  1.9325 0.0569  -0.2410 16.1989\n-------------------------------------------------------------------------\nOmnibus:                 0.545           Durbin-Watson:             1.735\nProb(Omnibus):           0.761           Jarque-Bera (JB):          0.682\nSkew:                    -0.093          Prob(JB):                  0.711\nKurtosis:                2.602           Condition No.:             6    \n=========================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```\n:::\n:::\n\n\n## Selected variables {.smaller}\n\n| variable         | selected |\n|------------------|:--------:|\n| chocolate        |    x     |\n| fruity           |    x     |\n| caramel          |          |\n| peanutyalmondy   |    x     |\n| nougat           |          |\n| crispedricewafer |    x     |\n| hard             |    x     |\n| bar              |          |\n| pluribus         |          |\n| sugarpercent     |    x     |\n| pricepercent     |          |\n\n------------------------------------------------------------------------\n\n## Coefficient interpretation {.smaller}\n\n::: question\nInterpret the slopes of `chocolate` and `sugarpercent` in context of the data.\n:::\n\n::: {#689af2aa .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept                   32.940573\nchocolate[T.True]           19.147029\nfruity[T.True]               8.881496\npeanutyalmondy[T.True]       9.482858\ncrispedricewafer[T.True]     8.385138\nhard[T.True]                -5.669297\nsugarpercent                 7.978930\ndtype: float64\n```\n:::\n:::\n\n\n## AIC {.smaller}\n\nAs expected, the selected model has a smaller AIC than the full model.\nIn fact, the selected model has the minimum AIC of all possible main effects models.\n\n::: {#917f0ecd .cell execution_count=8}\n``` {.python .cell-code}\nprint(f'AIC: {full_model.aic}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAIC: 655.2701107463729\n```\n:::\n:::\n\n\n::: {#c2f5c80e .cell execution_count=9}\n``` {.python .cell-code}\nprint(f'AIC: {selected_model.aic}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAIC: 647.5113366053722\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Parismony {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n::: question\nLook at the variables in the full and the selected model.\nCan you guess why some of them may have been dropped?\nRemember: We like parsimonious models.\n:::\n:::\n\n::: {.column width=\"50%\"}\n| variable         | selected |\n|------------------|:--------:|\n| chocolate        |    x     |\n| fruity           |    x     |\n| caramel          |          |\n| peanutyalmondy   |    x     |\n| nougat           |          |\n| crispedricewafer |    x     |\n| hard             |    x     |\n| bar              |          |\n| pluribus         |          |\n| sugarpercent     |    x     |\n| pricepercent     |          |\n:::\n:::\n\n# Prediction\n\n## New observation {.smaller}\n\nTo make a prediction for a new observation we need to create a data frame with that observation.\n\n::: question\nSuppose we want to make a prediction for a candy that contains chocolate, isn't fruity, has no peanuts or almonds, has a wafer, isn't hard, and has a sugar content in the 20th percentile.\n\n<br>\n\nThe following will result in an incorrect prediction.\nWhy?\nHow would you correct it?\n:::\n\n::: {#44c9c0f7 .cell execution_count=10}\n``` {.python .cell-code}\nnew_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [20]})\n```\n:::\n\n\n## New observation, corrected {.smaller}\n\n::: {#57ac37e2 .cell execution_count=11}\n``` {.python .cell-code}\nnew_candy = pd.DataFrame({'chocolate': [1], 'fruity': [0], 'peanutyalmondy': [0], 'crispedricewafer': [1], 'hard': [0], 'sugarpercent': [0.20]})\nnew_candy\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chocolate</th>\n      <th>fruity</th>\n      <th>peanutyalmondy</th>\n      <th>crispedricewafer</th>\n      <th>hard</th>\n      <th>sugarpercent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Prediction {.smaller}\n\n::: {#04096cb2 .cell execution_count=12}\n``` {.python .cell-code}\nprediction = selected_model.predict(new_candy)\nprint(f'Prediction: {prediction}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction: 0    62.068526\ndtype: float64\n```\n:::\n:::\n\n\n## Uncertainty around prediction {.smaller}\n\n::: fragment\n-   Confidence interval around $\\hat{y}$ for new data (average win percentage for candy types with the given characteristics):\n\n::: {#a962399a .cell execution_count=13}\n``` {.python .cell-code}\nconfidence_interval = selected_model.get_prediction(new_candy).conf_int()\nprint(f'Confidence Interval: {confidence_interval}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfidence Interval: [[53.65186346 70.48518939]]\n```\n:::\n:::\n\n\n:::\n\n::: fragment\n-   Prediction interval around $\\hat{y}$ for new data (predicted score for an individual type of candy with the given characteristics ):\n\n::: {#4dd1a312 .cell execution_count=14}\n``` {.python .cell-code}\nprediction_interval = selected_model.get_prediction(new_candy).summary_frame(alpha=0.05)\nprint(f'Prediction Interval: {prediction_interval}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction Interval:         mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n0  62.068526  4.227679      53.651863      70.485189     39.549428   \n\n   obs_ci_upper  \n0     84.587625  \n```\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "18-prediction-uncertainty_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}