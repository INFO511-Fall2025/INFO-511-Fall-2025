{
  "hash": "f80a215b0f192588b305d80af2a6bf21",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Sampling distributions + inference\nsubtitle: Lecture 14\nformat:\n  revealjs: default\neditor_options:\n  chunk_output_type: console\nexecute:\n  warning: false\n  error: false\n---\n\n## Setup\n\n::: {#e96b5e5f .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats \nimport seaborn as sns\n\nnp.random.seed(1)\n```\n:::\n\n\n# Sample Statistics and Sampling Distributions\n\n## Variability of sample statistics\n\n::: incremental\n-   We've seen that each sample from the population yields a slightly different sample statistic (sample mean, sample proportion, etc.)\n\n-   Previously we've quantified this value via simulation\n\n-   Today we talk about some of the theory underlying **sampling distributions**, particularly as they relate to sample means.\n:::\n\n------------------------------------------------------------------------\n\n## Statistical inference\n\n::: incremental\n-   Statistical inference is the act of generalizing from a sample in order to make conclusions regarding a population.\n\n-   We are interested in population parameters, which we do not observe.\n    Instead, we must calculate statistics from our sample in order to learn about them.\n\n-   As part of this process, we must quantify the degree of uncertainty in our sample statistic.\n:::\n\n------------------------------------------------------------------------\n\n## Sampling distribution of the mean {.smaller}\n\nSuppose weâ€™re interested in the mean resting heart rate of students at U of A, and are able to do the following:\n\n::: incremental\n1.  Take a random sample of size $n$ from this population, and calculate the mean resting heart rate in this sample, $\\bar{X}_1$\n\n2.  Put the sample back, take a second random sample of size $n$, and calculate the mean resting heart rate from this new sample, $\\bar{X}_2$\n\n3.  Put the sample back, take a third random sample of size $n$, and calculate the mean resting heart rate from this sample, too...\n:::\n\n::: fragment\n...and so on.\n:::\n\n## Sampling distribution of the mean {.smaller}\n\nAfter repeating this many times, we have a data set that has the sample means from the population: $\\bar{X}_1$, $\\bar{X}_2$, $\\cdots$, $\\bar{X}_K$ (assuming we took $K$ total samples).\n\n::: fragment\n::: question\nCan we say anything about the distribution of these sample means (that is, the **sampling distribution** of the mean?)\n:::\n:::\n\n::: fragment\n*(Keep in mind, we don't know what the underlying distribution of mean resting heart rate of U of A students looks like!)*\n:::\n\n# Central Limit Theorem {style=\"text-align: center;\"}\n\n::: {.fragment .large}\nðŸ˜±\n:::\n\n## The Central Limit Theorem\n\nA quick caveat...\n\n::: fragment\nFor now, let's assume we know the underlying standard deviation, $\\sigma$, from our distribution\n:::\n\n## The Central Limit Theorem {.smaller}\n\nFor a population with a well-defined mean $\\mu$ and standard deviation $\\sigma$, these three properties hold for the distribution of sample mean $\\bar{X}$, assuming certain conditions hold:\n\n::: incremental\n1.  The mean of the sampling distribution of the mean is identical to the population mean $\\mu$.\n\n2.  The standard deviation of the distribution of the sample means is $\\sigma/\\sqrt{n}$.\n\n-   This is called the **standard error (SE)** of the mean.\n\n3.  For $n$ large enough, the shape of the sampling distribution of means is approximately **normally distributed**.\n:::\n\n## The normal (Gaussian) distribution {.smaller}\n\nThe normal distribution is unimodal and symmetric and is described by its **density function**:\n\n::: fragment\nIf a random variable $X$ follows the normal distribution, then $$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2} \\right\\}$$ where $\\mu$ is the mean and $\\sigma^2$ is the variance $(\\sigma \\text{ is the standard deviation})$\n:::\n\n::: fragment\n::: callout-warning\nWe often write $N(\\mu, \\sigma)$ to describe this distribution.\n:::\n:::\n\n## The normal distribution (graphically)\n\n::: {#ed5998e1 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-3-output-1.png){width=821 height=485}\n:::\n:::\n\n\n## Wait, *any* distribution? {.smaller}\n\nThe central limit theorem tells us that **sample means** are normally distributed, if we have enough data and certain assumptions hold.\n\n::: fragment\nThis is true *even if our original variables are not normally distributed*.\n:::\n\n::: fragment\nClick [here](http://onlinestatbook.com/stat_sim/sampling_dist/index.html) to see an interactive demonstration of this idea.\n:::\n\n## Conditions for CLT {.smaller}\n\nWe need to check two conditions for CLT to hold: independence, sample size/distribution.\n\n::: fragment\nâœ… **Independence:** The sampled observations must be independent.\nThis is difficult to check, but the following are useful guidelines:\n\n::: incremental\n-   the sample must be randomly taken\n\n-   if sampling without replacement, sample size must be less than 10% of the population size\n:::\n:::\n\n::: fragment\nIf samples are independent, then by definition one sample's value does not \"influence\" another sample's value.\n:::\n\n## Conditions for CLT {.smaller}\n\nâœ… **Sample size / distribution:**\n\n::: incremental\n-   if data are numerical, usually n \\> 30 is considered a large enough sample for the CLT to apply\n\n-   if we know for sure that the underlying data are normally distributed, then the distribution of sample means will also be exactly normal, regardless of the sample size\n\n-   if data are categorical, at least 10 successes and 10 failures.\n:::\n\n# Let's run our own simulation\n\n## Underlying population (not observed in real life!) {.small}\n\n::: {#9e315160 .cell execution_count=3}\n``` {.python .cell-code}\nrs_pop = pd.DataFrame({'x': np.random.beta(a=1, b=5, size=100000) * 100})\n```\n:::\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {#18f02b7f .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-5-output-1.png){width=627 height=505}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**The true population parameters**\n\n::: {#2c54ff4d .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\n(16.6681176684249, 14.070251294281247)\n```\n:::\n:::\n\n\n:::\n:::\n\n## Sampling from the population - 1 {.smaller}\n\n::: {#2dd169ac .cell execution_count=6}\n``` {.python .cell-code}\nnp.random.seed(1)\nsamp_1 = rs_pop.sample(n=50)\nsamp_1_mean = samp_1['x'].mean()\n```\n:::\n\n\n<br>\n\n::: {#00ada2ea .cell execution_count=7}\n``` {.python .cell-code}\nsamp_1_mean\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\n15.459323508748778\n```\n:::\n:::\n\n\n## Sampling from the population - 2 {.smaller}\n\n::: {#b980a830 .cell execution_count=8}\n``` {.python .cell-code}\nnp.random.seed(2)\nsamp_2 = rs_pop.sample(n=50)\nsamp_2_mean = samp_2['x'].mean()\n```\n:::\n\n\n<br>\n\n::: {#677601a1 .cell execution_count=9}\n``` {.python .cell-code}\nsamp_2_mean\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\n16.54881766128449\n```\n:::\n:::\n\n\n## Sampling from the population - 3 {.smaller}\n\n::: {#ab8f64a0 .cell execution_count=10}\n``` {.python .cell-code}\nnp.random.seed(3)\nsamp_3 = rs_pop.sample(n=50)\nsamp_3_mean = samp_3['x'].mean()\n```\n:::\n\n\n<br>\n\n::: {#873692b5 .cell execution_count=11}\n``` {.python .cell-code}\nsamp_3_mean\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\n18.232335628488613\n```\n:::\n:::\n\n\n::: fragment\nkeep repeating...\n:::\n\n## Sampling distribution {.small}\n\n::: {#27936f3a .cell execution_count=12}\n``` {.python .cell-code}\nnp.random.seed(92620)\nsampling_means = [rs_pop.sample(n=50, replace=True)['x'].mean() for _ in range(5000)]\n\nsampling_df = pd.DataFrame({'xbar': sampling_means})\n```\n:::\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {#54e62e9c .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-14-output-1.png){width=614 height=523}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**The sample statistics**\n\n::: {#4c720a6c .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\n(16.71372258467488, 1.9996839510659443)\n```\n:::\n:::\n\n\n:::\n:::\n\n##  {.smaller}\n\n::: question\nCompare the shapes, centers, and spreads of these distributions:\n:::\n\n::: columns\n::: {.column width=\"50%\"}\n**The true population**\n\n::: {#6e7999a9 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-16-output-1.png){width=374 height=374}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**The sample statistics**\n\n::: {#74ad4ee3 .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-17-output-1.png){width=374 height=374}\n:::\n:::\n\n\n:::\n:::\n\n## Recap {.smaller}\n\n::: incremental\n-   If certain assumptions are satisfied, regardless of the shape of the population distribution, the sampling distribution of the mean follows an approximately normal distribution.\n\n-   The center of the sampling distribution is at the center of the population distribution.\n\n-   The sampling distribution is less variable than the population distribution (and we can quantify by how much).\n:::\n\n::: fragment\n::: question\nWhat is the standard error, and how are the standard error and sample size related?\nWhat does that say about how the spread of the sampling distribution changes as $n$ increases?\n:::\n:::\n\n## Why do we care? {.smaller}\n\nKnowing the distribution of the sample statistic $\\bar{X}$ can help us\n\n::: incremental\n-   Estimate a population parameter as **point estimate** $\\boldsymbol{\\pm}$ **margin of error**\n\n    The **margin of error** is comprised of a measure of how confident we want to be and how variable the sample statistic is\n\n<br>\n\n-   Test for a population parameter by evaluating how likely it is to obtain to observed sample statistic when assuming that the null hypothesis is true\n    -   This probability will depend on how variable the sampling distribution is\n:::\n\n# Inference based on the CLT\n\n## Inference based on the CLT {.smaller}\n\nIf necessary conditions are met, we can also use inference methods based on the CLT.\nSuppose we know the true population standard deviation, $\\sigma$.\n\n::: fragment\nThen the CLT tells us that $\\bar{X}$ approximately has the distribution $N\\left(\\mu, \\sigma/\\sqrt{n}\\right)$.\n\nThat is,\n\n$$Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)$$\n:::\n\n# What if $\\sigma$ isn't known?\n\n## T distribution\n\nIn practice, we never know the true value of $\\sigma$, and so we estimate it from our data with $s$.\n\nWe can make the following test statistic for testing a single sample's population mean, which has a **t-distribution with n-1 degrees of freedom**:\n\n::: fragment\n::: question\n$$ T = \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}$$\n:::\n:::\n\n## T distribution {.smaller}\n\n::: incremental\n-   The t-distribution is also unimodal and symmetric, and is centered at 0\n\n-   It has thicker tails than the normal distribution\n\n    -   This is to make up for additional variability introduced by using $s$ instead of $\\sigma$ in calculation of the SE\n\n-   It is defined by the degrees of freedom\n:::\n\n## T vs Z distributions\n\n::: {#50b8d46a .cell execution_count=17}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-18-output-1.png){width=852 height=523}\n:::\n:::\n\n\n## T distribution {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n**Finding probabilities under the t curve:**\n\n::: {#9921683f .cell execution_count=18}\n``` {.python .cell-code}\n# P(t < -1.96) for df = 9\np_less = stats.t.cdf(-1.96, df=9)\np_less\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\n0.04082220273020832\n```\n:::\n:::\n\n\n<br>\n\n::: {#9e40cde7 .cell execution_count=19}\n``` {.python .cell-code}\n# P(t > -1.96) for df = 9\np_greater = stats.t.sf(-1.96, df=9)\np_greater\n```\n\n::: {.cell-output .cell-output-display execution_count=74}\n```\n0.9591777972697917\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n**Finding cutoff values under the t curve:**\n\n::: {#530a0f6a .cell execution_count=20}\n``` {.python .cell-code}\n# Find Q1 (25th percentile) for df = 9\nq1 = stats.t.ppf(0.25, df=9)\nq1\n```\n\n::: {.cell-output .cell-output-display execution_count=75}\n```\n-0.7027221467513188\n```\n:::\n:::\n\n\n<br>\n\n::: {#56db53bc .cell execution_count=21}\n``` {.python .cell-code}\n# Find Q3 (75th percentile) for df = 9\nq3 = stats.t.ppf(0.75, df=9)\nq3\n```\n\n::: {.cell-output .cell-output-display execution_count=76}\n```\n0.7027221467513188\n```\n:::\n:::\n\n\n:::\n:::\n\n## Confidence interval for a mean {.smaller}\n\n::: callout-warning\n**General form of the confidence interval**\n\n$$point~estimate \\pm critical~value \\times SE$$\n:::\n\n::: fragment\n::: callout-warning\n**Confidence interval for the mean**\n\n$$\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}$$\n:::\n:::\n\n## Durham NC Resident Satisfaction {.smaller}\n\n`durham_survey` contains resident responses to a survey given by the City of Durham in 2018.\nThese are a randomly selected, representative sample of Durham residents.\n\nQuestions were rated 1 - 5, with 1 being \"highly dissatisfied\" and 5 being \"highly satisfied.\"\n\n## Exploratory Data Analysis\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {#9930bfa3 .cell execution_count=22}\n``` {.python .cell-code}\ndurham = pd.read_csv(\"data/durham_survey.csv\")\ndurham_filtered = durham[durham['quality_library'] != 9]\n```\n:::\n\n\n::: {#d8f2b426 .cell execution_count=23}\n``` {.python .cell-code}\nsummary_stats = durham_filtered['quality_library'].agg(['mean', 'median', 'std', 'count']).rename({\n    'mean': 'x_bar',\n    'median': 'med',\n    'std': 'sd',\n    'count': 'n'\n})\nprint(summary_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_bar      3.96929\nmed        4.00000\nsd         0.90033\nn        521.00000\nName: quality_library, dtype: float64\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#b46944dc .cell execution_count=24}\n\n::: {.cell-output .cell-output-display}\n![](14-sampling-inference_files/figure-revealjs/cell-25-output-1.png){width=567 height=376}\n:::\n:::\n\n\n:::\n:::\n\n## Calculate 95% confidence interval {.smaller}\n\n$$\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}$$\n\n::: fragment\n\n::: {#27c22567 .cell execution_count=25}\n``` {.python .cell-code}\npoint_est = summary_stats['x_bar'] # Point estimate\nse = summary_stats['sd'] / (summary_stats['n'] ** 0.5) # SE\ndf = summary_stats['n'] - 1 # Degrees of freedom\n```\n:::\n\n\n:::\n\n::: fragment\n\n::: {#24d0201e .cell execution_count=26}\n``` {.python .cell-code}\nt_star = stats.t.ppf(0.975, df) # Critical value \n```\n:::\n\n\n:::\n\n::: fragment\n\n::: {#eef23c21 .cell execution_count=27}\n``` {.python .cell-code}\n# Confidence interval\nCI = point_est + np.array([-1, 1]) * t_star * se\nCI_rounded = np.round(CI, 2)\nCI_rounded\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n```\narray([3.89, 4.05])\n```\n:::\n:::\n\n\n:::\n\n## Interpret 95% confidence interval {.smaller}\n\nThe 95% confidence interval is `3.89` to `4.05`.\n\n::: fragment\n::: question\nInterpret this interval in context of the data.\n:::\n:::\n\n::: fragment\n**We are 95% confident that the true mean rating for Durham residents' satisfaction with the library system is between 3.89 and 4.05.**\n:::\n\n",
    "supporting": [
      "14-sampling-inference_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}