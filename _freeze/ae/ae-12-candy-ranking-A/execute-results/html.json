{
  "hash": "eb9bb171d019512119a6e7e952e0cae8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'AE 12: Ultimate candy ranking'\ncategories:\n  - Application exercise\n  - Answers\neditor: visual\neditor_options:\n  chunk_output_type: console\nexecute:\n  warning: false\n  error: false\n---\n\n#### In this application exercise, we will:\n\n1.  Load the Penguins Dataset: Import and explore the dataset to understand its structure and the features available for analysis.\n\n2.  Preprocess the Data: Clean the data by handling missing values and standardize the numerical features for PCA.\n\n3.  Perform PCA: Apply Principal Component Analysis to reduce the dimensionality of the data and extract the principal components.\n\n4.  Visualize the PCA Result: Create a scatter plot of the principal components to visualize the clustering of different penguin species.\n\n::: {#47ce72c1 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\n# Examine the data\n\n-   We will use the `candy_rankings.csv` dataset for this analysis.\n\n::: {#e0446f91 .cell execution_count=2}\n``` {.python .cell-code}\ncandy_rankings = pd.read_csv('data/candy_rankings.csv')\ncandy_rankings.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 85 entries, 0 to 84\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   competitorname    85 non-null     object \n 1   chocolate         85 non-null     bool   \n 2   fruity            85 non-null     bool   \n 3   caramel           85 non-null     bool   \n 4   peanutyalmondy    85 non-null     bool   \n 5   nougat            85 non-null     bool   \n 6   crispedricewafer  85 non-null     bool   \n 7   hard              85 non-null     bool   \n 8   bar               85 non-null     bool   \n 9   pluribus          85 non-null     bool   \n 10  sugarpercent      85 non-null     float64\n 11  pricepercent      85 non-null     float64\n 12  winpercent        85 non-null     float64\ndtypes: bool(9), float64(3), object(1)\nmemory usage: 3.5+ KB\n```\n:::\n:::\n\n\n# Exercises\n\nUse the variables:\n\n`chocolate`, `fruity`, `nougat`, `pricepercent`, `sugarpercent`, `sugarpercent*chocolate`, `pricepercent*fruity`\n\n## Exercise 1\n\nCreate the full model and show the $R^2_{adj}$:\n\n::: {#51a8a967 .cell execution_count=3}\n``` {.python .cell-code}\nfull_model = smf.ols('winpercent ~ chocolate * sugarpercent + fruity * pricepercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nprint(f'Adjusted R-squared: {full_model.rsquared_adj}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdjusted R-squared: 0.42530696026839643\n```\n:::\n:::\n\n\nIs the model a good fit of the data?\n\nThe model moderately fits the data (42.5% variation explained).\n\n## Exercise 2\n\nProduce all possible models removing 1 term at a time from the full model. Describe what is being removed above each code cell.\n\n::: {#9b947df3 .cell execution_count=4}\n``` {.python .cell-code}\n# Blank dictionary to store new models\nmodels = {}\n```\n:::\n\n\n-   Remove `chocolate` and it's associated interaction\n\n::: {#4e362fab .cell execution_count=5}\n``` {.python .cell-code}\nmodel1 = smf.ols('winpercent ~ fruity * pricepercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model1'] = model1.rsquared_adj\n```\n:::\n\n\n-   Remove `fruity` and its associated interaction\n\n::: {#e8eaafb2 .cell execution_count=6}\n``` {.python .cell-code}\nmodel2 = smf.ols('winpercent ~ chocolate * sugarpercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model2'] = model2.rsquared_adj\n```\n:::\n\n\n-   Remove `nougat`\n\n::: {#5a526d0c .cell execution_count=7}\n``` {.python .cell-code}\nmodel3 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model3'] = model3.rsquared_adj\n```\n:::\n\n\n-   Remove `pricepercent` and its associated interactions\n\n::: {#990e64af .cell execution_count=8}\n``` {.python .cell-code}\nmodel4 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + nougat + sugarpercent', data=candy_rankings).fit()\nmodels['model4'] = model4.rsquared_adj\n```\n:::\n\n\n-   Remove `sugarpercent*chocolate`\n\n::: {#f0406452 .cell execution_count=9}\n``` {.python .cell-code}\nmodel5 = smf.ols('winpercent ~ chocolate + fruity * pricepercent + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model5'] = model5.rsquared_adj\n```\n:::\n\n\n-   Remove `pricepercent*fruity`\n\n::: {#a72bb194 .cell execution_count=10}\n``` {.python .cell-code}\nmodel6 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + nougat + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model6'] = model6.rsquared_adj\n```\n:::\n\n\n## Exercise 3\n\nCompare all models using the framework `best_model_step1 = max(models, key=models.get)`:\n\n::: {#430bb83c .cell execution_count=11}\n``` {.python .cell-code}\nbest_model_step1 = max(models, key=models.get)\nprint(f'Best model in Exercise 2: {best_model_step1} with Adjusted R-squared: {models[best_model_step1]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest model in Exercise 2: model3 with Adjusted R-squared: 0.4318254523153029\n```\n:::\n:::\n\n\n-   Which model is best:\n\nOf the models from Exercises 1 and 2, the model with the highest adjusted $R^2$ is the one with `nougat` removed. Therefore, we will go to Exercise 4 eliminating one variable at a time from that model.\n\n## Exercise 4\n\nCreate all possible models removing 1 term at a time from the model selected in the previous exercise. Again, describe what is being removed above each code cell.\n\n::: {#b2d43d13 .cell execution_count=12}\n``` {.python .cell-code}\n# Blank dictionary to store new models\nmodels = {}\n```\n:::\n\n\n-   Remove `chocolate` and its associated interactions\n\n::: {#f2a2879d .cell execution_count=13}\n``` {.python .cell-code}\nmodel7 = smf.ols('winpercent ~ fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model7'] = model7.rsquared_adj\n```\n:::\n\n\n-   Remove `fruity` and its associated interactions\n\n::: {#9f7e85e8 .cell execution_count=14}\n``` {.python .cell-code}\nmodel8 = smf.ols('winpercent ~ chocolate * sugarpercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model8'] = model8.rsquared_adj\n```\n:::\n\n\n-   Remove `pricepercent` and its associated interactions\n\n::: {#f84d5460 .cell execution_count=15}\n``` {.python .cell-code}\nmodel9 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + sugarpercent', data=candy_rankings).fit()\nmodels['model9'] = model9.rsquared_adj\n```\n:::\n\n\n-   Remove `sugarpercent*chocolate`\n\n::: {#68f9a964 .cell execution_count=16}\n``` {.python .cell-code}\nmodel10 = smf.ols('winpercent ~ chocolate + fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model10'] = model10.rsquared_adj\n```\n:::\n\n\n-   Remove `pricepercent*fruity`\n\n::: {#954646af .cell execution_count=17}\n``` {.python .cell-code}\nmodel11 = smf.ols('winpercent ~ chocolate * sugarpercent + fruity + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model11'] = model11.rsquared_adj\n```\n:::\n\n\n## Exercise 5\n\nCompare all models using the framework `best_model_step2 = max(models, key=models.get)`:\n\n::: {#10c7bab7 .cell execution_count=18}\n``` {.python .cell-code}\nbest_model_step2 = max(models, key=models.get)\nprint(f'Best model in Exercise 4: {best_model_step2} with Adjusted R-squared: {models[best_model_step2]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest model in Exercise 4: model10 with Adjusted R-squared: 0.43488989967424574\n```\n:::\n:::\n\n\n-   Which model is best:\n\nThe model with `sugarpercent*chocolate` has the highest $R^2$ of all the models we've tested so far, so we will now go to Exercise 6 eliminating one variable at a time from this model.\n\n## Exercise 6\n\nCreate all possible models removing 1 term at a time from the model selected in the previous step. Again, describe what is being removed above each code cell.\n\n::: {#58a3efe4 .cell execution_count=19}\n``` {.python .cell-code}\n# Blank dictionary to store new models\nmodels = {}\n```\n:::\n\n\n-   `Remove chocolate`\n\n::: {#8112ce5d .cell execution_count=20}\n``` {.python .cell-code}\nmodel12 = smf.ols('winpercent ~ fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model12'] = model12.rsquared_adj\n```\n:::\n\n\n-   Remove `fruity` and its associated interactions\n\n::: {#1cff2f57 .cell execution_count=21}\n``` {.python .cell-code}\nmodel13 = smf.ols('winpercent ~ chocolate * sugarpercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model13'] = model13.rsquared_adj\n```\n:::\n\n\n-   Remove `sugarpercent`\n\n::: {#579b9b17 .cell execution_count=22}\n``` {.python .cell-code}\nmodel14 = smf.ols('winpercent ~ chocolate + fruity * pricepercent + pricepercent + sugarpercent', data=candy_rankings).fit()\nmodels['model14'] = model14.rsquared_adj\n```\n:::\n\n\n-   Remove `pricepercent` and its associated interactions\n\n::: {#5a4925a0 .cell execution_count=23}\n``` {.python .cell-code}\nmodel15 = smf.ols('winpercent ~ chocolate + fruity + sugarpercent', data=candy_rankings).fit()\nmodels['model15'] = model15.rsquared_adj\n```\n:::\n\n\n-   Remove `pricepercent*fruity`\n\n::: {#712aaf16 .cell execution_count=24}\n``` {.python .cell-code}\nmodel16 = smf.ols('winpercent ~ chocolate + fruity + sugarpercent + pricepercent', data=candy_rankings).fit()\nmodels['model16'] = model16.rsquared_adj\n```\n:::\n\n\n## Exercise 7\n\nCompare all models using the framework `best_model_step3 = max(models, key=models.get)`:\n\n::: {#d2fbdcdb .cell execution_count=25}\n``` {.python .cell-code}\nbest_model_step3 = max(models, key=models.get)\nprint(f'Best model in Exercise 6: {best_model_step3} with Adjusted R-squared: {models[best_model_step3]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest model in Exercise 6: model14 with Adjusted R-squared: 0.43488989967424574\n```\n:::\n:::\n\n\n-   Which model is best:\n\nNone of the models in Exercise 6 resulted in a higher adjusted $R^2_{adj}$. Therefore our final model is the one selected in Exercise 5.\n\n::: {#b1b31ad6 .cell execution_count=26}\n``` {.python .cell-code}\nselected_model = smf.ols('winpercent ~ chocolate + fruity + sugarpercent + pricepercent + pricepercent*fruity', data=candy_rankings).fit()\nprint(selected_model.summary())\n\ncoefficients = selected_model.params\nprint(coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             winpercent   R-squared:                       0.469\nModel:                            OLS   Adj. R-squared:                  0.435\nMethod:                 Least Squares   F-statistic:                     13.93\nDate:                Mon, 19 Aug 2024   Prob (F-statistic):           9.38e-10\nTime:                        13:38:21   Log-Likelihood:                -321.79\nNo. Observations:                  85   AIC:                             655.6\nDf Residuals:                      79   BIC:                             670.2\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                      31.4753      4.271      7.369      0.000      22.974      39.977\nchocolate[T.True]              20.7720      3.923      5.295      0.000      12.964      28.580\nfruity[T.True]                 11.8091      5.130      2.302      0.024       1.598      22.020\nsugarpercent                    8.1016      4.560      1.777      0.079      -0.974      17.177\npricepercent                    6.8970      6.770      1.019      0.311      -6.579      20.373\npricepercent:fruity[T.True]   -17.4218      9.943     -1.752      0.084     -37.213       2.369\n==============================================================================\nOmnibus:                        0.714   Durbin-Watson:                   1.634\nProb(Omnibus):                  0.700   Jarque-Bera (JB):                0.808\nSkew:                           0.201   Prob(JB):                        0.668\nKurtosis:                       2.744   Cond. No.                         13.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nIntercept                      31.475334\nchocolate[T.True]              20.772023\nfruity[T.True]                 11.809087\nsugarpercent                    8.101581\npricepercent                    6.897010\npricepercent:fruity[T.True]   -17.421838\ndtype: float64\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ae-12-candy-ranking-A_files"
    ],
    "filters": [],
    "includes": {}
  }
}