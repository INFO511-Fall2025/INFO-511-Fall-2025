{
  "hash": "1c9520829a9be3b369d7b84be94c3611",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Lab 7 - Modeling II\ncategories: Lab\nexecute:\n  warning: false\n  error: false\n---\n\n# Introduction\n\nIn this lab you'll practice machine learning modeling and then venture on to model validation and quantifying uncertainty.\n\n## Packages\n\nIn this lab we will work with the **pandas** and **sklearn** modules.\n\n::: {#e0a48611 .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Guidelines\n\nAs we've discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\n\n::: callout-note\nRemember that continuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course.\nThere will be periodic reminders in this assignment to remind you to **Run all, commit, and sync** your changes to GitHub.\nYou should have at least 3 commits with meaningful commit messages by the end of the assignment.\n:::\n\nAdditionally, if you’re using functions that are not introduced in the course materials, you must cite your sources.\n\n::: callout-important\nFailure to cite outside resources used, including Large Language Models like Chat GPT, is a violation of the University of Arizona [Code of Academic Integrity](https://deanofstudents.arizona.edu/policies/code-academic-integrity) and will be treated as such.\n:::\n\n# Part 1 - Building a spam filter\n\nThe data come from incoming emails in David Diez's (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012.\nAll personally identifiable information has been removed.\nThe dataset is called `email` and it's in your `data` folder.\n\nThe outcome variable is `spam`, which takes the value `1` if the email is spam, `0` otherwise.\n\n## Question 1\n\na\\.\nFit a logistic regression model predicting `spam` from `exclaim_mess` (the number of exclamation points in the email message).\nThen, display the tidy output of the model.\n\nb\\.\nUsing this model and the Python function `.predict_proba()`, predict the probability the email is spam if it contains 10 exclamation points.\n\n## Question 2\n\na\\.\nFit another logistic regression model predicting `spam` from `exclaim_mess`, `winner` (indicating whether “winner” appeared in the email), and `urgent_subj` (whether the word \"urgent\" is in the subject of the email).\nThen, display the output of the model intercept and coefficients.\n\nb\\.\nUsing this model, predict spam / not spam for all emails in the email dataset.\n\nc\\.\nUsing your data frame from the previous part, determine, in a single pipeline, the numbers of emails:\n\n-   that are labelled as spam that are actually spam\n-   that are not labelled as spam that are actually spam\n-   that are labelled as spam that are actually not spam\n-   that are not labelled as spam that are actually not spam\n\nd\\.\nIn a single pipeline, calculate the false positive and false negative rates.\n\nIn addition to these numbers showing in your Python output, you must write a sentence that explicitly states and identifies the two rates.\n\n## Question 3\n\na\\.\nFit another logistic regression model predicting `spam` from `exclaim_mess` and another variable you think would be a good predictor.\nProvide a 1-sentence justification for why you chose this variable.\nDisplay the tidy output of the model.\n\nb\\.\nUsing this model, predict spam / not spam for all emails in the email dataset.\n\nc\\.\nUsing your data frame from the previous part, determine, in a single pipeline, the numbers of emails:\n\n-   that are labelled as spam that are actually spam\n-   that are not labelled as spam that are actually spam\n-   that are labelled as spam that are actually not spam\n-   that are not labelled as spam that are actually not spam\n\nd\\.\nIn a single pipeline, calculate the false positive and false negative rates.\n\nIn addition to these numbers showing in your Python output, you must write a sentence that explicitly states and identifies the two rates.\n\ne\\.\nBased on the false positive and false negatives rates of this model, comment, in 1-2 sentences, on which model is preferable and why.\n\n# Part 2 - Model validation\n\nIn this section, we will explore how to evaluate the performance of your models using k-fold cross validation.\nThis technique helps in assessing how the results of a statistical analysis will generalize to an independent dataset.\n\n## Question 4\n\na.  Explain in your own words what k-fold cross validation is and why it is useful.\n\nb.  Implement 5-fold cross validation on the logistic regression model from Question 1a predicting `spam` from `exclaim_mess.` Use the `cross_val_score` from `sklearn` to assist with this.\n\nc.  Summarize the results of your cross validation.\n    What is the average accuracy and standard deviation across the folds?\n\n## Question 5\n\na.  Implement 10-fold cross validation on the logistic regression model from Question 2a predicting `spam` from `exclaim_mess`, `winner`, and `urgent_subj.`\n\nb.  Summarize the results of your cross validation.\n    Compare the average accuracy and standard deviation across the folds to the results from Question 4.\n    What do you observe?\n\nc.  Based on the cross-validation results, which model do you think performs better in terms of generalizability?\n    Justify your answer.\n\n## Question 6\n\na.  Plot the accuracy scores of each fold for both the 5-fold and 10-fold cross-validation.\n\nb.  Describe any patterns you observe in the plot.\n\n# Part 3 - Hotel cancellations\n\nFor this exercise, we will work with hotel cancellations.\nThe data describe the demand of two different types of hotels.\nEach observation represents a hotel booking between July 1, 2015 and August 31, 2017.\nSome bookings were cancelled (`is_canceled = 1`) and others were kept, i.e., the guests checked into the hotel (`is_canceled = 0`).\nYou can view the code book for all variables [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md).\n\nThe data can be found in the `data` folder: `hotels.csv`.\n\n## Question 7\n\nRead the data and then explore attributes of bookings and summarize your findings in 5 bullet points.\nYou must provide a visualization or summary supporting each finding.\n\n::: callout-note\nThis is not meant to be an exhaustive exploration.\nWe anticipate a wide variety of answers to this question.\n:::\n\n## Question 8\n\nUsing these data, we will try to answer the following question:\n\n> *Do we expect reservations earlier in the month or later in the month to be cancelled?*\n\n(a) **Exploration:** In a single pipeline, calculate the mean arrival date (`arrival_date_day_of_month`) for both booking that were cancelled and that were not cancelled.\n\n(b) **Justification:** In your own words, explain why we can not fit a linear model to model the relationship between if a hotel reservation was cancelled and the day of month for the booking.\n\n(c) **Model fitting and interpretation:**\n\n    -   Fit the appropriate model and display a tidy summary of the model output.\n\n    -   Interpret the slope coefficient in context of the data and the research question.\n\n(d) **Predicted:** Calculate the probability that the hotel reservation is cancelled if it the arrival date date is on the 26th of the month.\n    Based on this probability, would you predict this booking would be cancelled or not cancelled.\n    Explain your reasoning for your classification.\n\n# Part 4 - Get creative\n\n## Question 9\n\nYour task is to make the following plot as ugly and as ineffective as possible.\nChange colors, axes, fonts, theme, or anything else you can think of in the code chunk below.\nYou can also search online for other themes, fonts, etc. that you want to tweak.\nTry to make it as ugly as possible, the sky is the limit!\n\nIn 2-3 sentences, explain why the plot you created is *ugly* (to you, at least) and ineffective.\n\n::: {#f92577ee .cell execution_count=2}\n``` {.python .cell-code}\npenguins = pd.read_csv(\"data/penguins.csv\")\n\nsns.set(style=\"darkgrid\")\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\nplt.show()\n```\n:::\n\n\n# Part 5 - Harassment at work\n\nThe General Social Survey (GSS) gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes.\nHundreds of trends have been tracked since 1972.\nIn addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years.\nThe GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest.\nAmong the topics covered are civil liberties, crime and violence, inter-group tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.\n\nThe data for this part comes from the `gss16` data frame (containing data from the 2016 GSS) found in your `data` folder.\n\n## Question 10\n\nIn 2016, the GSS added a new question on harassment at work.\nThe question is phrased as the following.\n\nOver the past five years, have you been harassed by your superiors or co-workers at your job, for example, have you experienced any bullying, physical or psychological abuse?\n\nAnswers to this question are stored in the `harass5` variable in our data set.\n\na.  Create a subset of the data that only contains `Yes` and `No` answers for the harassment question.\n    How many respondents chose each of these answers?\n\nb.  Describe how bootstrapping can be used to estimate the proportion of all Americans who have been harassed by their superiors or co-workers at their job.\n\nc.  Calculate a 95% bootstrap confidence interval for the proportion of Americans who have been harassed by their superiors or co-workers at their job.\n    Use 1000 iterations when creating your bootstrap distribution.\n    Interpret this interval in context of the data.\n\n# Wrap-up\n\n## Submission\n\n::: callout-warning\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo.\nWe will be checking these to make sure you have been practicing how to commit and push changes.\n\nYou must turn in the .ipynb file by the submission deadline to be considered \"on time\".\n:::\n\n::: callout-important\n## Checklist\n\nMake sure you have:\n\n-   attempted all questions\n-   run all code in your Jupyter notebook\n-   committed and pushed everything to your GitHub repository such that the Git pane in VS Code is empty\n:::\n\n## Grading\n\nThe lab is graded out of a total of 50 points.\n\nOn Questions 1 through 10, you can earn up to 5 points on each question:\n\n-   5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n\n-   4: Response shows good understanding and addresses most of the rubric items.\n\n-   3: Response shows understanding and addresses a majority of the rubric items.\n\n-   2: Response shows effort and misses many of the rubric items.\n\n-   1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n\n-   0: No attempt.\n\n",
    "supporting": [
      "lab-7_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}